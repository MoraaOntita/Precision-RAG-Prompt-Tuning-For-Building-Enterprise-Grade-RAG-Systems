{"docstore/metadata": {"e968cea1-a502-4cb3-9fa5-165fad0fbe97": {"doc_hash": "9342c251cd2c48a280dfe2de5d9c5ac40fb56ae421db98379890c9a479d74142"}, "1a9ac60b-68bc-4cb6-932c-4b6409424270": {"doc_hash": "f706299aa4269df344fcaec5f20cc90fd90660012453b62d3f81353b6bc25df0"}, "86b88a2a-506c-42c5-9be6-c89d9f176441": {"doc_hash": "02d6bca035a8939844190ea4d1340c75e82dec5aa93bb5f4890557d62363c0ac"}, "72ee50bd-91ae-40bf-9d6d-f0729f591944": {"doc_hash": "0b4a220d507759c53976a128aad402587d54be8299badc906f5db20005e84f20"}, "bb63bed9-e96d-450a-befe-74d04be6c3d7": {"doc_hash": "68f856b758cb7400ed3593c0f2c6177bd94e422d13819e04be4d1a8579d02ada"}, "c9745c1c-43b7-4c6f-b03e-9ed27d1b2c7c": {"doc_hash": "a90587571ac9fe6b364fc6f8da41e331c427770e88a0a8e81c426a248c737c28"}, "983be719-4f7d-4b46-a2e0-de573c270f83": {"doc_hash": "1077bd643740c20f234c2bf59fb0baf0c0e58579e70fc022adbd43332d35ab66"}, "c368a139-fdd0-4cfd-862f-89dedca67464": {"doc_hash": "5c01e05c9b66a4788e040063334b0394a37d352091746b61d778d9a7b008667d"}, "400ed0e3-96e8-4fac-b259-53fce81b308b": {"doc_hash": "6a7d821e3bcbb783caa03ffb9a3ff9cc75616259150a5452e0d5b454b89ddab2"}, "281d72ae-706e-4730-9aa2-7fed4b3dcd2c": {"doc_hash": "377c3a6c2eefbf3598e1a05a1f5ad8a30ae6c9a51bfc8c0d0ce3481c6a69f5c5"}, "ffe3dbcf-b7d0-4566-b4ae-d519caa9ed8a": {"doc_hash": "31bc542e171273b78f244410549f00131e9a7be620b4d4c7b872a8ae317f648c"}, "ca80b0f6-b94d-4be9-9164-f0933cd58098": {"doc_hash": "fe0b599e2f1f4782b56a7cceb6c92e14aa5f140f857547287d7974a57b125f63"}, "913bd659-b6ac-42c3-9d68-e20ec1639db6": {"doc_hash": "5430181e483ec5b2c1ca56af9e8029ec24fed3d1ea369f1496392386b04e8fd3"}, "85341c2a-a043-4165-8d5b-db221409e1d0": {"doc_hash": "f67cae515922564f39a5e8cc3d6b72212aaf6c422c2d40635c466dce72d91eb6"}, "fd04ce89-ee65-4c26-8cee-eb0ed9120c40": {"doc_hash": "c9737cde691d828cfb9b248a4999ba1bb2edf1a551f4a3c0f086dc538b16b6e0"}, "99eb7832-16cf-4b6e-8300-fb674fb0651d": {"doc_hash": "3987ed39b1e977fdd64b03d4de25a70c7b672fff7c1279192cfb6d2f8ac9c16a"}, "8706c933-385c-4f7a-8d1b-0f7c40870e61": {"doc_hash": "3fa2505ab0169fcb5f9e2a44b2b990d3f803afc36f3ec4c4edfd5eff339d84da"}, "5e027d43-3e8b-420d-a134-27e4da767cd7": {"doc_hash": "b64de7f8aa7a26429abd22d1e2233700195fb5e6660c1676b75ec4171495da0a"}, "d3f5a848-f1f9-4ba3-b78b-f2246d4186aa": {"doc_hash": "8e38a22c0397fbac740fd72141036d40a5edc99d129ba417773c3b70ed0989ba"}, "485e9781-7c77-4015-a0e4-1f399abd0e2d": {"doc_hash": "5eacfb6e5da64dad22c8c9de84a71b68bf83578cbefbe5e837ae08026bbc03a3"}, "22bb4129-e78e-48d3-921a-fa73a3d1f7df": {"doc_hash": "f1c3ec9c1d016a8d388efdaffec5901f6800636be87ea7874dd1820469383e70"}, "f1cd40ef-11ea-4e7a-a88a-05c774a32226": {"doc_hash": "a1209b4543d5136e6874f63adaaf96c6d77eaea1b3975a1a482583441b7fc4b2"}, "d97e69df-0a2f-4b0a-a3d4-3634e73dbab4": {"doc_hash": "aef7a8bd73dd6448eca921c357ece3f8fd92d0bb1881355b5421c015e8d5e2ef"}, "05f3a61d-ded7-4eb3-97e0-9b624ba9e052": {"doc_hash": "5d2c1b73b648d2e3d32c9253536db534423d4df28f7c0d2a19df6b99ff869b1e"}, "2caa6841-b9a9-434b-8739-8f1f25ac9fed": {"doc_hash": "a07dea7a4d12d3a256ba8c4c800d4fb56dbd6f3dbf57c6ee04ad030994d07f3a"}, "9e0e4a08-388f-4272-97d6-6915c4233ecd": {"doc_hash": "bd92fc3ec42f0a47e4570bb9c39abb5e519b6812d2a536b1a6ced45e35acaadd"}, "b0116d15-a8a7-4c8c-a4a5-d6249046e4ba": {"doc_hash": "1dd76d0fd722c116abe23d96b5fabc494a12d8a329c98913faf99c48a39dc368"}, "0cc264c1-fb4d-4cbf-be10-8841aa149d7d": {"doc_hash": "df061f59f46baf3e264119ad90f364516ffd7d2df2a7f08483bb3e814eae1789", "ref_doc_id": "e968cea1-a502-4cb3-9fa5-165fad0fbe97"}, "832a7118-3876-4f66-a980-63222ab5349d": {"doc_hash": "8c28e8b7ec9d7e44673a0bfab3432cc22fce449b61b976987f7ed2080ee41cc6", "ref_doc_id": "1a9ac60b-68bc-4cb6-932c-4b6409424270"}, "b2c785b1-167f-471f-92a3-70d1cd47447e": {"doc_hash": "a2bbd2655118c067b05daf349f8a31d3073f31d08ff3db343b9d319ffd9d3f17", "ref_doc_id": "86b88a2a-506c-42c5-9be6-c89d9f176441"}, "e7e6113e-562a-4166-96a6-1040ae75dde5": {"doc_hash": "a539e745541955578760bb192c555a998ffeec5563b3a849dce330993a134941", "ref_doc_id": "86b88a2a-506c-42c5-9be6-c89d9f176441"}, "27dd70b0-b8b8-43d1-860e-14b506ce2d09": {"doc_hash": "b03cd2f86f81266c433849fe78622c90d68ab6bc09c472d2db62df08b1a4861f", "ref_doc_id": "72ee50bd-91ae-40bf-9d6d-f0729f591944"}, "de4e5b1e-4261-4722-94b4-321670498b49": {"doc_hash": "315c0459319061f40bb14bac182b22c0471a033dd68fdc83d7ce68a63a0ba95d", "ref_doc_id": "72ee50bd-91ae-40bf-9d6d-f0729f591944"}, "f8eabfcb-05a3-40fa-bbf4-08deaec84d86": {"doc_hash": "7230745e063346a77dccfed422c29a04d1a694b38451f533da23d446f2b7d4c0", "ref_doc_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7"}, "f37d496b-4500-4fce-a23b-f42180c83aa6": {"doc_hash": "04d9b5d81f1cd869b97ad022e0d146bce7566c8929ab8acec480c69888d9e7e6", "ref_doc_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7"}, "d145b5c1-7984-4215-b3c2-2f37e3b1f599": {"doc_hash": "50b6f94fbeb4f02fe7d41e5ad3dd44acdec3a0888807dd5a2f522e4812464782", "ref_doc_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7"}, "21f68195-32d4-4909-92be-0b6c1b099fec": {"doc_hash": "ae298c1323e1a69ef0ac46b38aa1c04e4af70748e090b6c10a6ef7387fee0799", "ref_doc_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7"}, "14ebf21a-8198-4ee7-915a-268a0baa80cd": {"doc_hash": "3a087ffbf1768aa4b7b419af612e97c0fee82faa10d06f1390e561aa5e500803", "ref_doc_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7"}, "944c9001-5ef6-486f-9882-cef669827d0f": {"doc_hash": "1ff5e7865a07f789b4325ba66496f4a89793d2d853e5038c5d10b54a0f9744e8", "ref_doc_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7"}, "c96f8d42-e760-4e1a-baf9-75850d4c4e4d": {"doc_hash": "dc35979feb58e4464b5abd026c97e56875d58473afab0e9610c061fd432f82d1", "ref_doc_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7"}, "f1a974ba-729b-4d25-bc00-5b1631ae6eed": {"doc_hash": "813247ac8d2405af0f8d9e30880ec6839263986fce088ee1290aaa10f0d809a3", "ref_doc_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7"}, "07e103ba-50e5-4ec1-9cc1-584ec91aa634": {"doc_hash": "ed445c58ed02963a0bfc93cc611449bf11c899f9333593ded25a5ec5821c9144", "ref_doc_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7"}, "bd1ab8ec-ff13-4e1e-ae80-025843e92cde": {"doc_hash": "d8e69f5bb686291deba6429627808e23334bcb730cccd60514761d407307ba99", "ref_doc_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7"}, "5a4d36f3-355f-4f48-8aa8-47c41173d166": {"doc_hash": "8fc19f022e19891b25d2b36aab334d3e3711d8a3ab74c0f67b7c5cca31d52dc7", "ref_doc_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7"}, "a1f6190a-6a6d-4bb2-b4b7-a7f9f2ac96d3": {"doc_hash": "25d3c9b59218f9bcd8aa21973eec23b45c4f2aa0193220e4be0a4ef047feed99", "ref_doc_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7"}, "77d9da54-7c42-4207-a8e7-caee2ac87d6a": {"doc_hash": "88977ef52759b975102a8fd3630db6c18b4f70288bb60722728b12fc050f4963", "ref_doc_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7"}, "25eadb1e-1448-4df1-b9d3-69d08cfada22": {"doc_hash": "c4641087af3a9638ecb257103777394f4a67107a6ff71be8b7fa11abc77f60f0", "ref_doc_id": "c9745c1c-43b7-4c6f-b03e-9ed27d1b2c7c"}, "450022b1-1ffe-416f-9f58-4e00afa23d5d": {"doc_hash": "c11b30a37d1412259800cbfecd5318d4aedbc45012f3c7af1245a637d58a7b5b", "ref_doc_id": "983be719-4f7d-4b46-a2e0-de573c270f83"}, "34630ba6-49bb-4122-92eb-b42adf1f58c3": {"doc_hash": "40b3f66e0c0360fee0b3dce1c77e18fe5dcf0624da8c91026be28c2349ead32e", "ref_doc_id": "c368a139-fdd0-4cfd-862f-89dedca67464"}, "b452b052-3e9f-4ef5-a677-48b2239b1884": {"doc_hash": "fc556ce1424987186acd62d6192ce1e34588150faab2d210d4c9a6df4043266b", "ref_doc_id": "400ed0e3-96e8-4fac-b259-53fce81b308b"}, "f525727f-f691-4885-815d-02d4eb24c55a": {"doc_hash": "0c32ac4e4b1656c2cd935b0f1a53e251c3042158e150b7b08e49bbab099301b3", "ref_doc_id": "400ed0e3-96e8-4fac-b259-53fce81b308b"}, "b3415edd-b73b-44c4-be73-c16c66c62240": {"doc_hash": "e9fe85450184b674bc326920acfaede90ee7a4e3e0cea7f8b2fe0f8084800e04", "ref_doc_id": "400ed0e3-96e8-4fac-b259-53fce81b308b"}, "4f7a87bf-2e16-44fd-a3f1-d39ea3ba3eb9": {"doc_hash": "36d330063d6f8e4f5e5c2d6cef464ab51385dbe2ea632466c66cfb467513dc77", "ref_doc_id": "400ed0e3-96e8-4fac-b259-53fce81b308b"}, "cf883145-f96b-4c75-bd42-99f12ce780d9": {"doc_hash": "087f8009159974f5ed4ba5523d29736a0613272ca9ac736c22e14b903ee494bf", "ref_doc_id": "400ed0e3-96e8-4fac-b259-53fce81b308b"}, "2ad3ff9e-7acb-411f-bcd6-b3dc21f4a045": {"doc_hash": "e4b307c0da2b22b054ea7b8018de1ac56492c1feef339342b577fb8660bbaa0a", "ref_doc_id": "281d72ae-706e-4730-9aa2-7fed4b3dcd2c"}, "66ca1fde-05a7-4c6a-a2e2-5ebbfe650f3f": {"doc_hash": "eb653af2afa2a32321dd91483a1cfc162340bd943a8f15280d69173b91e0325b", "ref_doc_id": "ffe3dbcf-b7d0-4566-b4ae-d519caa9ed8a"}, "be69b7c1-ab89-4f61-807a-a9e778f60ae8": {"doc_hash": "da03490798871f2d0098f358ce8558bfb4a776efe748b0adefbf82930e187836", "ref_doc_id": "ffe3dbcf-b7d0-4566-b4ae-d519caa9ed8a"}, "32a9398f-0f4f-4a08-aff1-43c30a00529a": {"doc_hash": "58b575e2c18342d3db152aca15825ed0c97236a1123109cda99cd84067b2ce1f", "ref_doc_id": "ffe3dbcf-b7d0-4566-b4ae-d519caa9ed8a"}, "a66145b4-44c0-40bf-8ad1-fc8a43384b8f": {"doc_hash": "0d124d2c43b44dab3d30edb7c06f1c56bad23056cc9f4a17a1d3d8c4ad58613f", "ref_doc_id": "ffe3dbcf-b7d0-4566-b4ae-d519caa9ed8a"}, "fd13e060-dc5e-4c64-9ae6-5afebea58d32": {"doc_hash": "115e5c82ee513d803c57d3b97b7e1005a19a1c36617fb31004f664f42fc78d0b", "ref_doc_id": "ffe3dbcf-b7d0-4566-b4ae-d519caa9ed8a"}, "fc8fd058-446b-46fb-8f4d-09a4f3db721a": {"doc_hash": "b02b893c515d3f733e9d7f6076cb01b0d0c6769d9e66a65e6eee3500303dfcd0", "ref_doc_id": "ca80b0f6-b94d-4be9-9164-f0933cd58098"}, "8e105af5-c502-4415-a3c1-eb97d221356e": {"doc_hash": "a65a60a188a17c1f1419d541a477b6e847366c376ba33e12b29e9fd4efce40ad", "ref_doc_id": "ca80b0f6-b94d-4be9-9164-f0933cd58098"}, "5743c4ec-2a50-495b-9bae-c52c6842b09e": {"doc_hash": "b075a6ec1483f5ad22c0ffcd4716e41d7728d81d9c3a3155eb11428656967888", "ref_doc_id": "ca80b0f6-b94d-4be9-9164-f0933cd58098"}, "a3ba542d-51c7-4ebc-bf1f-be8ad36cb7d3": {"doc_hash": "54865ce0d9f61dd0f87d6403e9a9bb9c7230779a358092136997d46cf6c9c091", "ref_doc_id": "913bd659-b6ac-42c3-9d68-e20ec1639db6"}, "6e4eeb68-d623-4d3d-8e04-ad45c67a2a35": {"doc_hash": "82033d1af3485689d7f1300559f8b8e1242a3f3a0762c297db2f46ee56b5ef46", "ref_doc_id": "913bd659-b6ac-42c3-9d68-e20ec1639db6"}, "3216d25b-d3c5-4a7a-ae21-f1c0cbf4f168": {"doc_hash": "d4266542daf0b53988abf7c502dad83f0a82852ef7f8d350273bb3122beac5ac", "ref_doc_id": "913bd659-b6ac-42c3-9d68-e20ec1639db6"}, "50ca1b87-339f-4b5b-b9df-a0c7aba9682c": {"doc_hash": "dcb9ab4228e0147d513c596ff5fc6a768aa09aaeacfb0d4c09f44974cbd6c7a7", "ref_doc_id": "85341c2a-a043-4165-8d5b-db221409e1d0"}, "805357ac-c84c-4cab-acd4-55c6f0450b78": {"doc_hash": "1174689879b1812fa40d162f1867849954cf83e4f0c0f67e8282690574a255f0", "ref_doc_id": "fd04ce89-ee65-4c26-8cee-eb0ed9120c40"}, "68a074e6-6323-4b68-a70b-ad619a311f35": {"doc_hash": "fcb33048b35c6f7e657bb08e5278df9ce905c60248b4eb3a43099de0f3465d6c", "ref_doc_id": "fd04ce89-ee65-4c26-8cee-eb0ed9120c40"}, "045f4922-71e0-4de9-acca-71a3b5657f00": {"doc_hash": "ee31bd1863a68f4c881dd3940eb854fd49a5ce5cb4b5fcdbc2d879704a2eae3a", "ref_doc_id": "fd04ce89-ee65-4c26-8cee-eb0ed9120c40"}, "4fd96d3c-91ba-4f7d-9795-3ddc3c7545dc": {"doc_hash": "238d624ba723848ba09c1563597ff634d197e044b7bfc48907f40399c2f8f60f", "ref_doc_id": "fd04ce89-ee65-4c26-8cee-eb0ed9120c40"}, "197e6528-11b7-45ed-80d2-ee6800bc02ac": {"doc_hash": "6bcad876d091dc104f144ca0de44b2d9a224a5b9a39ebc1f2eced8d1736e06a5", "ref_doc_id": "99eb7832-16cf-4b6e-8300-fb674fb0651d"}, "9fa8dda1-b3c5-4ded-9f93-2ded8dc0afce": {"doc_hash": "67cb221fdbe07c5f39645e1c272c5eeaf130ab72c76034a629bb39b05cf81777", "ref_doc_id": "99eb7832-16cf-4b6e-8300-fb674fb0651d"}, "535fb58a-bbef-423f-972c-3592d270b0f7": {"doc_hash": "77d848a0f3490afbcbd98b689ea7bc77740c40aa8dc336c475c11b489865dd0d", "ref_doc_id": "8706c933-385c-4f7a-8d1b-0f7c40870e61"}, "93e4ae38-a046-457f-9d23-96f5155bb1c8": {"doc_hash": "32467f76b4b9da84666b7735add459fa1f9d63cb7498a2fe108003d19afb7c75", "ref_doc_id": "d3f5a848-f1f9-4ba3-b78b-f2246d4186aa"}, "c5086421-dd73-499c-9157-71e45a9463ea": {"doc_hash": "6d89461f667af01611809fcfc34d0f95c27b3ca7540c06dee0ee4baf0cd4a12a", "ref_doc_id": "d3f5a848-f1f9-4ba3-b78b-f2246d4186aa"}, "5cba45a4-5a27-404b-ba56-9767f5f47759": {"doc_hash": "5e96bdf192ec294e5f65de6232e9c483ebe76ed6766f66580b540057ba7d153a", "ref_doc_id": "d3f5a848-f1f9-4ba3-b78b-f2246d4186aa"}, "cd027211-1b03-4652-a363-f601de8f98e9": {"doc_hash": "295a4b8deee96d18b949dbd6f1a3a3f2f85122ff984eadf89abaac0a51b1aacf", "ref_doc_id": "d3f5a848-f1f9-4ba3-b78b-f2246d4186aa"}, "5d21f329-2c8d-453c-90d0-b18559722131": {"doc_hash": "efb4d2295b91d892ef9848243be43ca3d923c9d6f0733760ec13d6eedb0f51b1", "ref_doc_id": "d3f5a848-f1f9-4ba3-b78b-f2246d4186aa"}, "f2c40a1d-2be6-42e1-80a5-6cb208c16cff": {"doc_hash": "824fb620da42c6ff8ae9f553782b3b7c78f5b55f613a57b81fc6bb7e507c8f4c", "ref_doc_id": "d3f5a848-f1f9-4ba3-b78b-f2246d4186aa"}, "8ed6b151-b867-4ba5-9f5e-4ba929044d23": {"doc_hash": "cc268590a7a2ec6d24c385d1061027b99d64d75212f90c0a5e5ee8c8f4e78dbe", "ref_doc_id": "485e9781-7c77-4015-a0e4-1f399abd0e2d"}, "9836a060-fb0f-41c3-b0cc-39c736b961dc": {"doc_hash": "97df0f8d31f2c6acc32e22203fce7e0b66de8b7718bbf541549081934cafdb94", "ref_doc_id": "22bb4129-e78e-48d3-921a-fa73a3d1f7df"}, "8485f53e-4366-4925-94db-1f1f0f1db64a": {"doc_hash": "8b45cb03baaaa99fd9f8a2815bf0208a139ad66a25f4838f019681d3997ba800", "ref_doc_id": "22bb4129-e78e-48d3-921a-fa73a3d1f7df"}, "e8f7cca3-b9d9-4703-8274-93bdbd62048b": {"doc_hash": "e0ab3d923087c0c0fb2083db6875a8559f853d4227ddfa75ec03c681dee8d09b", "ref_doc_id": "22bb4129-e78e-48d3-921a-fa73a3d1f7df"}, "9e46ee6f-6e1e-4890-afde-dbe518f9fdfc": {"doc_hash": "1d5325aeaf3e5c8111686eb8ebfa32cfa7f64f3bc2fe4924cac99dcff02f2d27", "ref_doc_id": "f1cd40ef-11ea-4e7a-a88a-05c774a32226"}, "3e2f842f-f73d-42d0-a9c6-26735788256e": {"doc_hash": "cdfe54b04415dae229ecbc60f3b389baa0f48c3d6957a94e646eff83f4c57817", "ref_doc_id": "d97e69df-0a2f-4b0a-a3d4-3634e73dbab4"}, "c718f1af-8ad9-4372-9b32-7b285523ef03": {"doc_hash": "334e804aef8b2c1a3f034691b29ab5f6230bc177d56290e5bc8205a2f8f4f3bc", "ref_doc_id": "05f3a61d-ded7-4eb3-97e0-9b624ba9e052"}, "944464a4-f05f-4d22-90bf-e51df3ec7572": {"doc_hash": "a77b3eac9ce1d7ff9b919a1ebee7d6a9c2b289f71318a329fb5a5cdeff5706d6", "ref_doc_id": "05f3a61d-ded7-4eb3-97e0-9b624ba9e052"}, "200ae9be-35b3-4dce-b978-78a9ab45051c": {"doc_hash": "2bc67f1e7ca4c1bae3e64c5841abe512d6fe381e9ca7e587d298700c4b41de84", "ref_doc_id": "05f3a61d-ded7-4eb3-97e0-9b624ba9e052"}, "de101251-6a1b-45eb-862f-363b603f355a": {"doc_hash": "d6df9815566b5df8a42e678737c9ca374a02d58a03d82c5643833140fbf3d95d", "ref_doc_id": "05f3a61d-ded7-4eb3-97e0-9b624ba9e052"}, "7d1a4df3-e851-4dae-9dd2-07a0e1a8e5fa": {"doc_hash": "5497d7f14f5183f6ce08f3faa3cef5a98ac69b507583c8ad6b43c509466d778e", "ref_doc_id": "05f3a61d-ded7-4eb3-97e0-9b624ba9e052"}, "99e38979-48ca-4ada-a916-fc6d0a9b1599": {"doc_hash": "c59a87e46ae4c11b2ccde464bb2f64867e45c81ef189cfb352b3f31793b90b61", "ref_doc_id": "05f3a61d-ded7-4eb3-97e0-9b624ba9e052"}, "ef1e2138-0a34-48b8-b2fd-76be3b050586": {"doc_hash": "546929320ffcbf4b63ca24b36906bb19a6422ae46ddbc5da2d361ca4df286230", "ref_doc_id": "2caa6841-b9a9-434b-8739-8f1f25ac9fed"}, "cc32dd09-e343-46df-a203-14e691b18d43": {"doc_hash": "7b9d211936bb405966c25f08320e6368b1a747001e81b024f8b584757e217fc8", "ref_doc_id": "9e0e4a08-388f-4272-97d6-6915c4233ecd"}, "cc4ef1cb-dcea-46b9-a494-f0acc3d66efd": {"doc_hash": "7671874c9f9bbd01520a30e7281e7e2eecde3d4257242b91226e2f6f76b1b736", "ref_doc_id": "9e0e4a08-388f-4272-97d6-6915c4233ecd"}, "f04a4751-239b-46ed-9c00-ab824a58827b": {"doc_hash": "adc22827b85a662804c510b57b7a951fbffbef2a752bdc5d90edae821a351253", "ref_doc_id": "b0116d15-a8a7-4c8c-a4a5-d6249046e4ba"}, "2f3e0020-1373-44d5-b4d8-92c0f7c36f36": {"doc_hash": "0216f17cb2a73d916e6bd2e127c119679a100c35c64dab90404806d7dcfcd7bf", "ref_doc_id": "b0116d15-a8a7-4c8c-a4a5-d6249046e4ba"}, "469ccf88-5155-4e6c-b8ad-f7ce7e0cd6d9": {"doc_hash": "55e13a834f797362a8414fea424f9a7301fe1623b1603efedf19e72a27095576", "ref_doc_id": "b0116d15-a8a7-4c8c-a4a5-d6249046e4ba"}, "6777f239-17c9-45f8-a06a-e094d1332ded": {"doc_hash": "0b0ba60bb507a62daa29690437a87627d80972c825d65dd40225ea4c80886579", "ref_doc_id": "b0116d15-a8a7-4c8c-a4a5-d6249046e4ba"}, "abba96d6-93d0-4a8c-b1dd-f0b3b5492caf": {"doc_hash": "ccfdd4819e273630399fac25a35e071f8e8a09313865dd29ea02600ef8922faa", "ref_doc_id": "b0116d15-a8a7-4c8c-a4a5-d6249046e4ba"}}, "docstore/data": {"0cc264c1-fb4d-4cbf-be10-8841aa149d7d": {"__data__": {"id_": "0cc264c1-fb4d-4cbf-be10-8841aa149d7d", "embedding": null, "metadata": {"source": "W4 - Utilizing AI to Enhance Social Media Management and Customer Engagement.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e968cea1-a502-4cb3-9fa5-165fad0fbe97", "node_type": "4", "metadata": {"source": "W4 - Utilizing AI to Enhance Social Media Management and Customer Engagement.docx"}, "hash": "9342c251cd2c48a280dfe2de5d9c5ac40fb56ae421db98379890c9a479d74142", "class_name": "RelatedNodeInfo"}}, "text": "10 Academy cB : Week 4\nCareers - Exercise 2\nUtilizing AI to Enhance Social Media Management and Customer Engagement \n\n\nDeadline: Saturday, 18th May 2024, 8 pm UTC\n\n\nScenario:\n\nYou have been hired as a Social Media Associate for a prominent e-commerce company specializing in fast-moving beauty products for females. The company boasts an impressive online presence, particularly on Instagram, having 7 million followers, with hundreds of daily sales attributed to its social media marketing efforts.  The company has set up an Instagram Shop to allow easy checkout for products. Every day, the account gets hundreds of activities, including likes, comments on posts, reposts, tags, and inquiries on direct messages that require you to manage. \n\nAs a 10 Academy trainee, who is knowledgeable in implementing AI models, you are tasked to propose and implement AI solutions to streamline customer engagement and improve the overall efficiency of the company's social media management and customer satisfaction. The company\u2019s specific goals are for you to:\n\nManage a high volume of comments and inquiries on Instagram.\nIncrease sales for your company\n\n\nDeliverables and Tasks to be Done\nTasks:\nYou are required to present how you\u2019ll tackle this task to your non-technical managers. Prepare a presentation that you\u2019ll give entailing the following:\n\nA step-by-step procedure you will follow to implement an AI bot to efficiently respond to comments on each Instagram post and direct message(DMs). You can choose to create one of your own from scratch or use an existing one from the Facebook Developer Platform to train it. This should cover the following steps:\nData collection for training(previous chat histories, company files, emails, etc)\nAnnotation and labeling process, for example, label conversations based on topics, sentiments, etc.\nTrain the bot, which ML models/architectures do you plan to use and why?\nHow will you monitor and evaluate your bot? What metrics will you use and why? \nInform the client on any costs that might be incurred while setting up/running.\nHow would you get your bot to interact with instagram?\n\nReflect on any ethical implications of using AI in this context, particularly regarding customer privacy and data security. Discuss 4 strategies to ensure transparency and maintain customer trust while leveraging these chatbots. \nOutline 4 key performance indicators (KPIs) to evaluate the success of implementing AI. \nFinally, reflect on the potential impact of chatbots on the future of social media management and e-commerce. Discuss 5  advantages and limitations of using AI in this context, and how it can shape the industry moving forward. \n\n\n\nTeam\nInstructors:\nMargaret Chepkirui - Careers Tutor\nPascaline Iyodusenga - Lead Tutor\nArun Sharma - co-founder, 10 Academy\n\nKey Dates\nTutorial session: Thursday, 16 May 2024. \nSubmission deadline: Saturday, 18 May 2024, 8:00 pm UTC.\n\n\nSubmission\n\nA report answering the questions in the tasks above. \nSave your PPT as a PDF before submission.\n\n\n\n\nUsefulness in real life\nThis exercise aims to develop your understanding of applying AI to enhance social media management and customer engagement. You will gain practical insights into the power of AI in improving efficiency and the customer experience.\n\n\n\n\nUseful links\n\nA real-life company implementing this - https://go.schoolofbots.co/welcome\nYoutube Podcast on Instagram DM Automation Strategy - https://www.youtube.com/watch?v=tYF_4hZ6-bY\nChatGPT Can Now Assist With Travel Planning in the Expedia App -Expedia product for travel.", "start_char_idx": 0, "end_char_idx": 3563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "832a7118-3876-4f66-a980-63222ab5349d": {"__data__": {"id_": "832a7118-3876-4f66-a980-63222ab5349d", "embedding": null, "metadata": {"source": "cB - Careers challenge - Effective communication - W3.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a9ac60b-68bc-4cb6-932c-4b6409424270", "node_type": "4", "metadata": {"source": "cB - Careers challenge - Effective communication - W3.docx"}, "hash": "f706299aa4269df344fcaec5f20cc90fd90660012453b62d3f81353b6bc25df0", "class_name": "RelatedNodeInfo"}}, "text": "10 Academy cB : Week 3\nCareers - Exercise 1\n\nEffective Communication\n\nDue Date: Saturday, 11th May 2024, 8P\nM UTC\nBackground\nYou have just been hired to coordinate a team of 5 members that are currently working on the Alpha project because their team lead recently resigned. The team has been working on the project for 4 weeks and despite being composed of highly skilled professionals, they are significantly behind schedule. They have only two weeks remaining until the project deadline. On your second day of your employment, you realized that the team has some issues that have been negatively impacting their performance.\nThe team began the work without a well-defined understanding of the project's goals and its strategic plan. This led to inefficiencies as team members engaged in tasks without a clear direction or understanding of the overall project objectives.\n\nTeam members have not been assigned roles that best fit their skill sets, leading to inefficiencies and frustrations.\n\nTeam members do not regularly communicate their progress, challenges, or needs and this leaves some team members isolated when they encounter difficulties.\n\nAlthough team members are individually capable, they tend to work alone and do not share insights. This isolation not only slows down individual tasks but also prevents the team from leveraging collective knowledge to overcome complex challenges.\nFurthermore, two of the team members happen to be chronic late comers. They resume work late and are always late to schedule meetings too. And this attitude has some drawbacks on the overall team.\n\nTask\nDescribe four steps you would take to ensure that all team members understand the project goals and their individual responsibilities. \n\nWhat three ways would you ensure that each team member is assigned a role that fits their skills?\n\nDescribe three strategies you would implement to improve communication among team members?\n\nPropose three methods to foster a collaborative attitude among team members.\n\nUsing the framework for effective communication, outline how you would make your concern known to the two late-comers in the team.\n\nSubmission\nCreate a PowerPoint presentation with 7 slides that detail your answers to the tasks written above.\n\nRubrics\nUnderstanding Project Goals and Responsibilities: You are required to write steps ensuring team members understand project goals and individual responsibilities. Grading will focus on the clarity and specificity of proposed steps.\n\nOptimize Team Member Role Alignment with Skills: You should propose methods to align team members' skills with appropriate roles. Grading will consider the identification of skill sets, strategies for assessing skills, and flexibility in role assignments.\n\nImproving Team Communication: You are asked to describe strategies to enhance communication within the team. Grading criteria include the effectiveness and variety of proposed strategies.\n\nFostering Collaborative Attitudes within the Team: Grading will look on if you proposed methods to promote a collaborative attitude among team members. \n\nAddressing Late-Comers: You should outline how to address concerns with late-comers using effective communication. Grading criteria include application of communication framework, clarity and professionalism in addressing concerns.\n\nUsefulness in life\nThis activity is designed to give you a practical understanding of how communication issues can impact the workplace. By working through this scenario, you'll develop your critical thinking and problem-solving skills as you focus on improving communication strategies. This will ultimately lead to better results in your professional life.", "start_char_idx": 0, "end_char_idx": 3682, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2c785b1-167f-471f-92a3-70d1cd47447e": {"__data__": {"id_": "b2c785b1-167f-471f-92a3-70d1cd47447e", "embedding": null, "metadata": {"source": "cB - Careers challenge - Procrastination - W4.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86b88a2a-506c-42c5-9be6-c89d9f176441", "node_type": "4", "metadata": {"source": "cB - Careers challenge - Procrastination - W4.docx"}, "hash": "02d6bca035a8939844190ea4d1340c75e82dec5aa93bb5f4890557d62363c0ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7e6113e-562a-4166-96a6-1040ae75dde5", "node_type": "1", "metadata": {}, "hash": "42c0f1767bcce7d39bb1041bd685d774f2830f266a9dc2b6222c3fa95a45b22f", "class_name": "RelatedNodeInfo"}}, "text": "10 Academy cB : Week 4\nCareers - Exercise 1\n\nProcrastination\n\nDue Date: May 18, 2024. 8:00 PM UTC\n\nBackground\n\nWhat is procrastination?\nProcrastination is the act of unnecessarily postponing decisions or actions. For example, if someone delays working on an assignment until right before its deadline for no valid reason, even though they know that it would be better for them to start earlier, that person is procrastinating.\n\nRead more about procrastination in this introductory article written by Joseph Ferrari, Professor of Psychology at DePaul University. Find the article here.  \n\n\nWhy people procrastinate.\nTasks have always been associated with schedules and deadlines. And it is very evident that one of the most important resources of all time; time, cannot be carried over or transferred to the next day, week or month. Time spent is spent! There comes a point where you feel like postponing specific tasks. With a personal intention that you\u2019ll sit on the tasks later. This act of delaying or postponing a task that you are entirely sure that its deadline or schedule is active, is what we refer to as procrastination. Read why people procrastinate here. \n\nInside the Mind of a Procrastinator\n Watch this TED Talk presentation entitled, 'Inside the mind of a master procrastinator by Tim Urban. \n@Tim Urban: Inside the mind of a master procrastinator | TED\n\nThe witty, visual, and relatable nature of this video will help you to see procrastination through the eyes of another individual\u2019s experience. It adds humour to the concepts which make it easy to understand and to work with as we delve deeper into what procrastination means.\n\nTask:\nAs Tim shared his story in that TED talk, write your own procrastination story while at 10 academy too. Make sure it is original and it captures the essence of your own experiences. Make it yours! \n[While sharing us your story, emphasise on the following;\n\nThe name that specific task/challenge you procrastinated on. \nHighlight the particular elements procrastinated in that task/challenge. \nHow you felt while working on it at the very last minute?\nWere the results positive or negative? \n\nIn general, what are the underlying reasons or triggers for your procrastination? Provide a list of 4 reasons with their detailed explanation.\n\nIn general, Reflect on the role of distractions in your procrastination. What 5 external factors or distractions often derail your focus and contribute to procrastination? \n\nIn general, List 4 excuses or justification you use to rationalise your procrastination behaviour to yourself? Be honest about the thought patterns or beliefs that enable your procrastination habits.\n\nIn general, Reflect on past instances of procrastination. Were there any negative consequences or missed opportunities? Explain them. \n\nThink about times when you successfully avoided procrastination. What strategies did you use? (List as many strategies as possible.) Explain them.", "start_char_idx": 0, "end_char_idx": 2948, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7e6113e-562a-4166-96a6-1040ae75dde5": {"__data__": {"id_": "e7e6113e-562a-4166-96a6-1040ae75dde5", "embedding": null, "metadata": {"source": "cB - Careers challenge - Procrastination - W4.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86b88a2a-506c-42c5-9be6-c89d9f176441", "node_type": "4", "metadata": {"source": "cB - Careers challenge - Procrastination - W4.docx"}, "hash": "02d6bca035a8939844190ea4d1340c75e82dec5aa93bb5f4890557d62363c0ac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2c785b1-167f-471f-92a3-70d1cd47447e", "node_type": "1", "metadata": {"source": "cB - Careers challenge - Procrastination - W4.docx"}, "hash": "a2bbd2655118c067b05daf349f8a31d3073f31d08ff3db343b9d319ffd9d3f17", "class_name": "RelatedNodeInfo"}}, "text": "Remember, the goal of this challenge is not just to complete the task, but to gain insights into your procrastination habits and develop strategies for improvement. Take this opportunity to learn more about yourself and how you can better manage your time and responsibilities i.e staying true to set deadlines.\nSubmission\nYour responses should be on a maximum of 10 slides PPT. Convert it into PDF and submit the link on Tenx.\n\nUsefulness in life\nUnderstanding procrastination offers valuable insights into individual behaviour and habits, fostering self-awareness and personal growth while enabling effective time management strategies. Learning about procrastination empowers individuals to break tasks into manageable steps, utilise time-blocking techniques, and maintain focus on long-term goals, ultimately increasing their likelihood of success in both personal and professional endeavours.\n\nRubrics\n1. Originality and Personalization of Procrastination Story: Grading will focus on the originality and personalization of the procrastination story, assessing how well the trainee captures their own experiences and conveys them effectively.\n\n2. Identification and Analysis of Distractions: Evaluation will assess the trainee's ability to identify and analyze external factors or distractions contributing to procrastination. Five distractions should be listed with explanations of how they derail focus and contribute to procrastination.\n\n3. Excuses or Justifications for Procrastination: Grading will consider the honesty and depth of reflection on the excuses or justifications used to rationalize procrastination behavior. Four excuses or justifications should be listed with explanations of the thought patterns or beliefs enabling procrastination habits.\n\n4. Reflection on Negative Consequences of Procrastination: Evaluation will assess the trainee's reflection on past instances of procrastination, focusing on whether there were negative consequences or missed opportunities. Explanations should be provided to illustrate the impact of procrastination.\n\n5. Strategies for Avoiding Procrastination: Grading criteria will include the effectiveness and variety of strategies listed for avoiding procrastination. The trainees should list as many strategies as possible and provide detailed explanations of how each strategy works and how it has been successfully applied in the past.\n\n6. Overall Reflection and Self-Analysis: An overall assessment of the trainee's reflection and self-analysis regarding their procrastination habits will be conducted. The depth of reflection, self-awareness, and insights gained will be considered in the grading process.", "start_char_idx": 2952, "end_char_idx": 5618, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27dd70b0-b8b8-43d1-860e-14b506ce2d09": {"__data__": {"id_": "27dd70b0-b8b8-43d1-860e-14b506ce2d09", "embedding": null, "metadata": {"source": "W0 - Careers Exercise - Ideas to change the world-day4.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "72ee50bd-91ae-40bf-9d6d-f0729f591944", "node_type": "4", "metadata": {"source": "W0 - Careers Exercise - Ideas to change the world-day4.docx"}, "hash": "0b4a220d507759c53976a128aad402587d54be8299badc906f5db20005e84f20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de4e5b1e-4261-4722-94b4-321670498b49", "node_type": "1", "metadata": {}, "hash": "6490927254d8c4c6cf76c024c4131a8a00fc326b3b9ae52161806eed527c2098", "class_name": "RelatedNodeInfo"}}, "text": "Challenge\nIn your application, there was a question that said: \"tell us how you will use Generative AI to impact 5M young people in Africa in the coming 5 years. Be specific and describe how you will measure the impact of your idea, what role technology will play and where technology will play a role.\u201d  \n\nExtend the answer you provided to that question into a Storyboard or PPT presentation form of 6 slides maximum which would be suitable for sharing with others at 10 Academy to encourage them to join you to start implementing this project.\n\n\nMake sure that your presentation answers the following questions:\nState the problem that you are addressing, why you chose that problem among many others and as well as the field/industry/sector it is in. \nYour answer should clearly explain the problem you are solving and why you have chosen to solve this problem. Illustrate, using at least 1 data source, the scale of the problem - this problem should affect at least 5 million people globally. Mention if this issue is occurring only in your country/region? If yes, identify why it is only occurring in your country/region. If no, identify the other areas in the world where it is also occurring. When you answer, ask yourself, can this problem be solved by Gen AI solutions? \n\nHow would you solve this problem? What approach would you use and why? Is your proposed solution realistic in your country/region\u2019s current economic state? Can your proposed solution be realistically implemented using Gen AI technology that already exists? \n\nWhat role will technology play in your proposed solution to this problem? Will this technology be sustainable, affordable, and will it be applicable to all regions/countries where this problem is occurring? Compare your proposed solution to current technology being used to combat this problem. \n\nFind past solutions to this problem and illustrate how your solution has improved on the original/previous.  Include this in your max 6 slides.\n\nIf you do not receive funding from your government, identify three funding sources who you can realistically approach to fund your solution to this problem.  Include this in your max 6 slides. \n\nFor this assignment, feel free to use both images and words to communicate and complete this exercise. \n\nWhile this exercise will challenge you to be creative about your presentation, it is important that the content you submit remain within the following parameters: \nUse bullet points, short sentences, and remember to use keywords.\nMake sure your presentation is easy to read- do not use too many different colours, or too many different fonts.\nProofread your content before submission.. \nThere are only five main points that you need to address, so do not add any irrelevant information to this presentation. \nMake sure you have a cover page, with your name.\nEach slide should be titled, to indicate which point you are addressing.\nYou can use images in this presentation, but make sure everything in the presentation is relevant to the content you are delivering.   \n\nHere is a guiding template to help you organise your presentation- link\nBrainstorming and implementation of ideas slides -link \n\nSupport, meeting, and tutorials:\nLinks have been provided, directing you to resources that you can use to create storyboards and PPT presentations.\nDesign thinking- storyboarding\nCanva- storyboarding\nPPT presentation- basics\nCreating a good ppt presentation\nHubspot marketing- powerpoint presentation\n50 Powerpoint ideas\nHow to achieve flow during a presentation. \nHow to create a storyboard using Powerpoint. \nHow to storyboard: Our 4 step guide to the storyboarding process. \nHow to turn your essay into a Powerpoint. \n\nA tutorial session is planned on Thursday (11th April 2024)  to discuss this further and to answer any questions. Contact Margaret or Pascaline on Slack if you have any questions or concerns. \nThe first hand-in will result in feedback from your tutors, when your assignments will be handed back to you. \nYou are expected to listen to the feedback, and edit your assignments in an adequate response to the feedback you receive. \nYour final mark will be based on both your assignment and how you received and responded to feedback. \n\nMarking Rubric\n\nSlide structure - titles, references, slide, layout, font cohesion, wordiness  - 10%\nQuality of writing - use of English, comprehension , grammar, punctuation, tone and delivery, error free - 40 %\nProblem analysis - Has the individual analysed the problem well? Do they have a clear understanding of the problem? Is their analysis convincing? Is it logical? To what extent has the idea been thought-through?- 30%\nOverall Presentation- Does the assignment adhere to the guidelines of the exercise? Are the sentences short, and did the individual use bullet points? Has the assignment been proofread? Is it presentable ? - 20%", "start_char_idx": 1, "end_char_idx": 4873, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de4e5b1e-4261-4722-94b4-321670498b49": {"__data__": {"id_": "de4e5b1e-4261-4722-94b4-321670498b49", "embedding": null, "metadata": {"source": "W0 - Careers Exercise - Ideas to change the world-day4.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "72ee50bd-91ae-40bf-9d6d-f0729f591944", "node_type": "4", "metadata": {"source": "W0 - Careers Exercise - Ideas to change the world-day4.docx"}, "hash": "0b4a220d507759c53976a128aad402587d54be8299badc906f5db20005e84f20", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27dd70b0-b8b8-43d1-860e-14b506ce2d09", "node_type": "1", "metadata": {"source": "W0 - Careers Exercise - Ideas to change the world-day4.docx"}, "hash": "b03cd2f86f81266c433849fe78622c90d68ab6bc09c472d2db62df08b1a4861f", "class_name": "RelatedNodeInfo"}}, "text": "Note - Individual feedback will be given. \nUsefulness in real life\nThis exercise is designed to help you identify and recognize challenges that require modern day solutions. To help you develop realistic solutions to problems, and help you articulate and plan your solutions. Also to improve your writing skills, and challenge you to think critically about global issues and how they impact you.", "start_char_idx": 4876, "end_char_idx": 5271, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8eabfcb-05a3-40fa-bbf4-08deaec84d86": {"__data__": {"id_": "f8eabfcb-05a3-40fa-bbf4-08deaec84d86", "embedding": null, "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7", "node_type": "4", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "68f856b758cb7400ed3593c0f2c6177bd94e422d13819e04be4d1a8579d02ada", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f37d496b-4500-4fce-a23b-f42180c83aa6", "node_type": "1", "metadata": {}, "hash": "4d9042b161166d0fe124c1c27353f92f61b9a7114dae66630fe173ba372e0f11", "class_name": "RelatedNodeInfo"}}, "text": "Reverse Engineered Brief for Existing Telegram Ads  \n\n\nDMC Real State\nAd Copy (Content)\nTag line - \u201c \u1264\u1275 \u1260300,000 \u1265\u122d \u201d\nPost Content - \n\u201c \u1273\u120b\u1245 \u1245\u1293\u123d \u12a8\u12f2 \u12a4\u121d \u1232 (DMC) \u122a\u120d \u12a5\u1235\u1274\u1275 5% \u12a8\u1348\u1208\u12cd \u12ed\u1218\u12dd\u1308\u1261 \u12e8\u121a\u1348\u120d\u1309\u1275\u1295 \u1264\u1275 \u12ed\u121d\u1228\u1321 \n\u12a8 \u1235\u1272\u12f5\u12ee \u12a5\u1235\u12a8 \u1263\u1208 4 \u1218\u129d\u1273 \u1264\u1276\u127d \u1260\u1270\u1208\u12eb\u12e8 \u12e8\u12ab\u122c \u12a3\u121b\u122b\u132d \u12d8\u1218\u1293\u12ca \u12a0\u1353\u122d\u1275\u1218\u1295\u1276\u127d\u1295 \u1260\u1218\u1238\u1325 \u120b\u12ed \u1290\u1295\u1362 \n\ud83d\udc49Studio 56.6 \u12ab\u122c\n\ud83d\udc49\u1263\u1208 1\u1218\u129d\u1273 77.3\u12ab\u122c\n\ud83d\udc49\u1263\u1208 2 \u1218\u129d\u1273 123\u12ab\u122c\n\ud83d\udc49\u1263\u1208 2 \u1218\u129d\u1273 132.48\u12ab\u122c\n\ud83d\udc49\u1263\u1208 3 \u1218\u129d\u1273 146\u12ab\u122c - \u1263\u1208 4 \u1218\u129d\u1273 186 \u12ab\u122c \u12f5\u1228\u1235 \u1260\u12a3\u121b\u122b\u132d!", "start_char_idx": 0, "end_char_idx": 375, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f37d496b-4500-4fce-a23b-f42180c83aa6": {"__data__": {"id_": "f37d496b-4500-4fce-a23b-f42180c83aa6", "embedding": null, "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7", "node_type": "4", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "68f856b758cb7400ed3593c0f2c6177bd94e422d13819e04be4d1a8579d02ada", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8eabfcb-05a3-40fa-bbf4-08deaec84d86", "node_type": "1", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "7230745e063346a77dccfed422c29a04d1a694b38451f533da23d446f2b7d4c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d145b5c1-7984-4215-b3c2-2f37e3b1f599", "node_type": "1", "metadata": {}, "hash": "780977d0429caa7a0c9132be4a91cb1100fe20d2d123922b13e18ca5f9a3fa69", "class_name": "RelatedNodeInfo"}}, "text": "\u1231\u1246\u127d\u1295 \u12a828 \u12ab\u122c \u12a5\u1235\u12a8 920\u12ab\u122c \u12a8\u130d\u122b\u12cd\u1295\u12f5 \u12a5\u1235\u12a8 4\u1270\u129b \n\u260e\ufe0f\u1260+251908886615 \u12ed\u12f0\u12c9\u1209\n\ud83d\udeb0 \u12e8\u12a8\u122d\u1230 \u121d\u12f5\u122d \u12cd\u1203\n\ud83d\udc6e\u12e824 \u1230\u12d3\u1275 \u12e8\u12ab\u121c\u122b \u1325\u1260\u1243\n\ud83c\udfa2 \u1230\u134a \u12e8\u1218\u132b\u12c8\u127b \u1235\u134d\u122b\n\ud83d\ude9f \u12e8\u1235\u120d\u12ad\u1293 \u12e8\u12a2\u1295\u1270\u122d\u1294\u1275 \u1218\u1235\u1218\u122d \u12e8\u1270\u12d8\u1228\u130b\u1208\u1275\n\ud83d\uded7 4 \u12d8\u1218\u1293\u12ca \u12a0\u1233\u1295\u1230\u122e\u127d \u12e8\u1270\u1308\u1320\u1219\u1208\u1275\n\ud83d\udd0c \ud83d\ude98\u12e8\u12a4\u120c\u12ad\u1275\u122a\u12ad \u1218\u12aa\u1296\u127d \u127b\u122d\u1305 \u121b\u12f5\u1228\u130a\u12eb \u1266\u1273 \u12e8\u1270\u12d8\u130b\u1300\u1208\u1275\n\u26a1\ufe0f\u1300\u1290\u122c\u1270\u122d \u12eb\u1208\u12cd\n\u2666\ufe0f\u12a8\u1260\u1242 \u12e8\u1218\u12aa\u1293 \u121b\u1246\u121a\u12eb \u1235\u134d\u122b \u130b\u122d\n\ud83d\udc86\u1302\u121d:\u12e8\u12cd\u1260\u1275 \u1233\u120e\u1295 \u12a5\u1293 \u122c\u1235\u1276\u122b\u1295\u1276\u127d\u1295 \u12e8\u12eb\u12d8\n\ud83d\udc68\u200d\ud83d\udc67\u200d\ud83d\udc66 \u1208\u1218\u1296\u122a\u12eb \u121d\u1279 \u1260\u1206\u1290 \u12a0\u12ab\u1263\u1262 \u12e8\u1270\u1308\u1290\u1263 \u12a5\u1293 \u1260\u12a0\u1245\u122b\u1262\u12eb\u12cd \u12a0\u1235\u1348\u120b\u130a \u12a0\u1245\u122d\u1266\u1276\u127d\u1295 \u12e8\u121a\u12eb\u1308\u1299\u1260\u1275\n\ud83d\udc77 \u1260\u12d8\u1218\u1293\u12ca Aluminum Formwork Construction \u12e8\u130d\u1295\u1263\u1273 \u1325\u1260\u1265 \u12e8\u121a\u1308\u1290\u1263\n\u1208\u1260\u1208\u1320 \u1218\u1228\u1303 \n\u260e\ufe0f\u1260+251908886615 \u12ed\u12f0\u12c9\u1209\n @dmcrealestateplc \u201d\n\nReconstructed Brief (Can be used as Template)\n\nPost title:-\nDMC \u122a\u120d \u1235\u1274\u1275 \u1273\u120b\u1245 \u1245\u1293\u123d\nBackground.", "start_char_idx": 376, "end_char_idx": 899, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d145b5c1-7984-4215-b3c2-2f37e3b1f599": {"__data__": {"id_": "d145b5c1-7984-4215-b3c2-2f37e3b1f599", "embedding": null, "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7", "node_type": "4", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "68f856b758cb7400ed3593c0f2c6177bd94e422d13819e04be4d1a8579d02ada", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f37d496b-4500-4fce-a23b-f42180c83aa6", "node_type": "1", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "04d9b5d81f1cd869b97ad022e0d146bce7566c8929ab8acec480c69888d9e7e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21f68195-32d4-4909-92be-0b6c1b099fec", "node_type": "1", "metadata": {}, "hash": "e460a1b14d3e835c5bdab982ff92ada7321ada1ddeca496074e1d7fa6ff2f600", "class_name": "RelatedNodeInfo"}}, "text": "\u12e8\u122a\u120d\u1235\u1274\u1275 \u130d\u1295\u1263\u1273 \u12a5\u1293 \u123d\u12eb\u132d\nObjective\n\u1273\u120b\u1245 \u1245\u1293\u1239\u1295 \u1260\u1218\u1320\u1240\u121d \u12e8\u1264\u1276\u127d\u1295 \u123d\u12eb\u132d \u121b\u1233\u12f0\u130d\nTarget Audience\n\u1260\u12a0\u1290\u1235\u1270\u129b \u12c8\u132a \u1264\u1275 \u1218\u130d\u12db\u1275 \u12e8\u121a\u1348\u120d\u130d \u1264\u1270\u1230\u1266\u127d\nPromise\n\u12e8\u1270\u123b\u1208 \u12a0\u1297\u1297\u122d \u12a8\u1270\u1208\u12eb\u12e9 \u12a0\u1308\u120d\u130d\u120e\u1276\u127d \u130b\u122d\nSupport for your promise\n\u12e8\u12a8\u122d\u1230 \u121d\u12f5\u122d \u12cd\u1203\n\u12e824 \u1230\u12d3\u1275 \u12e8\u12ab\u121c\u122b \u1325\u1260\u1243\n\u1230\u134a \u12e8\u1218\u132b\u12c8\u127b \u1235\u134d\u122b\n \u12e8\u1235\u120d\u12ad\u1293 \u12e8\u12a2\u1295\u1270\u122d\u1294\u1275 \u1218\u1235\u1218\u122d \u12e8\u1270\u12d8\u1228\u130b\u1208\u1275\n\u1302\u121d:\u12e8\u12cd\u1260\u1275 \u1233\u120e\u1295 \u12a5\u1293 \u122c\u1235\u1276\u122b\u1295\u1276\u127d\u1295 \u12e8\u12eb\u12d8\nKey message\n\u12a8 DMC \u1264\u1275 \u12a5\u1295\u12f2\u1308\u12d9 \u121b\u12f5\u1228\u130d\n\n\n\nSafaricom M-pesa\n      Ad Content: \nTag line - \u201c \u1270\u1228\u12ad \u1260 M-pesa \u201d \u1363\n\u201c \u12a81 \u121a\u120a\u12ee\u1295 \u1260\u120b\u12ed \u1270\u1238\u120b\u121a\u12ce\u127d \u201d\n\u201c \u12a5\u1295\u1218\u12dd\u1308\u1265\u1363 \u12a5\u1295\u1308\u1260\u12eb\u12ed\u1363 \u12a5\u1290\u1238\u1208\u121d \u201d\nContent - \n\u201c M-PESA\u1295 \u1260\u121b\u12cd\u1228\u12f5 \u1364 \u12a0\u12f3\u12f2\u1235 \u1218\u12aa\u1296\u127d\u1363 \u1263\u1303\u1306\u127d\u1295\u1363 \u12a5\u1295\u12f2\u1201\u121d \u120c\u120e\u127d\u121d \u1265\u12d9 \u123d\u120d\u121b\u1276\u127d\u1295 \u12e8\u121b\u1238\u1290\u134d \u12d5\u12f5\u120d \u12a5\u1293\u130d\u129d\u1362 \n\u12a5\u1295\u12f4\u1275 \u12a5\u1293\u1238\u1295\u134b\u1208\u1295?", "start_char_idx": 900, "end_char_idx": 1411, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21f68195-32d4-4909-92be-0b6c1b099fec": {"__data__": {"id_": "21f68195-32d4-4909-92be-0b6c1b099fec", "embedding": null, "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7", "node_type": "4", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "68f856b758cb7400ed3593c0f2c6177bd94e422d13819e04be4d1a8579d02ada", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d145b5c1-7984-4215-b3c2-2f37e3b1f599", "node_type": "1", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "50b6f94fbeb4f02fe7d41e5ad3dd44acdec3a0888807dd5a2f522e4812464782", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14ebf21a-8198-4ee7-915a-268a0baa80cd", "node_type": "1", "metadata": {}, "hash": "9b5a89cd293df3493a75554bbf9eb0a449b818db91e4ed80ebb4562cbe2290fe", "class_name": "RelatedNodeInfo"}}, "text": "\u2022 M-PESA \u120b\u12ed \u1260\u1218\u1218\u12dd\u1308\u1265\n\u2022 \u1308\u1295\u12d8\u1265 \u12c8\u132a \u12a5\u1293 \u1308\u1262 \u1260\u121b\u12f5\u1228\u130d\n\u2022 \u12e8\u12a0\u12e8\u122d \u1230\u12d3\u1275 \u12a5\u1293 \u1325\u1245\u120e\u127d\u1295 \u1260\u1218\u130d\u12db\u1275\n\u2022 \u1208\u1290\u130b\u12f4\u12ce\u127d \u12a5\u1293 \u1208\u1262\u12dd\u1290\u1236\u127d \u1260M-PESA \u1260\u1218\u12ad\u1348\u120d\n\u2022 \u1308\u1295\u12d8\u1265 \u1260\u1218\u120b\u12ad \n\u2022 \u1308\u1295\u12d8\u1265 \u12a8\u1263\u1295\u12ad \u12c8\u12f0 M-PESA \u12a5\u1293 \u12a8 M-PESA \u12c8\u12f0 \u1263\u1295\u12ad \u1260\u121b\u1235\u1270\u120b\u1208\u134d \n\u2022 \u12a8\u12cd\u132d \u1200\u1308\u122d \u1260M-PESA \u1308\u1295\u12d8\u1265 \u1260\u1218\u1240\u1260\u120d\nM-PESA \u120b\u12ed \u12a5\u1295\u1218\u12dd\u1308\u1265\u1363 \u1260M-PESA \u12a5\u1295\u1308\u1260\u12eb\u12ed\u1363 \u1240\u1323\u12ed \u12a5\u12f5\u1208\u129b \u12a0\u1238\u1293\u134a\u12ce\u127d \u12a5\u1295\u1201\u1295\u1362\n\ud83d\udd17 \u12e8M-PESA \u1233\u134b\u122a\u12ae\u121d\u1295 \u1218\u1270\u130d\u1260\u122a\u12eb \u1260\u12da\u1205 \u120a\u1295\u12ad \u12eb\u12cd\u122d\u12f1: https://bit.", "start_char_idx": 1413, "end_char_idx": 1711, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14ebf21a-8198-4ee7-915a-268a0baa80cd": {"__data__": {"id_": "14ebf21a-8198-4ee7-915a-268a0baa80cd", "embedding": null, "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7", "node_type": "4", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "68f856b758cb7400ed3593c0f2c6177bd94e422d13819e04be4d1a8579d02ada", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21f68195-32d4-4909-92be-0b6c1b099fec", "node_type": "1", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "ae298c1323e1a69ef0ac46b38aa1c04e4af70748e090b6c10a6ef7387fee0799", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "944c9001-5ef6-486f-9882-cef669827d0f", "node_type": "1", "metadata": {}, "hash": "47580bbed17906e3fa00b0eba64878c3f40b7a1901bebb373870560c9c63310f", "class_name": "RelatedNodeInfo"}}, "text": "ly/M-PESA_SafaricomEthiopia \n\n#MPESASafaricom #TerekBeMPESA\n#FurtherAheadTogether \u201d\nReconstructed Brief (Can be used as Template)\n\nPost title:-\n\u1208M-pesa \u1218\u1270\u130d\u1260\u122a\u12eb \u1260\u1218\u1218\u12dd\u1308\u1265 \u1218\u1238\u1208\u121d\nBackground\n\u12e8\u1274\u120c\u12ae\u121d \u12a5\u1293 \u12e8\u12a8\u134d\u12eb \u1232\u1235\u1270\u121d \nObjective\n\u1260\u123d\u120d\u121b\u1275 \u1260\u121b\u1260\u1228\u1273\u1273\u1275 \u1208m-pesa \u12e8\u121a\u1218\u12d8\u1308\u1261 \u1230\u12ce\u127d\u1295 \u1218\u1328\u1218\u122d\nTarget Audience\n\u1235\u121b\u122d\u1275 \u1235\u120d\u12ad \u12eb\u120b\u1278\u12cd\u1293 \u1308\u1295\u12d8\u1265 \u12e8\u121a\u12eb\u1295\u12a8\u1233\u1245\u1231 \u1230\u12ce\u127d\nPromise\n\u12a8\u1270\u1218\u12d8\u1308\u1261 \u123d\u1208\u121b\u1275 \u12eb\u1308\u129b\u1209\u1363 \u1348\u1323\u1295 \u12a5\u1293 \u12a0\u1235\u1270\u121b\u121b\u129d \u12e8\u1308\u1295\u12d8\u1265 \u121b\u12d8\u12cb\u12c8\u122a\u12eb \u1218\u1270\u130d\u1260\u122a\u12eb \u12eb\u1308\u129b\u1209\nKey message\n\u1230\u12ce\u127d \u1208 m-pesa \u12a5\u1295\u12f2\u1218\u12d8\u1308\u1261 \u121b\u12f5\u1228\u130d\n  \n\n\n\n\nCash Go\n\t\n\tAd Content: \n\nTag line - \u201c \u1260\u121e\u1263\u12ed\u120d \u1235\u120d\u12ad\u12ce \u12a8\u12cd\u132a \u1203\u1308\u122d \u1260\u1290\u133b \u1308\u1295\u12d8\u1265 \u12ed\u120b\u12a9!", "start_char_idx": 1711, "end_char_idx": 2191, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "944c9001-5ef6-486f-9882-cef669827d0f": {"__data__": {"id_": "944c9001-5ef6-486f-9882-cef669827d0f", "embedding": null, "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7", "node_type": "4", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "68f856b758cb7400ed3593c0f2c6177bd94e422d13819e04be4d1a8579d02ada", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14ebf21a-8198-4ee7-915a-268a0baa80cd", "node_type": "1", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "3a087ffbf1768aa4b7b419af612e97c0fee82faa10d06f1390e561aa5e500803", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c96f8d42-e760-4e1a-baf9-75850d4c4e4d", "node_type": "1", "metadata": {}, "hash": "50b1e39e5a9ccc747cbaf31a6f19b4b7aa741101e6123261501d216ac52af81d", "class_name": "RelatedNodeInfo"}}, "text": "Cash Go\n\t\n\tAd Content: \n\nTag line - \u201c \u1260\u121e\u1263\u12ed\u120d \u1235\u120d\u12ad\u12ce \u12a8\u12cd\u132a \u1203\u1308\u122d \u1260\u1290\u133b \u1308\u1295\u12d8\u1265 \u12ed\u120b\u12a9! \u201d\nContent - \n\u201c CashGo (\u12ab\u123d \u130e) \n\u12a8\u12cd\u132d \u1200\u1308\u122b\u1275 \u1308\u1295\u12d8\u1265 \u1218\u120b\u12aa\u12eb \u12e8\u121e\u1263\u12ed\u120d \u1218\u1270\u130d\u1260\u122a\u12eb\n===========================\n\u1260\u121e\u1263\u12ed\u120d \u1235\u120d\u12ad\u12ce \u12a0\u121b\u12ab\u129d\u1290\u1275 \u126a\u12db\u1293 \u121b\u1235\u1270\u122d \u12ab\u122d\u12f5\u12ce\u1295 \u1260\u1218\u1320\u1240\u121d \n\u2022 \u12a8\u12cd\u132d \u1200\u1308\u122d \u1260\u1240\u1325\u1273 \u12c8\u12f0 \u12c8\u12f3\u1305 \u12d8\u1218\u12f5\u12ce \u12e8\u12a2\u1275\u12ee\u1335\u12eb \u1295\u130d\u12f5 \u1263\u1295\u12ad \u1202\u1233\u1265 \u12c8\u12ed\u121d \u12c8\u12f0 \u1232\u1262\u12a2 \u1265\u122d \u1202\u1233\u1265 \u1218\u120b\u12ad \u12ed\u127d\u120b\u1209\u1363 \u12a0\u120a\u12eb\u121d\n\u2022 \u1270\u1240\u1263\u12e9 \u12a81900 \u1260\u120b\u12ed \u1260\u121a\u1206\u1291\u1275 \u12e8\u1263\u1295\u12ab\u127d\u1295 \u1245\u122d\u1295\u132b\u134e\u127d \u1218\u1240\u1260\u120d \u12ed\u127d\u120b\u1209!", "start_char_idx": 2121, "end_char_idx": 2448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c96f8d42-e760-4e1a-baf9-75850d4c4e4d": {"__data__": {"id_": "c96f8d42-e760-4e1a-baf9-75850d4c4e4d", "embedding": null, "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7", "node_type": "4", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "68f856b758cb7400ed3593c0f2c6177bd94e422d13819e04be4d1a8579d02ada", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "944c9001-5ef6-486f-9882-cef669827d0f", "node_type": "1", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "1ff5e7865a07f789b4325ba66496f4a89793d2d853e5038c5d10b54a0f9744e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1a974ba-729b-4d25-bc00-5b1631ae6eed", "node_type": "1", "metadata": {}, "hash": "2218e741896e9f6401891e479a036bfeb6c2d0af5dff40b21473e6e45008d5d5", "class_name": "RelatedNodeInfo"}}, "text": "\u12e8CashGo \u121e\u1263\u12ed\u120d \u1218\u1270\u130d\u1260\u122a\u12eb\u1295 \u12a8 Play Store \u12c8\u12ed\u121d App Store \u1260\u121b\u12cd\u1228\u12f5 \u12ed\u1320\u1240\u1219\u1362 \n\u2022 \u1208\u12a0\u1295\u12f5\u122e\u12ed\u12f5 \u1235\u120d\u12ae\u127d \nhttps://play.google.com/store/apps/details?id=com.bankofabyssinia.cashgo&hl=en&gl=US\n\u2022 \u1208\u12a0\u1355\u120d \u1235\u120d\u12ae\u127d \nhttps://apps.apple.com/us/app/cashgo/id1559346306 \u201d\n\nReconstructed Brief (Can be used as Template)\n\nPost title:-\n\u1260\u12ab\u123d \u130e \u1218\u1270\u130d\u1260\u122a\u12eb \u1308\u1295\u12d8\u1265 \u1218\u120b\u12ad\nBackground\n\u12e8\u12a6\u1295 \u120b\u12ed\u1295 \u12ad\u1348\u12eb\nObjective\n\u12e8\u12ab\u123d \u130e\u1295 \u1218\u1270\u130d\u1260\u122a\u12eb \u121b\u12c8\u1228\u12f5 \u12a5\u1293 \u1308\u1295\u12d8\u1265 \u1218\u120b\u12ad\nTarget Audience\n\u12cd\u132a \u1203\u1308\u122d \u12d8\u1218\u12f5 \u12eb\u120b\u127d\u12cd \u1230\u12ce\u127d\nPromise\n\u1308\u1290\u12d8\u1265 \u12a8\u12cd\u132a \u1203\u1308\u122d \u1260\u134d\u1325\u1290\u1275 \u12a5\u1293 \u1260\u12a0\u1235\u1270\u121b\u121b\u129d \u1201\u1294\u1273 \u1218\u120b\u127d\nSupport for your promise\n\u126a\u12db \u12a5\u1293 \u121b\u1235\u1270\u122d \u12ab\u122d\u12f5 \u1260\u1218\u1320\u1240\u121d \u1218\u1235\u122b\u1271\n\u12a81900 \u1260\u120b\u12ed \u1245\u122d\u1295\u132b\u134e\u127d \u1308\u1290\u12d8\u1265 \u1218\u12cd\u1230\u12f5 \u1218\u127b\u1209 \nKey message\n\u1218\u1270\u130d\u1260\u122a\u12eb\u12cd\u1295 \u121b\u12c8\u1228\u12f5\u1363 \u1308\u1295\u12d8\u1265 \u12a8\u12cd\u132a \u1218\u120b\u12ad", "start_char_idx": 2449, "end_char_idx": 3027, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1a974ba-729b-4d25-bc00-5b1631ae6eed": {"__data__": {"id_": "f1a974ba-729b-4d25-bc00-5b1631ae6eed", "embedding": null, "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7", "node_type": "4", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "68f856b758cb7400ed3593c0f2c6177bd94e422d13819e04be4d1a8579d02ada", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c96f8d42-e760-4e1a-baf9-75850d4c4e4d", "node_type": "1", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "dc35979feb58e4464b5abd026c97e56875d58473afab0e9610c061fd432f82d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07e103ba-50e5-4ec1-9cc1-584ec91aa634", "node_type": "1", "metadata": {}, "hash": "4e8abfd32f1b1e4ef1b8d95115344f8493483ec5ddc258132a4bd1230b6badaa", "class_name": "RelatedNodeInfo"}}, "text": "Global Bank\n\nAd Content: \n\nTag line - \u1208\u130b\u122b \u1235\u12ac\u1273\u127d\u1295 \nContent:\n\u201c \u12c8\u12f0 \u121a\u1240\u122d\u1265\u12ce \u12e8\u130d\u120e\u1263\u120d \u1263\u1295\u12ad \u12a2\u1275\u12ee\u1335\u12eb \u1245\u122d\u1295\u132b\u134e\u127d \u130e\u122b \u1265\u1208\u12cd \u12e8\u1241\u1320\u1263 \u1212\u1233\u1265 \u1260\u1218\u12ad\u1348\u1275 \u12e8\u12a0\u1308\u120d\u130d\u120e\u1276\u127b\u127d\u1295 \u1270\u1320\u1243\u121a \u12ed\u1201\u1291\u1361\u1361\n\u1260\u121a\u1240\u122d\u1265\u12ce \u12a0\u121b\u122b\u132e\u127d \u12eb\u130d\u1299\u1295\u1361 https://bit.ly/3TkrrXD\n\u130d\u120e\u1263\u120d \u1263\u1295\u12ad \u12a2\u1275\u12ee\u1335\u12eb  \n\u1208\u130b\u122b \u1235\u12ac\u1273\u127d\u1295!\n#GlobalBankEthiopia #OurSharedSuccess #BankInEthiopia #Bank #GBE \u201d\n\t\nReconstructed Brief (Can be used as Template)\n\nPost title:-\n\u130d\u120e\u1263\u120d \u1263\u1295\u12ad\nBackground\n\u12e8\u1263\u1295\u12ad \u12a0\u1308\u120d\u130d\u120e\u1275\nObjective\n\u12e8\u1241\u1320\u1263 \u1212\u1233\u1265 \u121b\u1235\u12a8\u1348\u1275\nTarget Audience\n18 \u12a0\u1218\u1275 \u1260\u120b\u12ed \u1208\u1206\u1291 \u1201\u1209\u121d \u1230\u12ce\u127d\nPromise\n\u1348\u1323\u1295 \u12a5\u1293 \u12a0\u1235\u1270\u121b\u121b\u129d \u12e8\u1263\u1295\u12ad \u12a0\u1308\u120d\u130d\u120e\u1275\nSupport for your promise\n\u12e8\u1245\u122d\u1295\u132b\u134d \u1265\u12d9 \u1218\u1206\u1295\n\u12a81900 \u1260\u120b\u12ed \u1245\u122d\u1295\u132b\u134e\u127d \u1308\u1290\u12d8\u1265 \u1218\u12cd\u1230\u12f5 \u1218\u127b\u1209 \nKey message\n\u12a0\u12f2\u1235 \u12e8\u1241\u1320\u1263 \u1212\u1233\u1265 \u1218\u12ad\u1348\u1275\n\n\n\n\nDSTV\n\nAd Content:\n\nTag line - \u201c \u1260\u121b\u1295\u12aa\u12eb \u1232\u1209 \u1260\u132d\u120d\u134b \u1364 \u1260\u132d\u120d\u134b \u1232\u1209 \u1260\u12a0\u12ab\u134b! \u201d\n\u201c \u12a8\u1325\u122d 6 \u12a5\u1235\u12a8 \u1218\u130b\u1262\u1275 22 \u12f5\u1228\u1235 \u12a8\u121a\u1320\u1240\u1219\u1275 \u1353\u12ac\u1305 \u12a8\u134d \u12ab\u1209 \u1260\u12a5\u129b \u12c8\u132a \u12c8\u12f0 \u1240\u1323\u12e9 \u1275\u120d\u1245 \u1353\u12ac\u1305 \u1208\u12a0\u1295\u12f5 \u12c8\u122d \u12a8\u134d \u12ed\u120b\u1209!", "start_char_idx": 3033, "end_char_idx": 3745, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07e103ba-50e5-4ec1-9cc1-584ec91aa634": {"__data__": {"id_": "07e103ba-50e5-4ec1-9cc1-584ec91aa634", "embedding": null, "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7", "node_type": "4", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "68f856b758cb7400ed3593c0f2c6177bd94e422d13819e04be4d1a8579d02ada", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1a974ba-729b-4d25-bc00-5b1631ae6eed", "node_type": "1", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "813247ac8d2405af0f8d9e30880ec6839263986fce088ee1290aaa10f0d809a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd1ab8ec-ff13-4e1e-ae80-025843e92cde", "node_type": "1", "metadata": {}, "hash": "bad7109d4af773dcd023e82e5ddeee75bd2bc81235b862160333cb3808ca3ea2", "class_name": "RelatedNodeInfo"}}, "text": "\u201d\n\u201c \u12a8\u1325\u122d 6 \u12a5\u1235\u12a8 \u1218\u130b\u1262\u1275 22 \u12f5\u1228\u1235 \u12a8\u121a\u1320\u1240\u1219\u1275 \u1353\u12ac\u1305 \u12a8\u134d \u12ab\u1209 \u1260\u12a5\u129b \u12c8\u132a \u12c8\u12f0 \u1240\u1323\u12e9 \u1275\u120d\u1245 \u1353\u12ac\u1305 \u1208\u12a0\u1295\u12f5 \u12c8\u122d \u12a8\u134d \u12ed\u120b\u1209! \u201d\nContent - \n\u201c #DStv\n\ud83d\udce3 \u1230\u121d\u1270\u12cb\u120d?\n\u1260\u121b\u1295\u12aa\u12eb \u1232\u1209 \u1260\u132d\u120d\u134b \u1364 \u1260\u132d\u120d\u134b \u1232\u1209 \u1260\u12a0\u12ab\u134b!\n\ud83d\udc49 \u12a8\u1325\u122d 6 \u12a5\u1235\u12a8 \u1218\u130b\u1262\u1275 22 \u12f5\u1228\u1235 \u12a8\u121a\u1320\u1240\u1219\u1275 \u1353\u12ac\u1305 \u12a8\u134d \u12ab\u1209 \u1260\u12a5\u129b \u12c8\u132a \u12c8\u12f0 \u1240\u1323\u12e9 \u1275\u120d\u1245 \u1353\u12ac\u1305 \u1208\u12a0\u1295\u12f5 \u12c8\u122d \u12a8\u134d \u12ed\u120b\u1209! \u23f0 \u12ed\u1205 \u12a0\u1308\u120d\u130d\u120e\u1275 \u12ad\u134d\u12eb \u12a8\u1348\u1340\u1219 \u12a848 \u1230\u12d3\u1275 \u1260\u128b\u120b \u1270\u130d\u1263\u122b\u12ca \u12ed\u1206\u1293\u120d\u1362\n*\u12f0\u1295\u1265\u1293 \u1201\u1294\u1273\u12ce\u127d \u1270\u1348\u1343\u121a\u1290\u1275 \u12a0\u120b\u1278\u12cd\u1362\n\u12e8\u12f2\u12a4\u1235\u1272\u126a \u12a0\u1308\u120d\u130d\u120e\u1276\u127d\u1295 \u1208\u121b\u130d\u1298\u1275 \u12a8\u1273\u127d \u12eb\u1208\u12cd\u1295  \n\u12e8MyDStv Telegram \u120a\u1295\u12ad \u12ed\u132b\u1291!", "start_char_idx": 3665, "end_char_idx": 4013, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd1ab8ec-ff13-4e1e-ae80-025843e92cde": {"__data__": {"id_": "bd1ab8ec-ff13-4e1e-ae80-025843e92cde", "embedding": null, "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7", "node_type": "4", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "68f856b758cb7400ed3593c0f2c6177bd94e422d13819e04be4d1a8579d02ada", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07e103ba-50e5-4ec1-9cc1-584ec91aa634", "node_type": "1", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "ed445c58ed02963a0bfc93cc611449bf11c899f9333593ded25a5ec5821c9144", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a4d36f3-355f-4f48-8aa8-47c41173d166", "node_type": "1", "metadata": {}, "hash": "6d0d2c3cd5f3a4dc7525edc44e80a25acf9f18574c6bdcc14090bffca73f20d1", "class_name": "RelatedNodeInfo"}}, "text": "https://bit.ly/2WDuBLk\n#\u1201\u1209\u121d\u12eb\u1208\u12cd\u12a5\u129b\u130b\u122d\u1290\u12cd #DStvEthiopia #StepUp \u201d\n\nReconstructed Brief (Can be used as Template)\n\nPost title:-\n\u12f2\u12a4\u1235 \u1272\u126d \u1353\u12ac\u1305 \u123d\u120d\u121b\u1275 \nBackground\n\u12e8\u1218\u12dd\u1293\u129b \u1272\u126d \u127b\u1293\u120e\u127d \u121b\u1245\u1228\u1265\nObjective\n\u12f2\u12a4\u1235 \u1272\u126a \u1353\u12ac\u1305 \u12a8\u134d \u121b\u1235\u12f0\u1228\u130d \u12ab\u12f0\u1228\u1309 \u1218\u123d\u1208\u121d\nTarget Audience\n\u12f2\u12a4\u1235 \u1272\u126a \u12eb\u1235\u1308\u1261 \u1230\u12ce\u127d\nPromise\n\u1353\u12ac\u1305 \u12a8\u134d \u120b\u12f0\u1228\u1309 \u1260\u12a5\u129b \u12c8\u132a \u12c8\u12f0 \u1240\u1323\u12e9 \u1275\u120d\u1245 \u1353\u12ac\u1305 \u1208\u12a0\u1295\u12f5 \u12c8\u122d \u12a8\u134d \u12ed\u120b\u1209\nKey message\n\u12f2\u12a4\u1235 \u1272\u126a \u1353\u12ac\u1305 \u12a8\u134d \u121b\u1235\u12f0\u1228\u130d\u1363 \u12f2\u12a4\u1235 \u1272\u126a \u12ab\u120b\u1235\u1308\u1261 \u12a5\u1295\u12f2\u12eb\u1235\u1308\u1261\n\n\n\n\nLangano furniture\n\nAd Content: \n\nTag line - \u201c \u1208\u12d8\u1218\u1293\u12ca \u12e8\u1264\u1275 \u12a5\u1243\u12ce\u127d \u12a0\u121d\u122e\u1275\u12ce \u120b\u1295\u130b\u1296 \u1348\u122d\u1292\u1278\u122d! \u201d\nContent - \n\u201c \u1208\u12d8\u1218\u1293\u12ca \u12e8\u1264\u1275 \u12a5\u1243\u12ce\u127d \u12a0\u121d\u122e\u1275\u12ce \u120b\u1295\u130b\u1296 \u1348\u122d\u1292\u1278\u122d!\n\u1260\u1228\u1305\u121d \u130a\u12dc \u120d\u121d\u12f5 \u1260\u1270\u1230\u1229 \u1320\u1295\u12ab\u122b \u12a5\u1293 \u12e8\u1270\u1218\u1230\u12a8\u1228\u120b\u1278\u12cd \u12e8\u1348\u122d\u1292\u1278\u122d \u1235\u122b\u12ce\u127b\u127d\u1295 \u12a0\u1201\u1295\u121d \u1260\u1308\u1260\u12eb\u12cd \u120b\u12ed \u1290\u130d\u1230\u12cb\u120d!", "start_char_idx": 4014, "end_char_idx": 4566, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a4d36f3-355f-4f48-8aa8-47c41173d166": {"__data__": {"id_": "5a4d36f3-355f-4f48-8aa8-47c41173d166", "embedding": null, "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7", "node_type": "4", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "68f856b758cb7400ed3593c0f2c6177bd94e422d13819e04be4d1a8579d02ada", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd1ab8ec-ff13-4e1e-ae80-025843e92cde", "node_type": "1", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "d8e69f5bb686291deba6429627808e23334bcb730cccd60514761d407307ba99", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1f6190a-6a6d-4bb2-b4b7-a7f9f2ac96d3", "node_type": "1", "metadata": {}, "hash": "e51ce5bcb7d2a4e5d19a548ddc014235096ba8d59223c21b96a3cb9aa827d516", "class_name": "RelatedNodeInfo"}}, "text": "\u1260\u1228\u1305\u121d \u130a\u12dc \u120d\u121d\u12f5 \u1260\u1270\u1230\u1229 \u1320\u1295\u12ab\u122b \u12a5\u1293 \u12e8\u1270\u1218\u1230\u12a8\u1228\u120b\u1278\u12cd \u12e8\u1348\u122d\u1292\u1278\u122d \u1235\u122b\u12ce\u127b\u127d\u1295 \u12a0\u1201\u1295\u121d \u1260\u1308\u1260\u12eb\u12cd \u120b\u12ed \u1290\u130d\u1230\u12cb\u120d!\n\u12e8\u1348\u122d\u1292\u1278\u122d \u121d\u122d\u1276\u127b\u127d\u1295\u1361-\n\u2714\ufe0f \u120c\u12d8\u122d\u1293 \u12e8\u1328\u122d\u1245 \u1236\u134b\u12ce\u127d \n\u2714\ufe0f \u12e8\u12a0\u12cb\u1242 \u12a5\u1293 \u12e8\u1205\u133b\u1295 \u120d\u1305 \u12a0\u120d\u130b\u12ce\u127d \u1260\u1270\u1208\u12eb\u12e9 \u1218\u1320\u1295 \u12a5\u1293 \u12f2\u12db\u12ed\u1295 \n\u2714\ufe0f  \u1241/\u1233\u1325\u1296\u127d \u1260\u1270\u1208\u12eb\u12e9 \u12f2\u12db\u12ed\u1295\n\u2714\ufe0f \u12e8\u121d\u130d\u1265 \u1320\u1228\u1334\u12db\u12ce\u127d \u1260\u1270\u1208\u12eb\u12e8 \u12f2\u12db\u12ed\u1295\u1361-\u12606\u12c8\u1295\u1260\u122d,\u12608\u12c8\u1295\u1260\u122d\u1363\u12604\u12c8\u1295\u1260\u122d\n\u2714\ufe0f \u12e8\u1236\u134b \u1320\u1228\u1334\u12db\u12ce\u127d \u1260\u1270\u1208\u12eb\u12e9 \u1218\u1320\u1295\u1293 \u12f2\u12db\u12ed\u1295\n\u2714\ufe0f  \u12d8\u1218\u1293\u12ca \u12aa\u127d\u1296\u127d \n\u2714\ufe0f  \u12e8\u132b\u121b \u1238\u120d\u134d \u1260\u1270\u1208\u12eb\u12e9 \u1218\u1320\u1295\u1293 \u12f2\u12db\u12ed\u1295\u2026.\u12c8\u12d8\u1270 \u1232\u1206\u1295 \u1260\u12a5\u122d\u1235\u12ce \u134d\u120b\u1275 \u1218\u1230\u1228\u1275 \u12e8\u121a\u1240\u122d\u1261 \u12a0\u12f3\u12f2\u1235 \u12f2\u12db\u12ed\u1296\u127d\u1295 \u1270\u1240\u1265\u1208\u1295 \u12a5\u1293\u12d8\u130b\u1303\u1208\u1295\u1361\u1361\n\u120d\u12e9\u1290\u1273\u127d\u1295\n\u2666\ufe0f \u12a8\u130a\u12dc \u1260\u1283\u120b \u121b\u1233\u12f0\u1235 \u1262\u1348\u120d\u1309 \u12a5\u1295\u12f0 \u12a0\u12f2\u1235 \u12a0\u12f5\u122d\u1308\u1295 \u1218\u1233\u12f0\u1233\u127d\u1295\u1363\n\n\u2666\ufe0f \u12eb\u12d8\u12d9\u1275\u1295 \u1348\u122d\u1292\u1278\u122d \u1260\u1241\u122d\u1325 \u1240\u1320\u122e \u1240\u1295 \u12a5\u1293\u1240\u122d\u1263\u1295\u1363\n\u2666\ufe0f \u12a8\u129b", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1f6190a-6a6d-4bb2-b4b7-a7f9f2ac96d3": {"__data__": {"id_": "a1f6190a-6a6d-4bb2-b4b7-a7f9f2ac96d3", "embedding": null, "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7", "node_type": "4", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "68f856b758cb7400ed3593c0f2c6177bd94e422d13819e04be4d1a8579d02ada", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a4d36f3-355f-4f48-8aa8-47c41173d166", "node_type": "1", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "8fc19f022e19891b25d2b36aab334d3e3711d8a3ab74c0f67b7c5cca31d52dc7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77d9da54-7c42-4207-a8e7-caee2ac87d6a", "node_type": "1", "metadata": {}, "hash": "c03f57a63bd1baa28652106e73fe1c8d54b370d4a47a366e11cea08fd7d710aa", "class_name": "RelatedNodeInfo"}}, "text": "\u12a8\u130a\u12dc \u1260\u1283\u120b \u121b\u1233\u12f0\u1235 \u1262\u1348\u120d\u1309 \u12a5\u1295\u12f0 \u12a0\u12f2\u1235 \u12a0\u12f5\u122d\u1308\u1295 \u1218\u1233\u12f0\u1233\u127d\u1295\u1363\n\n\u2666\ufe0f \u12eb\u12d8\u12d9\u1275\u1295 \u1348\u122d\u1292\u1278\u122d \u1260\u1241\u122d\u1325 \u1240\u1320\u122e \u1240\u1295 \u12a5\u1293\u1240\u122d\u1263\u1295\u1363\n\u2666\ufe0f \u12a8\u129b \u12e8\u1308\u12d9\u1275\u1295 \u12d5\u1243\u12ce\u127d\u1295 \u12a8\u1228\u1305\u121d \u130a\u12dc \u1260\u1283\u120b \u1260\u12a0\u12f2\u1235 \u1218\u1208\u12c8\u1325 \u1232\u1348\u120d\u1309 \u12a5\u1295\u1240\u1260\u120b\u1208\u1295\n\u2666\ufe0f \u1218\u1313\u1313\u12e3\u1293 \u132b\u129d \u12a5\u1235\u12a8 \u1264\u1275\u12ce \u12f5\u1228\u1235 \u12a8\u129b\u12cd \u1218\u1206\u1291\u1363\n\u2666\ufe0f \u12a8\u12f0\u1295\u1260\u129e\u127d \u1208\u121a\u1240\u122d\u1261 \u1245\u122c\u1273\u12ce\u127d \u12a0\u134b\u1323\u129d \u121d\u120b\u123d \u1218\u1235\u1320\u1273\u127d\u1295\n\u2666\ufe0f \u1260\u121b\u1295\u129b\u12cd\u121d \u1260\u12d3\u120b\u1275 \u12c8\u1245\u1275 5% \u12e8\u12cb\u130b \u1245\u1293\u123d \u12e8\u121a\u1293\u12f0\u122d\u130d \u1218\u1206\u1293\u127d\u1295\u1363\n\u2666\ufe0f \u12a0\u12f2\u1235 \u12f0\u1295\u1260\u129b \u1260\u122d\u1236\u12ce \u1260\u12a9\u120d \u1232\u1218\u1323 \u121b\u1260\u1228\u1273\u127d \u12a0\u12d8\u130b\u1305\u1270\u1293\u120d\n\u1235\u1208\u1206\u1290\u121d \u1208\u12d8\u120b\u1242 \u1270\u1320\u1243\u121a\u1290\u1275 \u12a5\u1293 \u12a5\u122d\u12ab\u1273 \u12e8\u1200\u1308\u122d \u12cd\u1235\u1325 \u1348\u122d\u1292\u1278\u122d \u121d\u122d\u1276\u127d\u1295 \u12a8\u12a5\u129b \u12ed\u130d\u12d9\u1361\u1361\n\ud83d\uded1\u12a0\u12f5\u122b\u123b\u127d\u1295\u1361-\u1200\u12cb\u1233\n\u12a8\u1233\u1219\u12a4\u120d \u1308\u1260\u12eb \u12a0\u12f3\u122b\u123d \u1270\u123b\u130d\u122e / \u12cb\u1295\u12db \u12a0\u12f0\u1263\u1263\u12ed \u1233\u12ed\u12f0\u122d\u1231 \n\u1260\u1230\u120d\u12ad \u1241\u1325\u122e\u127b\u127d\u1295 \u13610916581884 /0907212223/0944221511\n\u12ed\u12f0\u12cd\u1209\u120d\u1295,\u12ed\u130e\u1260\u1299\u1295!", "start_char_idx": 4858, "end_char_idx": 5310, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77d9da54-7c42-4207-a8e7-caee2ac87d6a": {"__data__": {"id_": "77d9da54-7c42-4207-a8e7-caee2ac87d6a", "embedding": null, "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb63bed9-e96d-450a-befe-74d04be6c3d7", "node_type": "4", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "68f856b758cb7400ed3593c0f2c6177bd94e422d13819e04be4d1a8579d02ada", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1f6190a-6a6d-4bb2-b4b7-a7f9f2ac96d3", "node_type": "1", "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}, "hash": "25d3c9b59218f9bcd8aa21973eec23b45c4f2aa0193220e4be0a4ef047feed99", "class_name": "RelatedNodeInfo"}}, "text": "\u1308\u1260\u12eb \u12a0\u12f3\u122b\u123d \u1270\u123b\u130d\u122e / \u12cb\u1295\u12db \u12a0\u12f0\u1263\u1263\u12ed \u1233\u12ed\u12f0\u122d\u1231 \n\u1260\u1230\u120d\u12ad \u1241\u1325\u122e\u127b\u127d\u1295 \u13610916581884 /0907212223/0944221511\n\u12ed\u12f0\u12cd\u1209\u120d\u1295,\u12ed\u130e\u1260\u1299\u1295!    \t\n\u120b\u1295\u130b\u1296 \u1348\u122d\u1292\u1278\u122d\n\u1260\u1200\u12cb\u1233 \u12e8\u12a0\u1260\u122b \u120b\u1295\u130b\u1296 \u1348\u122d\u1292\u1278\u122d \u12a5\u1205\u1275 \u12f5\u122d\u1305\u1275 \u201d\nReconstructed Brief (Can be used as Template)\n\nPost title:-\n\u120d\u12e9 \u12e8\u1260\u12a0\u120d \u1273\u120b\u1245 \u1245\u1293\u123d \nBackground\n\u12e8\u1348\u122d\u1292\u1278\u122d \u1235\u122b \u12a5\u1293 \u123d\u12eb\u132d\nObjective\n\u12e8\u1348\u122d\u1292\u1278\u122d \u1235\u122b\u12ce\u127d\u1295 \u121b\u1235\u1270\u12cb\u12c8\u1245\u1363 \u123d\u12eb\u132d\u1295 \u121b\u1233\u12f0\u130d\nTarget Audience\n18 \u12a0\u1218\u1275 \u1260\u120b\u12ed \u1208\u1206\u1291 \u1201\u1209\u121d \u1230\u12ce\u127d\nPromise\n\u1325\u122b\u1275 \u12eb\u120b\u1278\u12cd \u12e8\u1348\u122d\u1292\u1278\u122d \u121d\u122d\u1276\u127d\nSupport for your promise\n\u120c\u12d8\u122d\u1293 \u12e8\u1328\u122d\u1245 \u1236\u134b\u12ce\u127d \n\u12e8\u12a0\u12cb\u1242 \u12a5\u1293 \u12e8\u1205\u133b\u1295 \u120d\u1305 \u12a0\u120d\u130b\u12ce\u127d \u1260\u1270\u1208\u12eb\u12e9 \u1218\u1320\u1295 \u12a5\u1293 \u12f2\u12db\u12ed\u1295 \n\u1241/\u1233\u1325\u1296\u127d \u1260\u1270\u1208\u12eb\u12e9 \u12f2\u12db\u12ed\u1295\nKey message\n\u12e8\u1348\u122d\u1292\u1278\u122d \u1218\u130d\u12db\u1275", "start_char_idx": 5217, "end_char_idx": 5696, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25eadb1e-1448-4df1-b9d3-69d08cfada22": {"__data__": {"id_": "25eadb1e-1448-4df1-b9d3-69d08cfada22", "embedding": null, "metadata": {"source": "Extra Tuition Deferral Scholarship - Cohort B.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c9745c1c-43b7-4c6f-b03e-9ed27d1b2c7c", "node_type": "4", "metadata": {"source": "Extra Tuition Deferral Scholarship - Cohort B.docx"}, "hash": "a90587571ac9fe6b364fc6f8da41e331c427770e88a0a8e81c426a248c737c28", "class_name": "RelatedNodeInfo"}}, "text": "Extra Tuition Deferral - Special Opportunity for Top Performers from Underrepresented Groups and Women\nAt 10 Academy, we believe in rewarding exceptional talent and dedication. That\u2019s why we offer an Extra Tuition Deferral opportunity for the top 10% of performers from underrepresented groups (Benin, Cameroon, Ghana, Kenya, Nigeria, Rwanda, Sudan, Uganda) and women during our assessment period.\nWe\u2019ll be offering the extra tuition deferral for top 2 women and top 2 men for cohort B.\nThese individuals demonstrate exceptional commitment, performance, and potential during the assessment period, typically Week 0 of our selection process.\nDifference between regular deferred payment program and tuition deferral scholarship\nEvaluation Criteria\nCandidates are evaluated based on various factors, including:\n\nSubmission Count: Applicants are assessed based on their completion of all required submissions.\nTask Completion: Performance on optional or bonus tasks within the weekly challenges is considered.\nAttendance Rate: An attendance rate of 80% or higher is expected to demonstrate commitment.\nSubmission Quality: Quality and relevance of submissions are evaluated for their depth and applicability.\nCommunity Activity: Participation in program activities, such as helping peers and actively engaging in discussions, is taken into account.\n\nPayment Detail\nThrough this scholarship, we empower exceptional individuals to use their full potential, regardless of financial circumstances.", "start_char_idx": 1, "end_char_idx": 1489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "450022b1-1ffe-416f-9f58-4e00afa23d5d": {"__data__": {"id_": "450022b1-1ffe-416f-9f58-4e00afa23d5d", "embedding": null, "metadata": {"source": "Week-0 Materials.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "983be719-4f7d-4b46-a2e0-de573c270f83", "node_type": "4", "metadata": {"source": "Week-0 Materials.docx"}, "hash": "1077bd643740c20f234c2bf59fb0baf0c0e58579e70fc022adbd43332d35ab66", "class_name": "RelatedNodeInfo"}}, "text": "Week 0 Materials - 10 Academy\nWelcome to Week 0 (Assessment Week)  of your training at 10 Academy! Access the materials for each session through the following links:\nDay 1 - Monday\nMorning Session: Introduction to the Challenge here\nAfternoon Session: Python Environment Setup - here, Git and Github, CI/CD - here &  Project Planning & EDA - Data Science Workflow using CRISP-DM and EDA techniques - here\nDay 2 - Tuesday\nMorning Session: Data Science Component Building (Architecture Designs, Wireframing, logic flow)\n(Link to materials will be added)\nAfternoon Session: Topic Modeling, Sentiment Analysis, Time Series Analysis & ML Engineering components\n(Link to materials will be added)\nDay 3 - Wednesday\nMorning Session: Working with SQL and NOSQL\n(Link to materials will be added)\nAfternoon Session: Database schema design\n(Link to materials will be added)\nDay 4 - Thursday\nMorning Session: Building Dashboards using Streamlit\n(Link to materials will be added)\nAfternoon Session: Introduction to Fullstack programming using React and Python\n(Link to materials will be added)\nDay 5 - Friday\nMorning Session: Introduction to Cloud Computing and terminologies & Introduction to Terraform\n(Link to materials will be added)\nAfternoon Session: Introduction to Kubernetes, Serverless, and distributed docker-based deployment\n(Link to materials will be added)\n\nFor any questions or support, reach out on Slack or via email rodas@10academy.org.", "start_char_idx": 0, "end_char_idx": 1440, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "34630ba6-49bb-4122-92eb-b42adf1f58c3": {"__data__": {"id_": "34630ba6-49bb-4122-92eb-b42adf1f58c3", "embedding": null, "metadata": {"source": "cB - Careers challenge - Proactivity - W6.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c368a139-fdd0-4cfd-862f-89dedca67464", "node_type": "4", "metadata": {"source": "cB - Careers challenge - Proactivity - W6.docx"}, "hash": "5c01e05c9b66a4788e040063334b0394a37d352091746b61d778d9a7b008667d", "class_name": "RelatedNodeInfo"}}, "text": "10 Academy cB : Week 6\nCareers - Exercise 1\n\nProactivity\n\nDue Date: Saturday, 1st June 2024, 8PM UTC\n\nBackground\nTo succeed in the workplace, it's essential to be proactive. This means taking the initiative, anticipating challenges, and finding solutions without being told what to do. Proactive employees are a valuable asset to any organisation, driving innovation, improving efficiency, and boosting productivity.\nBeing proactive is the first habit discussed in the 7 habits of highly effective people. It is more than just taking initiative; it's about taking responsibility for our own lives and choices. We have the power to choose our responses and make things happen. Proactive individuals recognize this responsibility and don't blame external circumstances for their behaviour. Instead, they make conscious choices based on their values, not their feelings.\nIn contrast, reactive individuals are often affected by their environment and the behaviour of others. They let external factors dictate their mood and performance. Proactive people, on the other hand, are driven by their values and can maintain a positive attitude and high performance regardless of the circumstances.\nTask\nYou're a Data Engineer at a company that provides data analytics services. Your boss, Rachel, sends you an email about a data pipeline issue:\n\"Hi Team,\nOne of our clients is experiencing issues with their data pipeline. The data is not being updated in real-time, and they're seeing delays of up to 24 hours. The client is frustrated and threatening to cancel their subscription. Additionally, the client mentioned:\nThe data is incomplete, with some tables missing entirely\nThe data schema has changed, but the pipeline hasn't been updated to reflect this\nThe client has tried to contact the support team multiple times, but received no response\nPlease investigate and resolve this issue ASAP. We can't afford to lose clients due to data pipeline issues and poor communication.\nBest,\nRachel\"\nReference Data:\nThe data pipeline is built using Apache Beam and Google Cloud Dataflow\nThe data is sourced from multiple databases and APIs\nThe client's data schema changed recently, but the pipeline wasn't updated\nThe support team has been overwhelmed with requests, leading to delayed responses\nExercise:\nAssuming you're the Data Engineer, create a reactive email and a proactive email reply to Rachel.\nWrite down and explain at least 4 differences you spot between your reactive email reply and your proactive email reply. \nShare a detailed story about a situation at 10 Academy where you took a proactive approach. What happened, what did you do, and how did it turn out? What three lessons did you learn from this experience that will help you continue to grow and improve in the future?\nShare a detailed story about a situation at 10 Academy where you reacted to a situation without thinking ahead. What happened, how did you respond, and what was the outcome? What lessons did you learn from this experience that will help you improve and become more proactive in the future?\nReflect on your typical behaviour in challenging situations. Do you tend to be proactive or reactive? What are your strengths and weaknesses in this regard? What strategies can you use to improve your proactive behaviour and minimise reactive responses?\n\n\n\nSubmission\nCreate a PowerPoint presentation with a maximum of 12 slides that detail your answers to the tasks written above.\n\n\n\nRubrics\n\nUsefulness in life", "start_char_idx": 0, "end_char_idx": 3480, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b452b052-3e9f-4ef5-a677-48b2239b1884": {"__data__": {"id_": "b452b052-3e9f-4ef5-a677-48b2239b1884", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 3.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "400ed0e3-96e8-4fac-b259-53fce81b308b", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 3.docx"}, "hash": "6a7d821e3bcbb783caa03ffb9a3ff9cc75616259150a5452e0d5b454b89ddab2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f525727f-f691-4885-815d-02d4eb24c55a", "node_type": "1", "metadata": {}, "hash": "a76d831c02d02044a6ffa42b397617a9772b00bcaafb3349f87cc3c0823d6ef8", "class_name": "RelatedNodeInfo"}}, "text": "p\n\n10 Academy Cohort B - Weekly Challenge: Week 3\nRedash chatbot add-on: LLM based chatbot for Advanced Data Analytics, Visualisation, and Automated Insight Extraction\nOverview\nBusiness Need\nOur company is seeking to significantly enhance its data analysis capabilities, specifically focusing on comprehensive YouTube data exploration. The aim of this project is to build a novel Redash chat add-on that our team members can use to extract insight from multiple Redash dashboards and from connected databases using natural language. The chat add-on enables a seamless  conversation in a question and answer format and autonomous knowledge discovery.  User queries could be about what is already displayed in the dashboard or questions that require generating SQL query using LLMs to be run against our connected databases. The end-to-end system you will help us build will empower and allow our team members to extract deep, meaningful, and actionable insights from our business intelligence (BI) platforms. \nOur company's BI dashboards are not only to give us monitoring capability of our business process, but also to help us transform the data we collect from multiple systems such as YouTube, Slack and Gmeet into actionable insights that can drive strategic decisions and offer a competitive edge in understanding digital content consumption trends.\nThe scope of this project extends to developing a Redash add-on in the frontend and an intelligence backend that translates user queries into one of the following \nSummary of visualisations in the current dashboard\nInsight about data returned by existing SQL queries\nAuto generate SQL queries and visualisations \nAuto generate new Redash dashboards from existing and auto generated SQL queries and its associated visualisations.\nThis tool will be a game-changer to BI and data analysis using Redash - helping translate natural language queries into complex SQL queries. This functionality is expected to democratize data analytics, allowing team members with non-technical backgrounds to easily extract and visualize data without deep knowledge of SQL. The integration of this add-on with Redash will streamline our analytical processes, making data exploration more accessible, efficient, and user-friendly.\nAdditionally, the backend API you will develop could be usable in many data analysis and BI projects. \nData\nVideo metadata for all videos uploaded in our YouTube channel\nTime Series viewership metadata for all videos uploaded in our YouTube channel\nTime Series comments for all our videos uploaded in our YouTube channel\nTime Series transcribed text for selected videos uploaded in our channel\nThis week data\nExpected Outcomes\nSkills:\nData Analysis and Visualization:\nProficiency in analysing time series data\nProficiency in EDA techniques.\nExpertise in creating intuitive and informative dashboards.\nProgramming and Development Skills:\nProficiency in Python and Javascript (React) programming languages\nProficiency in SQL \nProficiency in understanding code bases for complex software packages\nProficiency in Prompt Engineering\nProficiency in using OpenAI API\nExperience in developing add-ons or plugins\nSQL and Database Management \nExperience in running concurrent tasks using Celery \nExperience in deploying complex software package using Docker and docker-compose\nUI/UX Design\nKnowledge:\nNatural Language Processing (NLP) Knowledge:\nUse of vector databases\nMachine Learning and AI Knowledge\nTeam\nInstructors: \nYabebal\nEmtinan\nRehmet\nKey Dates\nDiscussion on the case - 9:00 UTC time on 07 May 2024.  Use #all-week-3 to ask questions.\nInterim Submission - 8:00 PM UTC time on Wednesday 08 May 2024.\nFinal Submission - 8:00 PM UTC time on Saturday 11 May 2024\nLeaderboard for the week\nThere are 100 points available for the week.\n20 points - community growth and peer support. \n\t13 points - technical public and group-based RC channels\nTotal number of messages (5)\nTotal number of Mentions (3)\nTotal number of DM connections (5) \n\t7 points - community activities\nNumber of messages in non-technical channels (4)\nOn-time presence in Gmeet sessions (3) \n30 points - presentation and reporting.\n\t15 points - interim submission. PDF\n    \n\t15 points for the final submission.  Blog entry or PDF with 5-8 pages.  \n50 points - Technical content\n\t20 points - Interim submission\nGithub link submission (20)\n\n30 points - Final submission \n\nGithub Link submission (25)   \n\nBadges\nEach week, one user will be awarded one of the badges below for the best performance in the category below.\nIn addition to being the badge holder for that badge, each badge winner will get +20 points to the overall score.", "start_char_idx": 0, "end_char_idx": 4654, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f525727f-f691-4885-815d-02d4eb24c55a": {"__data__": {"id_": "f525727f-f691-4885-815d-02d4eb24c55a", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 3.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "400ed0e3-96e8-4fac-b259-53fce81b308b", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 3.docx"}, "hash": "6a7d821e3bcbb783caa03ffb9a3ff9cc75616259150a5452e0d5b454b89ddab2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b452b052-3e9f-4ef5-a677-48b2239b1884", "node_type": "1", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 3.docx"}, "hash": "fc556ce1424987186acd62d6192ce1e34588150faab2d210d4c9a6df4043266b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3415edd-b73b-44c4-be73-c16c66c62240", "node_type": "1", "metadata": {}, "hash": "103b84ef8aa086ce3cf09d25e24ffe71d57e8e351b055e5cdd9c6dd6646ab2df", "class_name": "RelatedNodeInfo"}}, "text": "20 points - community growth and peer support. \n\t13 points - technical public and group-based RC channels\nTotal number of messages (5)\nTotal number of Mentions (3)\nTotal number of DM connections (5) \n\t7 points - community activities\nNumber of messages in non-technical channels (4)\nOn-time presence in Gmeet sessions (3) \n30 points - presentation and reporting.\n\t15 points - interim submission. PDF\n    \n\t15 points for the final submission.  Blog entry or PDF with 5-8 pages.  \n50 points - Technical content\n\t20 points - Interim submission\nGithub link submission (20)\n\n30 points - Final submission \n\nGithub Link submission (25)   \n\nBadges\nEach week, one user will be awarded one of the badges below for the best performance in the category below.\nIn addition to being the badge holder for that badge, each badge winner will get +20 points to the overall score.\nVisualization - the quality of visualizations, understandability, skimmability, choice of visualization\nQuality of code - reliability, maintainability, efficiency, commenting - in the future this will be CICD\nAn innovative approach to analysis -using latest algorithms, adding in research paper content and other innovative approaches\nWriting and presentation - clarity of written outputs, clarity of slides, overall production value\nMost supportive in the community - helping others, adding links, tutoring those struggling\nThe goal of this approach is to support and reward expertise in different parts of the Machine learning engineering toolbox.\nGroup Work Policy\nThis week, you are expected to complete the project with your assigned group. In the table below, your name is assigned to one of the groups we formed.\n\n\n\nLate Submission Policy\nOur goal is to prepare successful learners for a global level job. At work, deadlines are sometimes very strict - either you do it before the deadline or the company loses a substantial opportunity.  Moreover, the late communication behaviour (submission in 10 Academy can be considered as progress communication to team leads), blinds team leads and CEOs and is very determinantal in hindering the success of the company.\nWe have set our late submission as follows\nSubmissions are accepted only within the 12 hrs window - 17:00 UTC - 7:00 UTC  of the submission deadline\nFrequently late submissions (exceeding 6 total late submissions) will disqualify a person from the list of trainees 10 Academy recommends to partner employers.\nBadges will be rewarded for the cumulative on-time appearances (gmeet calls, on-time assignment submissions, and other places where being on-time is important) \n\n\nInstructions\nThe fundamental tasks in this week\u2019s challenge are the following \nThe workflow for this week's challenge is as follows\nUnderstand Redash and Data Exploration: Gain a comprehensive understanding of Redash's capabilities and data exploration requirements.\nCreate a Database Schema: Design a schema that is well-suited for YouTube data and analytics.\nOutline implementation: Develop an implementation plan for the Redash add-on and backend systems.\nPlan Integration of Large Language Models: Design the integration approach for large language models for natural language understanding.\nPlan your work and set up a development environment to assist in completing the project\nBuild frontend system\nImprove starter Redash chat add-on and make it Robust. You could try the following if you have time\nChat window is available in Redash query editor\nTemporary chat window pops up next to any visualisation in the Redash dashboard. Context for this temporary chat window is the data and query that generated the given visualisation.\nBuild Backend System Development,\nBuild a system that translates Natural Language questions in English to SQL \nAllow generation of relevant visualisation from existing or auto generated SQL queries \nAllow generation of Redash dashboards from natural language query\nSet up a GitHub repo, integrate unit testing and CICD for proper code package test and deployment\n\n\nPossible Work Plan\nStage 1: Basic Solution with Python and OpenAI API\n- Objective: Establish foundational chatbot capabilities for interpreting and translating natural language queries into SQL.\n- Python and OpenAI API: Utilize Python's robust data processing abilities and the OpenAI API for initial natural language understanding and SQL translation.\n- Implementation: Develop Python scripts for parsing YouTube data, generating SQL queries, and interfacing with Redash for visualizations. This stage addresses primary tasks like data exploration, schema creation, and basic language to SQL translation.\n\nStage 2: Enhanced NLP with LangChain Integration\n- Objective: Improve the chatbot's NLP capabilities to process more complex queries and provide accurate SQL translations.\n- LangChain: An intuitive open-source framework that enhances the functionality of large language models (LLMs), such as OpenAI or Hugging Face, for dynamic, data-responsive applications.\n- Implementation: Embed LangChain into the Python backend, replacing basic OpenAI API usage.", "start_char_idx": 3794, "end_char_idx": 8856, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3415edd-b73b-44c4-be73-c16c66c62240": {"__data__": {"id_": "b3415edd-b73b-44c4-be73-c16c66c62240", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 3.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "400ed0e3-96e8-4fac-b259-53fce81b308b", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 3.docx"}, "hash": "6a7d821e3bcbb783caa03ffb9a3ff9cc75616259150a5452e0d5b454b89ddab2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f525727f-f691-4885-815d-02d4eb24c55a", "node_type": "1", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 3.docx"}, "hash": "0c32ac4e4b1656c2cd935b0f1a53e251c3042158e150b7b08e49bbab099301b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f7a87bf-2e16-44fd-a3f1-d39ea3ba3eb9", "node_type": "1", "metadata": {}, "hash": "24a860beb60948ff9c9fc20fb74813631964f2e7b9f85830895c050a0500b4bb", "class_name": "RelatedNodeInfo"}}, "text": "Possible Work Plan\nStage 1: Basic Solution with Python and OpenAI API\n- Objective: Establish foundational chatbot capabilities for interpreting and translating natural language queries into SQL.\n- Python and OpenAI API: Utilize Python's robust data processing abilities and the OpenAI API for initial natural language understanding and SQL translation.\n- Implementation: Develop Python scripts for parsing YouTube data, generating SQL queries, and interfacing with Redash for visualizations. This stage addresses primary tasks like data exploration, schema creation, and basic language to SQL translation.\n\nStage 2: Enhanced NLP with LangChain Integration\n- Objective: Improve the chatbot's NLP capabilities to process more complex queries and provide accurate SQL translations.\n- LangChain: An intuitive open-source framework that enhances the functionality of large language models (LLMs), such as OpenAI or Hugging Face, for dynamic, data-responsive applications.\n- Implementation: Embed LangChain into the Python backend, replacing basic OpenAI API usage. This will enhance natural language understanding and processing, allowing for sophisticated reasoning and context-aware interactions with data.\n\nStage 3: Advanced Data Handling with LLamaIndex\n- Objective: Optimize data management for more efficient data retrieval and processing.\n- LLamaIndex: A data framework designed for LLM-based applications, offering tools for data ingestion, indexing, and querying, and facilitating the integration of various data sources.\n- Implementation: Implement LLamaIndex in the backend to streamline data access. It will enhance the chatbot\u2019s efficiency in data retrieval and query processing, particularly for the diverse data types associated with YouTube analytics.\n\nStage 4: Incorporation of Vector Databases for Semantic Search\n- Objective: Enhance the chatbot\u2019s semantic search capabilities for more relevant and contextual data retrieval.\n- Vector Databases: Specialized databases designed to handle high-dimensional vector data, crucial for semantic searches and understanding data in terms of semantic similarities.\n- Implementation: Utilize vector databases to store and search transcribed text, comments, and metadata from YouTube. This will significantly improve the chatbot\u2019s ability to deliver insightful responses based on semantic understanding.\n\nTask 1: Review the LLM Revolution and Plan your Work\nThere has been so much development in the area of Large Language and Multi-modal (text, image, audio, video) Models. It is paramount to have a basic understanding of key developments in this space to be able complete the current project. \n\nEnsure you understand the following key topics and tools\n\nOpenAI Tools\nOpenAI Chat Completion using ChatGPT (the first tool that starts the LLM Revolution)\nOpenAI Assistants, Threads, and Run\nOpenAI Function Call\nOpenAI Advanced Data Analysis \nOpenAI Code Interpreter\nOpenAI Actions (previously called Plugins)\n\nLangChain Components\nLangChain Tools\nLangChain Agents \nLangChain Memory\nLangChain Retrievers\nLangChain Adapters\n\nLLamaIndex Components \nLLamaIndex Data connectors\nLLamaIndex Query functions\nLLamaIndex Indexing\n\nVector Databases\nEmbedding Documents (image, text, audio, video)\nSemantic Search using Vector Similarity  \n\nBrowse the the following links and summarise your understanding in a report\nSQL generation using LLMs: Can LLMs Replace Data Analysts? Getting Answers Using SQL\nOpenai-cookbook examples e.g Backtranslation_of_SQL_queries.py\nExperiment quickly using Flowise AI\nPaper to read: InsightPilot: An LLM-Empowered Automated Data Exploration System\nLLMs and SQL Langchain Blog\nAwesome collection of tools, models, and ideas in the LLM Arena\nLlamaindex: An Imperative For Building Context-Aware Llm-Based Apps\n\nTask 2: Tool understanding and Data exploration \nUnderstanding Redash and Data Exploration:\nLearn about Redash's capabilities for data visualization and analytics.\nExplore how Redash can connect to various data sources, including YouTube analytics .\nCreate a Database Schema for YouTube Data and Analytics:\nDesign a schema to efficiently store and query YouTube data.https://docs.google.com/spreadsheets/d/1amkmUHkU06z_UaDMr9ROm7FNreRpAti_uzvSPISMJoY/edit?usp=sharing\nConsider aspects like channel performance, user base, and video expenses.\nTask 3: Building Redash Chat Add-on\nOutline the Architecture for the Redash Add-on and Backend Systems:\nPlan the structure of the Redash add-on and how it integrates with backend systems.\nDevelop and Implement the Dashboard Interface with the add-on\nDesign the UI of the dashboard, focusing on user-friendliness for Redash add-on.\nImplement the design using React and integrate it with Redash.\nDevelop the Backend System for Data Storage and Processing:\nBuild a system to store and process data, aligning with the designed database schema.\nEnsure efficient data retrieval and processing capabilities.", "start_char_idx": 7797, "end_char_idx": 12721, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f7a87bf-2e16-44fd-a3f1-d39ea3ba3eb9": {"__data__": {"id_": "4f7a87bf-2e16-44fd-a3f1-d39ea3ba3eb9", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 3.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "400ed0e3-96e8-4fac-b259-53fce81b308b", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 3.docx"}, "hash": "6a7d821e3bcbb783caa03ffb9a3ff9cc75616259150a5452e0d5b454b89ddab2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b3415edd-b73b-44c4-be73-c16c66c62240", "node_type": "1", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 3.docx"}, "hash": "e9fe85450184b674bc326920acfaede90ee7a4e3e0cea7f8b2fe0f8084800e04", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf883145-f96b-4c75-bd42-99f12ce780d9", "node_type": "1", "metadata": {}, "hash": "46a17924c2ab1578832312c2fa3886b1db78ec0864f6163e82e57db4325bcd74", "class_name": "RelatedNodeInfo"}}, "text": "Create a Database Schema for YouTube Data and Analytics:\nDesign a schema to efficiently store and query YouTube data.https://docs.google.com/spreadsheets/d/1amkmUHkU06z_UaDMr9ROm7FNreRpAti_uzvSPISMJoY/edit?usp=sharing\nConsider aspects like channel performance, user base, and video expenses.\nTask 3: Building Redash Chat Add-on\nOutline the Architecture for the Redash Add-on and Backend Systems:\nPlan the structure of the Redash add-on and how it integrates with backend systems.\nDevelop and Implement the Dashboard Interface with the add-on\nDesign the UI of the dashboard, focusing on user-friendliness for Redash add-on.\nImplement the design using React and integrate it with Redash.\nDevelop the Backend System for Data Storage and Processing:\nBuild a system to store and process data, aligning with the designed database schema.\nEnsure efficient data retrieval and processing capabilities.\nFor your python backend Framework, you may use Quart - the lightweight async version of Flask \nTask 4: LLM Understanding and Integration \nIntegrate Large Language Models for Natural Language Understanding:\nIncorporate language models to interpret and process natural language queries.\nFocus on translating these queries into actionable data insights.\nIntegrate the Add-on to Convert Natural Language Queries into SQL\nEnsure accurate translation and compatibility with the Redash environment.\n\nTask 5: Automatic Dashboard Generation\nImplement automatic visualisation generation based on user queries, generated SQL queries, and existing visualisation context.:\nImplement creating Redash dashboards using a collection of visualisations .\nTask 6: Blog Reporting\nWrite a blog-like report that details the process followed, challenges faced. dashboard building process, LLM integration and lessons learnt from this week\u2019s challenge.\nN.B for reporting\nYour report should start with the Introduction, the overall body of your report, and then a Conclusion.\n\nTutorials Schedule\nIn the following, the colour purple indicates morning sessions, and blue indicates afternoon sessions.\nTuesday: Understanding Redash\nHere the trainees will understand the week\u2019s challenge.\nIntroduction to Week Challenge (Yabebal)\nIntroduction to Building Redash Add-on using React (Rehmet)\n\nKey Performance Indicators:\n\nUnderstanding week\u2019s challenge\nUnderstanding the \u2018what is\u2019 Redash\nAbility to reuse previous knowledge\nWednesday: Redash source code review and setup \nHere the trainees will understand the components of chatbot and data analysis using OpenAi, LangChain, and LIDA.\n\nPrompt Engineering tools and ideas (Emtinan)\nBuilding chatbot Backend using LangChain (Emtinan)\nThursday: LLM usage and Integration \nHere the trainees will understand how advanced data analysis and automatic sql query  generation .works using LLMs\n\nAsynchronous execution for OpenAI API Streaming and Redash query execution (Rehmet)\nHow ADA, LIDA, AutoGPT, InsightPilot, and other LLM agents work (Rehmet)\nFriday: Concurrent execution for Robust Chatbot\nHere the trainees will understand how to do aysnc request to OpenAI API and execute SQL queries in Redash Celerey executer \n\nVector Database Layer for RAG Bots (Emtinan)\nSubmission \nInterim: Due Wednesday 08 May 20:00 UTC\nA Pdf report with an overview of your understanding of the key LLM tools and APIs\nReview of OpenAI technologies\nReview of LangChain components\nReview of Vector Databases\nReview of LLM based AI applications e.g Retrieval Augmented Generation s and Agents\nReview of Redash server, worker, and scheduler. Comment on the use/need for asynchronous computing  in chatbot development.\nA PDF report providing a concise and comprehensive analysis of the SQL query used to address Task 2, focusing on its efficiency and performance under scaling conditions. Comment on how long (time) will it take your query if the data size grows by 10x, 1000x, 100000x, 1000000x.\n\nGithub link submission that demonstrates:\nWork in progress for Redash chat add-on  frontend\nWork in progress for Redash chat add-on backend\nWork in progress for prompts to help generate SQL using OpenAI APIs\nWork in progress for \ndatabase schema design\ndata loading to database\nDockerfile and docker-compose based Redas installation\nCelery", "start_char_idx": 11829, "end_char_idx": 16048, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf883145-f96b-4c75-bd42-99f12ce780d9": {"__data__": {"id_": "cf883145-f96b-4c75-bd42-99f12ce780d9", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 3.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "400ed0e3-96e8-4fac-b259-53fce81b308b", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 3.docx"}, "hash": "6a7d821e3bcbb783caa03ffb9a3ff9cc75616259150a5452e0d5b454b89ddab2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f7a87bf-2e16-44fd-a3f1-d39ea3ba3eb9", "node_type": "1", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 3.docx"}, "hash": "36d330063d6f8e4f5e5c2d6cef464ab51385dbe2ea632466c66cfb467513dc77", "class_name": "RelatedNodeInfo"}}, "text": "Comment on the use/need for asynchronous computing  in chatbot development.\nA PDF report providing a concise and comprehensive analysis of the SQL query used to address Task 2, focusing on its efficiency and performance under scaling conditions. Comment on how long (time) will it take your query if the data size grows by 10x, 1000x, 100000x, 1000000x.\n\nGithub link submission that demonstrates:\nWork in progress for Redash chat add-on  frontend\nWork in progress for Redash chat add-on backend\nWork in progress for prompts to help generate SQL using OpenAI APIs\nWork in progress for \ndatabase schema design\ndata loading to database\nDockerfile and docker-compose based Redas installation\nCelery \n\n\nFeedback\nYou may not receive detailed comments on your interim submission but will receive a grade.\nInterim 2: Due Saturday 11 May 20:00 UTC\nPdf document (to be published) or a published Blog link detailing the process followed. This should include:\nThe business objective of the project\nThe project design \nThe tech-stack used\nThe methodologies followed\nThe challenges faced\nGithub link submission that includes the following\nWork in progress Redash Chat Add-on frontend code\nWork in progress Redash Chat Add-on backend code\nDatabase schema design\nData loading to database\nInstallation scripts (Dockerfile, docker-compose, bash/python/Makefile scripts to automate installation) and Readme explaining how to install and use\nWell demonstrated CI (frequent commits, multiple branches with good names, unit test and linting github actions) and CD (github actions to build docker images from PR to dev and prod branches)\nFinal: Due Tuesday 14 May 20:00 UTC\nPdf document (to be published) or a published Blog link detailing the process followed. This should include:\nThe business objective of the project\nThe project design \nThe tech-stack used\nThe methodologies followed\nThe challenges faced\nThe results obtained\nThe lessons learned\nLimitations and future plans\n\nGithub link submission that includes the following\nRedash Chat Add-on frontend code\nRedash Chat Add-on backend code\nInstallation scripts (Dockerfile, docker-compose, bash/python/Makefile scripts to automate installation) and Readme explaining how to install and use\nWell demonstrated CI (frequent commits, multiple branches with good names, unit test and linting github actions) and CD (github actions to build docker images from PR to dev and prod branches)\nDemonstration of coding best practices (Following python-openai or langchain code base coding style)\nFeedback\nYou will receive comments/feedback in addition to a grade.\n\n\nReferences\nKey References\nRedash ApI Usage \nPrompt engineering : Openai Prompt Engineering Guide\npromptingguide.ai: A prompt engineering guide that demonstrates many techniques.\nlearnprompting.org: An introductory course to prompt engineering.\nYouTube API reference\nHow to extract YouTube data \nYouTube data usage", "start_char_idx": 15354, "end_char_idx": 18254, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ad3ff9e-7acb-411f-bcd6-b3dc21f4a045": {"__data__": {"id_": "2ad3ff9e-7acb-411f-bcd6-b3dc21f4a045", "embedding": null, "metadata": {"source": "cB - Career Challenge - 3 Real World Jobs.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "281d72ae-706e-4730-9aa2-7fed4b3dcd2c", "node_type": "4", "metadata": {"source": "cB - Career Challenge - 3 Real World Jobs.docx"}, "hash": "377c3a6c2eefbf3598e1a05a1f5ad8a30ae6c9a51bfc8c0d0ce3481c6a69f5c5", "class_name": "RelatedNodeInfo"}}, "text": "10 Academy cB: Week 1\n\nCareers Challenge - 1\n3 real world jobs \n\nDeadline: 27th April 2024, 8pm UTC\n\nIntroduction\nIn preparation for your transition into the professional world, it's essential to understand the expectations of employers in your chosen career track of Data Engineering, Machine Learning and Gen AI Engineering. This exercise provides practical experience in researching job opportunities, analysing employer requirements, and reflecting on your own skills.\n\nBy exploring real-world job postings, you'll gain insights into the skills and qualifications sought by employers, helping you tailor your training efforts and increase your competitiveness in the job market as 10 Academy expects you to be while at the end of the training programme.\n\n\nInstructions:\n\nYour goal is to explore job opportunities in the field of DE, or ML Eng, or Gen AI Eng, and Find three (3) positions that you could potentially pursue by August 2024. Ensure that each position meets the following criteria:\n\nIt is currently available and seeking placement.\nIt is suitable for someone with your level of experience. \nRemote positions are encouraged. \nEach position must have an online job posting with a link to apply directly.\n\nCreate a google slide or PPT of 6 slides (strictly 6 slides). The first slide should be a cover page and the following 3 should be for each job you found, and then the final 2 slides should be of the 2nd and 3rd questions written down here.\n\n\n\n\n\nTask:\nIn the 3 middle slides of your PPT, provide the following details For each job you found:\n\nLink to the online job advertisement \nName of the company\nTitle of the position\nTeam you would be joining\nIs the role remote or hybrid or physical? \nPhysical location of the job (if applicable)\nApplications closing date (if provided)\nSkills requirements listed on the job application - (Abbreviations can be used)\nYears of experience requirements.\nRequirements on nationality, right-to-work, or related, (if any)\nSalary information (if known)  \nLinkedIn contact of the hiring manager (if available)\nAnalysis of key challenges you might face in securing this role by August 2024 after completing the training. \n\n\nWrite down the minimum of 3 differences between your current skills and the skills requirements of each of the  jobs you found. \n\nWrite 4 lessons learned from conducting this assignment and how it has influenced your career exploration journey. \n\n\nUsefulness in life\n\nThis exercise will challenge you to actively engage in the job search process, utilizing online resources to identify suitable positions, analyze job requirements, and reflect on how your own skills align with employer expectations. Through this hands-on exploration, you will not only enhance your understanding of the job market but also gain valuable insights into your own strengths and areas for growth.", "start_char_idx": 0, "end_char_idx": 2850, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66ca1fde-05a7-4c6a-a2e2-5ebbfe650f3f": {"__data__": {"id_": "66ca1fde-05a7-4c6a-a2e2-5ebbfe650f3f", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 6.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffe3dbcf-b7d0-4566-b4ae-d519caa9ed8a", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 6.docx"}, "hash": "31bc542e171273b78f244410549f00131e9a7be620b4d4c7b872a8ae317f648c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be69b7c1-ab89-4f61-807a-a9e778f60ae8", "node_type": "1", "metadata": {}, "hash": "bb030e04948ad9f481cdf9a9dd15671ef4f14fab7ba2569ccbd13349358bcfc7", "class_name": "RelatedNodeInfo"}}, "text": "10 Academy Cohort B\nWeekly Challenge: Week 6\nLLM Finetuning: Enabling Quality Embedding and Text Generation for Amharic, Swahili, and Yoruba Languages\nOverview\nBusiness Need\nAfroTech Solutions is an innovative African company that focuses on using AI to improve customer support and engagement. Our goal is to help African businesses by using new technology in AI. Our latest project is an AI-powered customer support system designed for the African market. By using advanced AI, this system aims to provide smooth, multilingual support across different platforms.\nThis project aims to make our customer support services better by using AI to generate text in Amharic, Swahili, and Yoruba. We plan to create systems that can generate quick and relevant responses in these languages based on customer questions and past interactions.\nFor this project to be successful, our customer support must be both efficient and meaningful to our diverse clients. To achieve this, the technology needs to have strong capabilities in text embedding and generation for Amharic, Swahili, and Yoruba. We will collect large datasets for each language and fine-tune suitable open-source LLM models based on the datasets collected in the previous week. For Amharic, we may use models like Nous Hermes Mistral 8 7B or amharic language finetuned version of  LLama 2 (Samuael/llama-2-7b-tebot-amharic or iocuydi/llama-2-amharic-3784m) -  and finetune it further to deliver the business objective. We will also select and fine-tune equivalent models for Swahili and Yoruba based on the collected data and the chosen language in the previous week to meet our business goals effectively.\n\nInspirations \nThe following works are our inspiration. We envision to collaborate with all stakeholders to create a robust quality LLM for the languages. \nLlama2-Chinese/README_EN.md at main \u00b7 FlagAlpha/Llama2-Chinese (github.com) \n\nData\nYou will use the data you collected in the previous week.\nExpected Outcomes\nSkills:\nExperience working with Huggingface APIs and platform \nFine-tuning and deploying LLMs\nExperience in using multiple GPUs for parallel training and inference  \nWorking with Deep Learning Frameworks \nAmharic, Swahili, and Yoruba text processing\nProficiency in Python programming language\nProficiency in Prompt Engineering\nKnowledge:\nUnderstanding Transformer Models and their components\nUnderstanding the building blocks of Instruction Based LLMs \nUnderstanding Chat Models such as ChatML chat template  \nNatural Language Processing (NLP) Knowledge\nMachine Learning and AI Knowledge\n\nTeam\nTutors: \nYabebal\nEmitinan\nRehmet\nBadges\nEach week, one user will be awarded one of the badges below for the best performance in the category below.\n\nIn addition to being the badge holder for that badge, each badge winner will get +20 points to the overall score.\n\nVisualization - quality of visualizations, understandability, skimmability, choice of visualization\nQuality of code - reliability, maintainability, efficiency, commenting - in future this will be CICD/CML\nInnovative approach to analysis -using latest algorithms, adding in research paper content and other innovative approaches\nWriting and presentation - clarity of written outputs, clarity of slides, overall production value\nMost supportive in the community - helping others, adding links, tutoring those struggling\n\nThe goal of this approach is to support and reward expertise in different parts of the Machine learning engineering toolbox.\nGroup Work Policy\nThis week, you are expected to complete the project with your assigned group. In the table below, your name is assigned to one of the groups we formed.\n\n\n\n\nInstructions\nThe rapidly evolving landscape of LLMs benefitted from the unprecedented scales of model size and training data producing models with strong capabilities, including  reasoning and learning from experience at levels surpassing humans\u2019. \n\nHowever, due to the high imbalance in training data (text sources from the internet), English dominates in these models. Models are not as proficient in other languages, especially low-resource languages that are absent from the multilingual training corpora. \n\nCollecting large-scale data for a  low-resource language and retraining an LLM can be prohibitively expensive due to computational and data collection costs. A better approach is transfer learning; transferring an LLM\u2019s capabilities from English to a non-English language through further pre-training and fine-tuning.", "start_char_idx": 1, "end_char_idx": 4483, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be69b7c1-ab89-4f61-807a-a9e778f60ae8": {"__data__": {"id_": "be69b7c1-ab89-4f61-807a-a9e778f60ae8", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 6.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffe3dbcf-b7d0-4566-b4ae-d519caa9ed8a", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 6.docx"}, "hash": "31bc542e171273b78f244410549f00131e9a7be620b4d4c7b872a8ae317f648c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66ca1fde-05a7-4c6a-a2e2-5ebbfe650f3f", "node_type": "1", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 6.docx"}, "hash": "eb653af2afa2a32321dd91483a1cfc162340bd943a8f15280d69173b91e0325b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32a9398f-0f4f-4a08-aff1-43c30a00529a", "node_type": "1", "metadata": {}, "hash": "a48939fc5531c645b8a885421d1feec46b03fb82b4cd222cbb5b5dbec437de3a", "class_name": "RelatedNodeInfo"}}, "text": "Instructions\nThe rapidly evolving landscape of LLMs benefitted from the unprecedented scales of model size and training data producing models with strong capabilities, including  reasoning and learning from experience at levels surpassing humans\u2019. \n\nHowever, due to the high imbalance in training data (text sources from the internet), English dominates in these models. Models are not as proficient in other languages, especially low-resource languages that are absent from the multilingual training corpora. \n\nCollecting large-scale data for a  low-resource language and retraining an LLM can be prohibitively expensive due to computational and data collection costs. A better approach is transfer learning; transferring an LLM\u2019s capabilities from English to a non-English language through further pre-training and fine-tuning.\n\nAs part of this challenge, you are required to do the following tasks \nUnderstand the LLM landscape as of Jan 2024\nUnderstand the building blocks of \nLLM base models (encoder only, encoder-decoder, decoder only)\nGPU memory needs (15-20GB, 20-80GB, >80GB), Numbers of GPUs, and time to full pretraining and finetuning.\nHow to finetune\nInstruct finetuning\nChat finetuning\nUnderstand the key components of LLM Training and Finetuning\nPre-training: self-supervised learning predicting the next word in a given context\nSupervised fine tuning (SFT)\nParameter-efficient Tuning (PEFT)\nLow-Rank Adaptation (LoRA)  \nOverview of best contender open source LLM models and their variations\nMistral:  7B, 8x 7b, \nLlama 2/3: 7b\nFalcon: 7b\nStable AI 2:  1.6B\nOpenLLama: 3B, 7B \nExplore huggingface documentation for inference and finetuning \nTest hugging face embedding examples on your local machine tasks\nTest hugging face small LLM models on your local machine\nTest hugging face modules for \nData loading, preprocessing, batching, and tokenizing \nLoading quantized models (BitsAndBytes)\nApplying parameter-efficient finetuining (PEFT) \nApplying LoRA \nGeneral techniques to reduce memory and finetune efficiently with a optimal tradeoff among, memory,  speed,  and accuracy\nUnderstand and Prepare the collected data for finetuning \nExplore provided data as well as what you can find in the web\nPrepare data to be ingested in your instruction-finetuning pipeline\nPrepare evaluation datasets to benchmark your finetuned model with baseline OpenAI/Huggingface deployed models\nSelect an open-source LLM model and finetune it \n\t\nTask 1: Literature Review & Hugging face ecosystem \nIn this task you are expected to review basic concepts and methods used to perform further pre-training and fine-tuning of a LLM:\n\nGet good understanding of the following concepts and techniques relevant to LLMs\nDefinitions of key terms and concepts\nhackerllama - The Llama Hitchiking Guide to Local LLMs (osanseviero.github.io)\nBackground knowledge on LLMs: \nIntroduction to Large Language Models\nUnderstanding LLMs: A Comprehensive Overview from Training to Inference\nTransformer architecture (encoder, decoder, self-attention)\nTransformers made easy: architecture and data flow | by Ma\u00e2li Mnasri | Opla | Medium\nHow Self Attention works in Transformer\nGenerative AI: The Science Behind Large Language Models - Simplified\nLLM Landscape\nArchitecture: three categories: Encoder-only , Encoderdecoder and Decoder-only\nLLM Boxing \u2022 Choose your Champion\nComparing the Best Open-Source Large Language Models | Shakudo\nlibrary (ollama.ai)\nEmbedding of the input data: Tokenization, Positional Embedding\ngoogle/sentencepiece: Unsupervised text tokenizer for Neural Network-based text generation.", "start_char_idx": 3654, "end_char_idx": 7236, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32a9398f-0f4f-4a08-aff1-43c30a00529a": {"__data__": {"id_": "32a9398f-0f4f-4a08-aff1-43c30a00529a", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 6.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffe3dbcf-b7d0-4566-b4ae-d519caa9ed8a", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 6.docx"}, "hash": "31bc542e171273b78f244410549f00131e9a7be620b4d4c7b872a8ae317f648c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be69b7c1-ab89-4f61-807a-a9e778f60ae8", "node_type": "1", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 6.docx"}, "hash": "da03490798871f2d0098f358ce8558bfb4a776efe748b0adefbf82930e187836", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a66145b4-44c0-40bf-8ad1-fc8a43384b8f", "node_type": "1", "metadata": {}, "hash": "b708453c2ab07293e2589aae206a3de02e69a832aa8a6a6b02e8630ef588ab48", "class_name": "RelatedNodeInfo"}}, "text": "(github.com)\nTest how tokenization works with OpenAI tokenizer \nMaster Positional Encoding: Part I | by Jonathan Kernes | Towards Data Science\nUnderstanding positional embeddings in transformer models (harrisonpim.com)\nKey concepts in Fine-tuning an LLM\nFine-Tuning Embedding Model with PEFT and LoRA\nHosting A Text Embedding Model That is Better, Cheaper, and Faster Than OpenAI\u2019s Solution \n\nGet familiar with Huggingface ecosystem\nReview huggingface documentation\nHugging Face Hub documentation\n\ud83e\udd17 Transformers (huggingface.co)\nTemplates for Chat Models (huggingface.co)\nTry some examples  on your local machine\nBuilding a PDF Knowledge Bot With Open-Source LLMs - A Step-by-Step Guide | Shakudo\nAn Introduction to Using Transformers and Hugging Face\nHow instruction finetuning works\nhuggingface/alignment-handbook: Robust recipes for to align language models with human and AI preferences (github.com)\nAmharic, Yoruba, and Swahili data collection, processing, and pipeline for LLM finetuning \nLlama-2-Amharic: LLMs for Low Resource Languages | by Garri Logistics | Dec, 2023 | Medium,\nhttps://huggingface.co/UBC-NLP/serengeti\nhttps://huggingface.co/Mollel/Swahili_Gemma\nhttps://huggingface.co/LeroyDyer/Mixtral_AI_SwahiliTron_7b\n\n\nTask 2: Load an LLM and Use It for Inference \nThis task will see you setting up your work environment, load an open source pre-trained LLM and use it to generate output for a variety of scenarios (text generation, translation, question answering, summarization ..etc).\nSet up a Huggingface account (this is required for accessing some open source models e.g. LLaMA 2, and to  upload your fine-tuned model later)\nNote about LLaMA: to get access to the model you need to\nFill this  Meta\u2019s Form, with the same email address you used to create your Hugging Face account. \nVisit the page of one of the LLaMA 2 available models, and accept Hugging Face\u2019s licence terms and acceptable use policy.\nSet up your environment [GPU enabled notebooks]\nMake a choice of open source LLM to use.\nThe choice of model (including the size of the model) depends on the use case and computational resource available.\nChoosing the Right Open-Source LLM for Your Needs\nWhich Open Source LLM Best to FINE-TUNE ? \nCheck the comprehensive list of open source/access LLMs on Hugging Face.\n[optional] You can use HuggingChat (Hugging Face's open-source chat UI for LLMs) to check a couple of LLMs (eg:  Mixtral-8x7B and Llama 2/3 model  fine-tuned for dialogue) \nLoad an open source LLM from Hugging Face. The size of the model will depend on whether we use quantization\nWithout model quantization:\nHow to download open source LLM models from hugging face and use it locally on your machine\nRunning a Hugging Face Large Language Model\nWith model quantization: Model Quantization with \ud83e\udd17 Hugging Face Transformers and Bitsandbytes Integration\nInference: use the loaded LLM to generate output. Make sure to test multiple inference scenarios  (text generation, translation, question answering, summarization) and test the model\u2019s ability to handle Amharic, Yoruba, and Swahili  language \nHugging Face docs: Generation with LLMs\n[optional] use a pipeline for inference\nTask 3: Data preprocessing and preparation\nYou may follow Llama-2-Amharic: LLMs for Low Resource Languages | by Garri Logistics | Dec, 2023 | Medium to understand the important steps of amharic data preparation for LLM finetuning.", "start_char_idx": 7237, "end_char_idx": 10635, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a66145b4-44c0-40bf-8ad1-fc8a43384b8f": {"__data__": {"id_": "a66145b4-44c0-40bf-8ad1-fc8a43384b8f", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 6.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffe3dbcf-b7d0-4566-b4ae-d519caa9ed8a", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 6.docx"}, "hash": "31bc542e171273b78f244410549f00131e9a7be620b4d4c7b872a8ae317f648c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32a9398f-0f4f-4a08-aff1-43c30a00529a", "node_type": "1", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 6.docx"}, "hash": "58b575e2c18342d3db152aca15825ed0c97236a1123109cda99cd84067b2ce1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd13e060-dc5e-4c64-9ae6-5afebea58d32", "node_type": "1", "metadata": {}, "hash": "abfc1afec8561d6efec72458b75ade921a023eb795c6fedfada88f10d4b993ed", "class_name": "RelatedNodeInfo"}}, "text": "Suggested tasks (be creative to do more) are:\nFor Amharic:\n* ['\u1210', '\u1211', '\u1212', '\u1213', '\u1214', '\u1216'] with ['\u1200', '\u1201', '\u1202', '\u1203', '\u1204', '\u1205', '\u1206']\n* ['\u1280', '\u1281', '\u1282', '\u1283', '\u1284', '\u1285', '\u1286'] with ['\u1200', '\u1201', '\u1202', '\u1203', '\u1204', '\u1205', '\u1206']\n* ['\u1220', '\u1221', '\u1222', '\u1223', '\u1224', '\u1226', '\u1226', '\u1227'] with ['\u1230, '\u1231', '\u1232', '\u1233', '\u1234', '\u1235', '\u1236', '\u1237']\n* ['\u12d0', '\u12d1', '\u12d2', '\u12d3', '\u12d4', '\u12d5', '\u12d6'] with ['\u12a0', '\u12a1', '\u12a2', '\u12a3', '\u12a4', '\u12a5', '\u12a6']\n* ['\u1338', '\u1339', '\u133a', '\u133b', '\u133c', '\u133d', '\u133e'] with ['\u1340', '\u1341', '\u1342', '\u1343', '\u1344', '\u1345', '\u1346']\nFor Yoruba \nEliminate non-Yoruba text, HTML tags, special characters, and irrelevant content.\nFor Swahili\nEliminate non-Swahili text, HTML tags, special characters, and irrelevant content.\nTask 4: Fine-Tuning the LLM\nSteps needed to fine-tune the LLM. Steps from inputting the data to model deployment. \n\nFor Amharic \nGarri logistics model: iocuydi/amharic-llama-llava (github.com) \nChinese-LLaMA-Alpaca-2 v4.0 released long context LLMs (64K) and RLHF-tuned LLMs\nfacebookresearch/llama-recipes: Examples and recipes for Llama 2 model (github.com)\nalignment-handbook/scripts/README.md at main \u00b7 huggingface/alignment-handbook (github.com)\nHow to Fine-tune an LLM Part 3: The HuggingFace Trainer | alpaca_ft \u2013 Weights & Biases (wandb.ai) (check out part 1 & 2 as well)\nas starter.\nFor Swahili\nMollel/Swahili_Gemma\nhttps://huggingface.co/LeroyDyer/Mixtral_AI_SwahiliTron_7b\nhttps://huggingface.co/Mollel/Swahili-Alpaca-Llama-3-8b_16bit\nFor Yoruba\nhttps://huggingface.co/UBC-NLP/serengeti\n\n\nTo use the Garri logistics model:\nAccept Llama2 license on huggingface and download it like this:\ngit lfs install\ngit clone https://huggingface.co/meta-llama/Llama-2-7b-hf\nDownload the amharic finetune from huggingface like this:\ngit lfs install\ngit clone https://huggingface.co/iocuydi/llama-2-amharic-3784m\nClone https://github.com/iocuydi/amharic-llama-llava repository\nThen inside inference/run_inf.py:\ncomment the import safety_utils line\nchange the MAIN_PATH to the path to folder you downloaded from step 1\nchange the peft_model to the path you cloned in the step 2\ngo to your llama2 folder (from step 1) and replace all the tokenizer related files with the one you find from the 2nd step\nset quanitzation=True inside the main function before the load_model function call\nFinally run the inference/run_inf.py file", "start_char_idx": 10639, "end_char_idx": 12905, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd13e060-dc5e-4c64-9ae6-5afebea58d32": {"__data__": {"id_": "fd13e060-dc5e-4c64-9ae6-5afebea58d32", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 6.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffe3dbcf-b7d0-4566-b4ae-d519caa9ed8a", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 6.docx"}, "hash": "31bc542e171273b78f244410549f00131e9a7be620b4d4c7b872a8ae317f648c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a66145b4-44c0-40bf-8ad1-fc8a43384b8f", "node_type": "1", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 6.docx"}, "hash": "0d124d2c43b44dab3d30edb7c06f1c56bad23056cc9f4a17a1d3d8c4ad58613f", "class_name": "RelatedNodeInfo"}}, "text": "You may choose one of the followng as your base model\nIntroducing Stable LM 2 1.6B \u2014 Stability AI\nMistral 7B | Mistral AI | Open-weight models 2310.06825.pdf (arxiv.org)\nimoneoi/openchat: OpenChat: Advancing Open-source Language Models with Imperfect Data (github.com)\ntiiuae/falcon-7b \u00b7 Hugging Face\nLlama 2 - Meta AI\n\n\n\nTutorials Schedule\nIn the following, the colour purple indicates morning sessions, and blue indicates afternoon sessions.\nMonday: The Mechanics of LLMs\nChallenge walk through and Introduction to transformers.\nIntroduction to Week Challenge (Yabebal)\nOverview of LLMs:  Their transformer architecture and main techniques (Emtinan)\n\nKey Performance Indicators:\nUnderstand the project \nGet a good understanding of how LLMs work\nTuesday: Tokenization and Vocabulary Creation\nMore concepts\nQ&A session (Yabebal)\nTokenization and Word embedding (Rehmet)\n\nWednesday: LLMs Fine-tuning\nGet a good understanding of data preparation and LLM finetuning .\nDifferent types of fine-tuning a pre-trained LLM (Emtinan)\nComponents of a transformer Model (Emtinan)\n\nThursday: Inference & LLMOps \nMore concepts \nComponents of a transformer Model (Emtinan)\nAdvanced use of Huggingface (Rehmet)\nFriday: Deployment \nMore concepts \nData Preparation for Instruction Tuning (Rehmet) \n\nDeliverables\nNOTE: Document should be a PDF stored in google drive or published blog link. DO NOT SUBMIT A LINK as PDF! If you want to submit pdf document, it should be the content of your report not a link. \nInterim Submission - Wednesday 8pm UTC\nLink to your code in GitHub\nRepository where you will be using to complete the tasks in this week's challenge. A minimum requirement is that you have a well structured repository and some coding progress is made.\n\n\nA review report of your reading and understanding of Task 1 & 2 and any progress you made in other tasks. \nFeedback\nYou may not receive detailed comments on your interim submission, but will receive a grade.\nFinal Submission - Saturday 8pm UTC\nLink to your code in GitHub \nComplete work  for Finetuning LLMs with Amharic, Yoruba, and Swahili (Depends on which language you chose in the previous week) data\nComplete work  for Generating texts  \n\nA blog post entry (which you can submit for example to Medium publishing) or a pdf report. . \nFeedback\nYou will receive comments/feedback in addition to a grade.\n\nReferences\nComplete Beginner\u2019s Guide to Hugging Face LLM Tools\nMaking LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA\nPEFT (Parameter-Efficient Fine-Tuning)\nFinetuning Large Language Models (LLMs) BERT\n\nInfrastructure \nBenchmarking Popular Opensource LLMs: Llama2, Falcon, and Mistral (truefoundry.com)", "start_char_idx": 12909, "end_char_idx": 15589, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc8fd058-446b-46fb-8f4d-09a4f3db721a": {"__data__": {"id_": "fc8fd058-446b-46fb-8f4d-09a4f3db721a", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 2.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca80b0f6-b94d-4be9-9164-f0933cd58098", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 2.docx"}, "hash": "fe0b599e2f1f4782b56a7cceb6c92e14aa5f140f857547287d7974a57b125f63", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e105af5-c502-4415-a3c1-eb97d221356e", "node_type": "1", "metadata": {}, "hash": "dd4e34c97280507ca096c06ff723d9467fc95377fa3382b5e839101f70526f36", "class_name": "RelatedNodeInfo"}}, "text": "10 Academy Cohort B - Weekly Challenge: Week 2\nData Engineering: Data warehouse tech stack with MySQL/PostgreSQL, DBT, Airflow\nOverview\nBusiness Need\nYou and your colleagues have joined to create an AI startup that deploys sensors to businesses, collects data from all activities in a business - people\u2019s interaction, traffic flows, smart appliances installed in a company. Your startup helps organisations obtain critical intelligence based on public and private data they collect and organise. \nA city traffic department wants to collect traffic data using swarm UAVs (drones) from a number of locations in the city and use the data collected for improving traffic flow in the city and for a number of other undisclosed projects. Your startup is responsible for creating a scalable data warehouse that will host the vehicle trajectory data extracted by analysing footage taken by swarm drones and static roadside cameras. \nThe data warehouse should take into account future needs, organise data such that a number of downstream projects query the data efficiently. You should use the Extract Load Transform (ELT) framework using DBT.  Unlike the Extract, Transform, Load (ETL), the ELT framework helps analytic engineers in the city traffic department setup transformation workflows on a need basis.  \nData\nIn Downloads \u2013 pNEUMA | open-traffic (epfl.ch) or from https://zenodo.org/records/7426506 you can find a pNEUMA data: pNEUMA is an open large-scale dataset of naturalistic trajectories of half a million vehicles that have been collected by a one-of-a-kind experiment by a swarm of drones in the congested downtown area of Athens, Greece. Each file for a single (area, date, time) is ~87MB data.  \nYou may refer to the following references to understand how the data is generated from video frames recorded with swarm drones.\nPIA15_poster.pdf (datafromsky.com)\n(PDF) Automatic vehicle trajectory extraction for traffic analysis from aerial video data (researchgate.net)\nYou may use the following github packages to visualise and interact with the data (and obtain other similar data)\ntud-hri/travia: a Traffic data Visualization and Annotation tool (github.com)\nJoachimLandtmeters/pNEUMA_mastersproject: Written python files to work with pNEUMA dataset (github.com)\n\nExpected Outcomes\nSkills:\nCreate and maintain Airflow DAGs\nWork with Apache Airflow, dbt, redash  and a DWH\nApply ELT techniques to DWH\nBuild data pipelines and orchestration workflows\n\nKnowledge:\nEnterprise-grade data engineering - using Apache and Databricks tools\nTeam\nInstructors: \nYabebal\nEmtinan\nRehmet\nKey Dates\nDiscussion on the case - 09:00 UTC time on Monday 29 Apr 2024.  Use #all-week2 to ask questions.\nInterim Submission - 8:00 PM UTC time on Wednesday 01 May 2024.\nFinal Submission - 8:00 PM UTC time on Saturday 04 May 2024.\nLeaderboard for the week\nThere are 100 points available for the week.\n20 points - community growth and peer support. \n30 points - presentation and reporting.\n15 points - interim submission. PDF slide or report format.     \n15 points for the final submission.  Blog entry or PDF with 5-8 pages.  \n\n50 points - Technical content\n20 points - Interim submission\n30 points - Final submission\nBadges\nEach week, one user will be awarded one of the badges below for the best performance in the category below.\nIn addition to being the badge holder for that badge, each badge winner will get +20 points to the overall score.\n\nVisualization - the quality of visualizations, understandability, skimmability, choice of visualization\nQuality of code - reliability, maintainability, efficiency, commenting - in the future this will be CICD\nAn innovative approach to analysis -using latest algorithms, adding in research paper content and other innovative approaches\nWriting and presentation - clarity of written outputs, clarity of slides, overall production value\nMost supportive in the community - helping others, adding links, tutoring those struggling\nThe goal of this approach is to support and reward expertise in different parts of the Machine learning engineering toolbox.\nLate Submission Policy\nOur goal is to prepare successful learners for the work and submitting late when given enough notice, shouldn\u2019t be necessary.\nFor interim submissions, those submitted 1-6 hours late will receive a maximum of 50% of the total possible grade. Those submitted >6 hours late may receive feedback, but will not receive a grade.\n\nFor final submissions, those submitted 1-24 hours late, will receive a maximum of 50% of the total possible grade. Those submitted >24 hours late may receive feedback, but will not receive a grade.", "start_char_idx": 1, "end_char_idx": 4628, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e105af5-c502-4415-a3c1-eb97d221356e": {"__data__": {"id_": "8e105af5-c502-4415-a3c1-eb97d221356e", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 2.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca80b0f6-b94d-4be9-9164-f0933cd58098", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 2.docx"}, "hash": "fe0b599e2f1f4782b56a7cceb6c92e14aa5f140f857547287d7974a57b125f63", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc8fd058-446b-46fb-8f4d-09a4f3db721a", "node_type": "1", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 2.docx"}, "hash": "b02b893c515d3f733e9d7f6076cb01b0d0c6769d9e66a65e6eee3500303dfcd0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5743c4ec-2a50-495b-9bae-c52c6842b09e", "node_type": "1", "metadata": {}, "hash": "4a7abf772e8fa14dfeb91d028ea87fb672127d081a6107d807a361f0cae216af", "class_name": "RelatedNodeInfo"}}, "text": "Instructions\nThe fundamental tasks in this week\u2019s challenge are the following - building data warehouse techstack\nConsisting of\nA \u201cdata warehouse\u201d (PostgreSQL)\nAn orchestration service (Airflow)\nAn ELT tool (dbt)\nA reporting environment (redash)\nSet it up locally using \nfully dockerized \n\nComplete the following tasks:\nCreate a DAG in Airflow that uses the bash/python operator to load the data files into your database. Think about a useful separation of Prod, Dev and Staging\nConnect dbt with your DWH and write transformations codes for the data you can execute via the Bash or Python operator in Airflow. Write proper documentation for your data models and access the dbt docs UI for presentation. \nCheck additional modules of dbt that can support you with data quality monitoring (e.g. great_expectations, dbt_expectations or re-data). \nConnect the reporting environment and create a dashboard out of this data\nWrite a short article about your approach and what were the most important decisions along the way\n\nConsider the following elements when doing the above tasks\nAIRFLOW: \nIf you want to use templates in Airflow, what is a good way to manage metadata and variables within your DAGS? (read about context)\nAutomated Alerting - what happens if the DAG is failing, e.g. a slack or email alert\nBuild hard circuit breaker pipelines with dbt (e.g. if a test fails, do not update the production tables)\ndbt\nAutomate the generation of dbt docs and make it available via web frontend\nExplore macros and write your own to create dynamic documentation and functions\nAutomate the dbt to Airflow connection by automatically creating DAGS out of dbt metadata (see here: https://www.astronomer.io/blog/airflow-dbt-2) \nRedash\nBuild a version control script system by hitting the API, download the queries and built an automated git storage process\nTutorials Schedule\nIn the following, the colour purple indicates morning sessions, and blue indicates afternoon sessions.\nMonday: Introduction to Week Challenge\nHere the trainees will understand the week\u2019s challenge.\nIntroduction to Week Challenge (Yabebal)\nData Models (kedro, data vault, star schema, database normalisation) (Emtinan)\n\nKey Performance Indicators:\n\nUnderstanding week\u2019s challenge\nUnderstanding Data Warehousing\nAbility to reuse previous knowledge\nSharing references and content around data warehousing\nGetting familiar with data models and frameworks\n\nTuesday: dbt Orchestration with Airflow\nHere the trainees will understand Airflow and how to use it to schedule and orchestrate dbt.\nAnalytics Engineering using DBT (Emtinan)\nScheduling and Orchestration using Airflow (Rehmet)\n\nKey Performance Indicators:\n\nUnderstanding directed acyclic graphs (DAGs) in Airflow\nUsing Airflow as a scheduler to orchestrate dbt\nSharing references and content around dbt and airflow\nWednesday: From Data Lakehouse to BI Dashboards\nHere the trainees will understand the different data warehouse and data lake tech stacks \nKedro Data Layers - Data Lakehouse (Dibora)\nBuilding Dashboards using Redash (Abel)\n\nKey Performance Indicators:\n\nUnderstanding Data Warehouse, Data Lake, and Data Warehouse\nUnderstanding redash\nSharing references and content around redash and data warehousing concepts\nThursday: Data Models and Tools\nHere the students will understand Redash.\nUsing DBT and Tableau/Looker/Microsoft BI to build BI Dashboard \nConversation with the Data Engineering team (Betty et al.) - Adludio Team\nAWS Cloud Ecosystem for Data Engineering (Nabil et al.)\n\nKey Performance Indicators:\n\nIntroduction to AWS S3, Redshift, RDS, Athena, Glue\nUnderstanding Amazon Athena (managed presto) & Amazon Glue (managed HIve)\nSharing references and content around Snowflakes, Databricks Data Lakes, Google Big Query, Amazon Redshift and other  data warehousing and data lake technologies.\nLearning the data engineering tools ecosystem\nSharing references and content around redash and data warehousing concepts\nTBD: Data models, tools, and frameworks\nELT vs ETL\nAnalytics Engineering with DBT \nData Models for scalable data warehouse\nData Lakes vs Data Warehouse: Tools and principles \nSnowflake \nAmazon Athena (managed presto) & Amazon Glue (managed HIve)\nDatabricks\nGoogle BigQuery\nAmazon Redshift\n\nKey Performance Indicators:\n\nLearning the data engineering tools ecosystem\nGetting familiar with data models and frameworks\n\nSubmission \nInterim: Due Wednesday 01 May 20:00 UTC\nLink to your code in GitHub.", "start_char_idx": 4631, "end_char_idx": 9073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5743c4ec-2a50-495b-9bae-c52c6842b09e": {"__data__": {"id_": "5743c4ec-2a50-495b-9bae-c52c6842b09e", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 2.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca80b0f6-b94d-4be9-9164-f0933cd58098", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 2.docx"}, "hash": "fe0b599e2f1f4782b56a7cceb6c92e14aa5f140f857547287d7974a57b125f63", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e105af5-c502-4415-a3c1-eb97d221356e", "node_type": "1", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 2.docx"}, "hash": "a65a60a188a17c1f1419d541a477b6e847366c376ba33e12b29e9fd4efce40ad", "class_name": "RelatedNodeInfo"}}, "text": "Learning the data engineering tools ecosystem\nSharing references and content around redash and data warehousing concepts\nTBD: Data models, tools, and frameworks\nELT vs ETL\nAnalytics Engineering with DBT \nData Models for scalable data warehouse\nData Lakes vs Data Warehouse: Tools and principles \nSnowflake \nAmazon Athena (managed presto) & Amazon Glue (managed HIve)\nDatabricks\nGoogle BigQuery\nAmazon Redshift\n\nKey Performance Indicators:\n\nLearning the data engineering tools ecosystem\nGetting familiar with data models and frameworks\n\nSubmission \nInterim: Due Wednesday 01 May 20:00 UTC\nLink to your code in GitHub. You should have a screenshot folder that includes a screenshot of your data lineage from dbt\nSubmit a two pages max document that shows the tech-stack flow diagram, and explanation of the different elements\nFinal Due Saturday 04 May 20:00 UTC\nLink to your code in GitHub. Your readme should contain the link to your deployed dbt data warehouse documentation. You should also have a screenshot folder that includes a screenshot of the data view you built\nA blog or report explaining the process you followed to build the tech stack. What are the challenges? What can be improved with more time?\n\nFeedback\nYou will receive comments/feedback in addition to a grade.\n\n\nReferences\nData Warehouse, Data Models, and Data Tools\nData Warehouse Testing 101 | Panoply\nBuilding a Data Vault (matillion.com)\nDataHub Open Source Metadata Platform\nModern Data Warehouses: Functions, Architecture, & Examples | Estuary\ndbt:\nGeneral Information:\nInstalling dbt https://docs.getdbt.com/dbt-cli/installation/#pip\ndbt(Data Build Tool) Tutorial \u00b7 Start Data Engineering\ndbt - Package hub (getdbt.com)\nIntroduction Videos on dbt https://www.youtube.com/playlist?list=PLy4OcwImJzBLJzLYxpxaPUmCWp8j1esvT\nRedshift config; https://docs.getdbt.com/reference/resource-configs/redshift-configs/\nDocs from Gitlab: https://about.gitlab.com/handbook/business-ops/data-team/platform/dbt-guide/\nCLI command reference: https://docs.getdbt.com/reference/dbt-commands/\nBasic Introduction to dbt https://www.kdnuggets.com/2021/07/dbt-data-transformation-tutorial.html\nArticles:\nhttps://medium.com/the-telegraph-engineering/dbt-a-new-way-to-handle-data-transformation-at-the-telegraph-868ce3964eb4\nhttps://medium.com/hashmapinc/dont-do-analytics-engineering-in-snowflake-until-you-read-this-hint-dbt-bdd527fa1795\nRepo Examples:\nhttps://github.com/mattermost/mattermost-data-warehouse\nhttps://gitlab.com/gitlab-data/analytics/-/tree/master/\nHow to structure repo's \nhttps://discourse.getdbt.com/t/how-we-structure-our-dbt-projects/355\nhttps://discourse.getdbt.com/t/should-i-have-an-organisation-wide-project-a-monorepo-or-should-each-work-flow-have-their-own/666/2\nhttps://discourse.getdbt.com/t/how-to-configure-your-dbt-repository-one-or-many/2121\nhttps://medium.com/photobox-technology-product-and-design/practical-tips-to-get-the-best-out-of-data-building-tool-dbt-part-1-8cfa21ef97c5\n\nAirflow:\nhttps://livebook.manning.com/book/data-pipelines-with-apache-airflow/chapter-1/v-6\n https://www.linkedin.com/in/marclamberti/ \n\nDocker: \nhttps://www.youtube.com/watch?v=fqMOX6JJhGo\nhttps://docker-curriculum.com/#docker-images\n\nRedash:\nhttps://github.com/dwyl/learn-redash\nhttps://fitdevops.in/how-to-setup-redash-dashboard-on-ubuntu\nVirtual environments:\nhttps://www.ianmaddaus.com/post/manage-multiple-versions-python-mac/\nhttps://www.codeblocq.com/2016/01/Search-through-history-in-OSX-terminal/\nhttps://janakiev.com/blog/jupyter-virtual-envs/\nhttps://medium.com/@blessedmarcel1/how-to-install-jupyter-notebook-on-mac-using-homebrew-528c39fd530f", "start_char_idx": 8457, "end_char_idx": 12081, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3ba542d-51c7-4ebc-bf1f-be8ad36cb7d3": {"__data__": {"id_": "a3ba542d-51c7-4ebc-bf1f-be8ad36cb7d3", "embedding": null, "metadata": {"source": "cB - Careers challenge - Peer Mentorship - W1.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "913bd659-b6ac-42c3-9d68-e20ec1639db6", "node_type": "4", "metadata": {"source": "cB - Careers challenge - Peer Mentorship - W1.docx"}, "hash": "5430181e483ec5b2c1ca56af9e8029ec24fed3d1ea369f1496392386b04e8fd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e4eeb68-d623-4d3d-8e04-ad45c67a2a35", "node_type": "1", "metadata": {}, "hash": "ff7b796f01f12c9316fa51af2095450ecb95ecf7e2c6b1ff38fc637c5822d0bf", "class_name": "RelatedNodeInfo"}}, "text": "10 Academy cA: Week 1\nCareer Exercise 2 \nPeer Mentorship\n\nDue Date: Saturday, 27th April 2024, 8PM UTC\nBackground\nHighlighting the critical significance of intentional teamwork within any organisation is essential. Regardless of the profession you pursue, teamwork remains indispensable. With the increasing adoption of remote and hybrid work models, it's crucial to navigate digital interactions with colleagues over different platforms. Collaborating with peers/ workmates fosters mutual growth and enhances professional development, contributing to collective success within the organisation. \n\nThe objective of this exercise is to gain a comprehensive understanding of the role and practice of peer mentorship as well as the importance of collaboration in a remote work setting. \n\nGuidelines:\n\nThis exercise is for you to practise peer mentorship with one of your colleagues. You have been paired with one of one of your colleagues for this exercise. The list can be found here.\nUsing the reference material below as additional insight and guidelines, get a clear definition of who a peer mentor is and how peer mentorship is practised. \nYou are supposed to meet your peer via Google meet. Follow the instructions in the task section below to understand how you will navigate that meeting.", "start_char_idx": 0, "end_char_idx": 1293, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e4eeb68-d623-4d3d-8e04-ad45c67a2a35": {"__data__": {"id_": "6e4eeb68-d623-4d3d-8e04-ad45c67a2a35", "embedding": null, "metadata": {"source": "cB - Careers challenge - Peer Mentorship - W1.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "913bd659-b6ac-42c3-9d68-e20ec1639db6", "node_type": "4", "metadata": {"source": "cB - Careers challenge - Peer Mentorship - W1.docx"}, "hash": "5430181e483ec5b2c1ca56af9e8029ec24fed3d1ea369f1496392386b04e8fd3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3ba542d-51c7-4ebc-bf1f-be8ad36cb7d3", "node_type": "1", "metadata": {"source": "cB - Careers challenge - Peer Mentorship - W1.docx"}, "hash": "54865ce0d9f61dd0f87d6403e9a9bb9c7230779a358092136997d46cf6c9c091", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3216d25b-d3c5-4a7a-ae21-f1c0cbf4f168", "node_type": "1", "metadata": {}, "hash": "ac9f4959f320ddcb5f65141935f253005be6fe8b62bde45557d65bcb8730be8d", "class_name": "RelatedNodeInfo"}}, "text": "Before the meeting:\nDo a quick search about your peer via LinkedIn or Google to have some basic information (in order to get to know them a little). Be careful to respect their digital privacy and stick to professional platforms only. This little research on your peer on Linkedin or the internet could help guide your choice of questions to ask.\nPrepare a list of 5 tailored questions to ask during the meeting to gather insights into the peer's tech background, experience and challenges in their 10 Academy\u2019s journey so far, and any interesting facts about them. \nReach out to them and schedule a Google Meet session on the time that is convenient to both of you. \n\nDuring the meeting;\nIntroduce yourselves, demonstrate curiosity, and inquire about the colleague's background and interests. \nGet to know your peer\u2019s strengths and weaknesses. Keep an open-minded conversation and consider the list of questions and insight areas you\u2019ve mentioned above. See if you can find mutual ways to guide each other during the training because this will be your accountability partner during the training. \nTake a screenshot of both of you during the call with your videos on.\n\nTask:\n\nAfter your session with your peer, we would like you to draft a report documenting the exercise from your perspective. Your report should be a PowerPoint of 6 slides [Respect the number of slides.  \n\nYour report should answer the following 7 questions:\nWhat challenges did you have before and during the preparation of your meeting? How did you overcome these challenges?\nWrite down a short bio of your peer according to how they introduced themselves. \nList of the 5 questions you asked your peer and their answers to them.\nList of 5 questions your peer asked you.\nWhat are 5 things you learnt or benefited from this exercise and how they might benefit your career in future?\nWhat is your CTA (Call to Action)? (Did you exchange contact and social media handles? Did you schedule a subsequent meeting? Did you agree on frequent checkups?)\nOn your last slide, attach a screenshot of you both with your cameras on.\n\nUsefulness in real life\nUnderstanding how to work well with others is really important in any job. No matter what you do, teamwork matters. Especially now, with more people working from home or in a mix of office and remote settings, knowing how to talk and collaborate online is key. This activity is all about learning from each other and helping each other grow. By teaming up with a colleague, you'll get to share experiences and support each other in your work. It's like having a buddy to learn and grow with. Plus, you'll learn how to use tools like Google Calendar and Google Meet to schedule and have online meetings. It's a chance to get better at working together, which is super useful for your career.\n\nRubrics: \n\nPreparation Challenges:  This question aims to evaluate the trainee's ability to recognize and navigate obstacles encountered in the preparation phase of the peer mentorship meeting. Grading will focus on their identification of challenges, problem-solving approach, and reflection on solutions implemented.\n\nPeer Short Bio: This question assesses the trainee's capacity to synthesise information and communicate it clearly, demonstrating active listening skills and effective summarization techniques. Grading will focus on whether the trainee offered a brief yet comprehensive overview of the peer's background, focusing on relevant details, and also evaluate the clarity and organisation of the peer introduction, ensuring that essential details are effectively communicated.\n\nQuestions Asked and Answers: This question focuses on the quality of the interaction between the trainee and the peer, assessing the effectiveness of the trainee's questions and the peer's responses in facilitating constructive dialogue and knowledge sharing. Grading will focus on  Evaluating if the trainee listed 5 questions to ask their peer. These questions should show ability to gain valuable insights from the peer. And also look for the presence of the answers provided in regards the asked questions.\n\nList of questions from the peer: In this question, the trainee is expected to document 5 questions asked by their peer about them during the meeting, demonstrating accuracy, relevance, and completeness in their response. Grading will focus on that.\n\nLearnings from this exercise: In this question, the trainee is expected to reflect on the learning outcomes and benefits of the peer mentorship exercise, and articulate how these insights can positively impact their future career development with clarity and specificity. Grading will focus on Assessing the depth of reflection, evaluating the trainee's ability to articulate how the insights gained from the exercise can benefit their career development in the future, and the clarity and conciseness of the trainees answer.\n\nCall to Actions: In this question, grading will focus on the trainee's proactive approach in establishing ongoing communication and collaboration with their peer is evaluated based on their initiative, clarity of communication, and emphasis on continued collaboration. \n\nScreenshot! Grading will check the presence of the meeting screenshot in your slide.", "start_char_idx": 1296, "end_char_idx": 6538, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3216d25b-d3c5-4a7a-ae21-f1c0cbf4f168": {"__data__": {"id_": "3216d25b-d3c5-4a7a-ae21-f1c0cbf4f168", "embedding": null, "metadata": {"source": "cB - Careers challenge - Peer Mentorship - W1.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "913bd659-b6ac-42c3-9d68-e20ec1639db6", "node_type": "4", "metadata": {"source": "cB - Careers challenge - Peer Mentorship - W1.docx"}, "hash": "5430181e483ec5b2c1ca56af9e8029ec24fed3d1ea369f1496392386b04e8fd3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e4eeb68-d623-4d3d-8e04-ad45c67a2a35", "node_type": "1", "metadata": {"source": "cB - Careers challenge - Peer Mentorship - W1.docx"}, "hash": "82033d1af3485689d7f1300559f8b8e1242a3f3a0762c297db2f46ee56b5ef46", "class_name": "RelatedNodeInfo"}}, "text": "Important Links\n\nMentoring Alternatives: The Role of Peer Relationships \nWhat is Peer Mentoring\nSKILLS FOR SUCCESSFUL MENTORING:\nHow to be a peer mentor \nHow to build a successful mentoring relationship.\n8 Tips for an amazing Mentor relationship.\nHow to build a great relationship with a mentor", "start_char_idx": 6542, "end_char_idx": 6836, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50ca1b87-339f-4b5b-b9df-a0c7aba9682c": {"__data__": {"id_": "50ca1b87-339f-4b5b-b9df-a0c7aba9682c", "embedding": null, "metadata": {"source": "cB - Careers challenge - Leadership - W5.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "85341c2a-a043-4165-8d5b-db221409e1d0", "node_type": "4", "metadata": {"source": "cB - Careers challenge - Leadership - W5.docx"}, "hash": "f67cae515922564f39a5e8cc3d6b72212aaf6c422c2d40635c466dce72d91eb6", "class_name": "RelatedNodeInfo"}}, "text": "10 Academy cB : Week 5\nCareers - Exercise 2\n\nLeadership\n\nDue Date: May 25, 2024. 8:00 PM UTC\n\nBackground\nHave you ever wondered what makes a truly great leader? We often picture powerful figures who have shaped history, from revolutionary leaders like Gandhi and Martin Luther King Jr. to influential business titans. But what exactly sets these individuals apart? Why do some leaders inspire unwavering devotion while others struggle to gain traction?\n\nAt its core, leadership is about influencing others to achieve a shared goal. As leaders, we inspire and guide our teams to navigate change and accomplish the organisation's objectives. \n\nTask A\nAs a leader, it's essential to recognize and acknowledge your strengths and weaknesses to effectively manage and develop your team. Reflecting on past experiences with ineffective leaders can help you identify traits to avoid and areas for personal growth.\nThink about the least favourite leader you've ever had the experience of working with. This could be a manager, supervisor, or team lead.\nExercise\nWho was the least favourite leader you ever had? Describe their position, behaviour, and any notable characteristics.\nWhat did they do to be your least favourite leader? Identify specific actions, decisions, or behaviours that made them ineffective .\nDo you resonate with any of the behaviours your least favourite leader has? Highlight 3 weaknesses you believe you have as a leader.\nFor each of the weaknesses identified, explain how you aim to improve on them as a leader.\n\nTask B\nYou are the project manager at NovaTech, a software development company. Your team has been tasked with developing \"Project Phoenix,\" a critical new product that will determine the company's future in the competitive tech market. \nProject Phoenix launch deadline is three months away, and the company's stakeholders are eagerly awaiting its success.\nYour team consists of diverse individuals with varying levels of experience:\nJohn (leaving in two weeks): A seasoned developer and key team member with in-depth knowledge of the project's core component. His departure will leave a significant gap in expertise..\nAisha: A new team member with a background in emerging technologies. She's suggested using a different technology, \"EchoStack,\" which she's familiar with, but it's different from the team's usual process.\nMark: A senior developer with experience in software development. He expressed concerns about the feasibility of Aisha\u2019s proposed approach to the project.\nRaj: A mid-level developer with experience in software design patterns. He's been working on the project's UI components.\nLena: A junior developer who's been assisting with testing and debugging.\nDavid: A Quality Assurance (QA) engineer responsible for ensuring the product meets the required quality standards.\nDuring the team meeting on Monday, John announced his unexpected departure, leaving the team with a significant knowledge gap (remember he\u2019s the one with knowledge of the project's core component) and added workload. John's expertise will be sorely missed. Aisha's suggestion to use EchoStack has sparked debate, as it would require significant changes to the project's architecture.Mark has expressed concerns about Aisha\u2019s proposed approach, citing potential risks and delays. \nAs the project manager, it's your responsibility to:\nAddress the knowledge gap left by John's departure\nResolve the disagreement between Mark and Aisha\nEnsure the project stays on track and meets the immovable deadline\nMotivate and guide the team to overcome the challenges ahead\n\n\nExercise\nExplain five ways to ensure that John\u2019s departure does not have a significant negative impact on the success of the project?\nUsing the tips on managing people, outline a step-by-step approach you\u2019ll use to address Mark's concerns and Aisha's suggestion?\nWhich leadership style(s) will you use to motivate and guide the team to ensure the project stays on track and meets the deadline? Explain why you chose that style(s) giving examples of occasions where you\u2019ll use them.\n\nSubmission\nYour responses should be on a maximum of 10 slides PPT. Convert it into PDF and submit the link on Tenx.\n\n\nRubrics", "start_char_idx": 0, "end_char_idx": 4190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "805357ac-c84c-4cab-acd4-55c6f0450b78": {"__data__": {"id_": "805357ac-c84c-4cab-acd4-55c6f0450b78", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 5.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd04ce89-ee65-4c26-8cee-eb0ed9120c40", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 5.docx"}, "hash": "c9737cde691d828cfb9b248a4999ba1bb2edf1a551f4a3c0f086dc538b16b6e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68a074e6-6323-4b68-a70b-ad619a311f35", "node_type": "1", "metadata": {}, "hash": "9fef678fdbea47a8b7cd550b3eb081d36469a79aeff778b2fd05ef52e5ebb670", "class_name": "RelatedNodeInfo"}}, "text": "10 Academy Cohort B - Weekly Challenge: Week 5\nScalable Data Warehouse for LLM Finetuning: API Design for High Throughput Data Ingestion and RAG Retrieval\nOverview\nBusiness Need\nRoots Tech Solutions is committed to enhancing its Natural Language Processing (NLP) capabilities for multiple African languages, particularly for Swahili, Yoruba, and Amharic languages. As part of this initiative, the company aims to develop a comprehensive data corpus to support various NLP applications, such as semantic search, content generation, chatbot support, sentiment analysis, speech recognition, and more.\nThe ability to process and understand Swahili, Yoruba, and Amharic text/audio accurately is critical for Roots Tech Solutions to develop innovative and competitive products. The current lack of extensive, high-quality text/audio datasets for these languages is a significant bottleneck. By collecting a vast amount of  text/audio dataset from diverse online sources, including news websites, blogs, and social media platforms, the company can overcome this challenge.\nThe collected data will undergo cleaning and processing to ensure its quality and relevance. It will be stored in a structured database, making it easily accessible for various NLP tasks. Additionally, APIs will be developed to facilitate seamless integration and querying of the dataset, enabling the company to leverage this resource effectively.\nThis project is expected to significantly enhance Roots Tech Solutions' NLP capabilities, enabling the development of sophisticated African languages processing tools and applications, thereby providing a competitive edge in the technology market. \nData\nYou will be collecting the data from various data sources.\nExpected Outcomes\nSkills:\nData Collection and Web Scraping:\nProficiency in web scraping techniques using tools like BeautifulSoup, Scrapy, and Selenium.\nExpertise in identifying and extracting data from various online sources, including news websites, blogs, and social media platforms.\nProgramming and Development Skills:\nProficiency in Python and Javascript (React) programming languages\nProficiency in SQL \nExperience in developing APIs using frameworks like Flask or Django for data access and integration.\nProficiency in designing database schemas and managing structured data storage.\nData Processing and Cleaning\nSQL and Database Management  \nExperience in deploying complex software package using Docker and docker-compose\nUI/UX Design\nKnowledge:\nNatural Language Processing (NLP) Knowledge:\nUse of vector databases\nMachine Learning and AI Knowledge\nTeam\nInstructors: \nYabebal\nNatnael\nEmtinan\nRehmet\nKey Dates\nDiscussion on the case - 9:00 UTC time on 20 May 2024.  Use #all-week-3 to ask questions.\nInterim Submission - 8:00 PM UTC time on Wednesday 22 May 2024.\nFinal Submission - 8:00 PM UTC time on Saturday 25 May 2024\nLeaderboard for the week\nThere are 100 points available for the week.\n20 points - community growth and peer support. \n\t13 points - technical public and group-based RC channels\nTotal number of messages (5)\nTotal number of Mentions (3)\nTotal number of DM connections (5) \n\t7 points - community activities\nNumber of messages in non-technical channels (4)\nOn-time presence in Gmeet sessions (3) \n30 points - presentation and reporting.\n\t15 points - interim submission. PDF\n    \n\t15 points for the final submission.  Blog entry or PDF with 5-8 pages.  \n50 points - Technical content\n\t20 points - Interim submission\nGithub link submission (20)\n\n30 points - Final submission \n\nGithub Link submission (25)   \n\nBadges\nEach week, one user will be awarded one of the badges below for the best performance in the category below.\nIn addition to being the badge holder for that badge, each badge winner will get +20 points to the overall score.\nVisualization - the quality of visualizations, understandability, skimmability, choice of visualization\nQuality of code - reliability, maintainability, efficiency, commenting - in the future this will be CICD\nAn innovative approach to analysis -using latest algorithms, adding in research paper content and other innovative approaches\nWriting and presentation - clarity of written outputs, clarity of slides, overall production value\nMost supportive in the community - helping others, adding links, tutoring those struggling\nThe goal of this approach is to support and reward expertise in different parts of the Machine learning engineering toolbox.\nGroup Work Policy\nThis week, you are expected to complete the project with your assigned group. In the table below, your name is assigned to one of the groups we formed.\nPlease check out this document for a guideline on how to connect to your cloud machine.", "start_char_idx": 2, "end_char_idx": 4700, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68a074e6-6323-4b68-a70b-ad619a311f35": {"__data__": {"id_": "68a074e6-6323-4b68-a70b-ad619a311f35", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 5.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd04ce89-ee65-4c26-8cee-eb0ed9120c40", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 5.docx"}, "hash": "c9737cde691d828cfb9b248a4999ba1bb2edf1a551f4a3c0f086dc538b16b6e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "805357ac-c84c-4cab-acd4-55c6f0450b78", "node_type": "1", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 5.docx"}, "hash": "1174689879b1812fa40d162f1867849954cf83e4f0c0f67e8282690574a255f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "045f4922-71e0-4de9-acca-71a3b5657f00", "node_type": "1", "metadata": {}, "hash": "1afe65a2b606370aa1257437377b2f72e5731eaef589e04631bc57e265df6b2d", "class_name": "RelatedNodeInfo"}}, "text": "Late Submission Policy\nOur goal is to prepare successful learners for a global level job. At work, deadlines are sometimes very strict - either you do it before the deadline or the company loses a substantial opportunity.  Moreover, the late communication behaviour (submission in 10 Academy can be considered as progress communication to team leads), blinds team leads and CEOs and is very determinantal in hindering the success of the company.\nWe have set our late submission as follows\nSubmissions are accepted only within the 12 hrs window - 17:00 UTC - 7:00 UTC  of the submission deadline\nFrequently late submissions (exceeding 6 total late submissions) will disqualify a person from the list of trainees 10 Academy recommends to partner employers.\nBadges will be rewarded for the cumulative on-time appearances (gmeet calls, on-time assignment submissions, and other places where being on-time is important) \n\n\n\n\nPossible Work Plan\nDecisions:\nSelect group lead and other essential personals to facilitate efficient team work\nSelect the language you will focus your effort to collect data for. The options are:\nSwahili\nYoruba\nAmharic\nDefine the tech-stacks for your pipeline\n\nWork Plan\nWithin your group, plan how you divide the work to group members\nUsing Figma or similar tool, design your tech-stack and how data flows in your pipeline  \nStage 1: Initial Data Collection and Storage\n- Objective: Establish foundational capabilities for collecting, storing, and accessing text data.\n- Tools: Scrapy, BeautifulSoup, Selenium, MongoDB/PostgreSQL.\n- Implementation:\nIdentify and list potential data sources such as news websites, blogs, and social media.\nDevelop web scraping scripts using Scrapy, BeautifulSoup, and Selenium to collect text data.\nDesign and implement a database schema in PostgreSQL/MongoDb for storing the collected text data.\nStore the raw scraped data in the database.\n\nStage 2: Data processing\n- Objective: Clean, preprocess, and annotate the collected text data to ensure high quality for NLP applications.\n- Implementation:\nDevelop Python scripts to clean and preprocess the text data (e.g., remove HTML tags, and special characters).\nFilter data where possible to minimize data contamination by english or other languages that are not your focus. You may also consider creating a separate data table/category for english rich texts.\nStore the cleaned and processed data back into the database.\n\nStage 3: API Development for Data Access\n- Objective: Develop APIs to provide easy access to the collected and processed text data.\n- Implementation:\nDevelop RESTful APIs using Flask or FastAPI to enable querying and retrieving data from the database.\nImplement endpoints for searching and filtering text data based on keywords and metadata.\nEnsure efficiency in handling large-scale data access requests.\n\nStage 4: Automation & Stream Processing \n- Objective: \n Implement an automated pipeline to ensure continuous data collection, cleaning, processing, storage, and access.\n- Implementation:\nSetup workflow automation using Apache Airflow\nDefine workflow DAG\nContainerize the workflow with Docker\nAutomate deployment and monitoring\n\nHandle data streams using a streaming processing library\n- Implementation:\nUse the Faust library to setup data producer and consumer \nMeasure the speed with which your pipeline can consume data in async mode\nLocust - A modern load testing framework", "start_char_idx": 4703, "end_char_idx": 8111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "045f4922-71e0-4de9-acca-71a3b5657f00": {"__data__": {"id_": "045f4922-71e0-4de9-acca-71a3b5657f00", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 5.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd04ce89-ee65-4c26-8cee-eb0ed9120c40", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 5.docx"}, "hash": "c9737cde691d828cfb9b248a4999ba1bb2edf1a551f4a3c0f086dc538b16b6e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68a074e6-6323-4b68-a70b-ad619a311f35", "node_type": "1", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 5.docx"}, "hash": "fcb33048b35c6f7e657bb08e5278df9ce905c60248b4eb3a43099de0f3465d6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4fd96d3c-91ba-4f7d-9795-3ddc3c7545dc", "node_type": "1", "metadata": {}, "hash": "111569399b17dc353b7c86186e9d23afc8ad82538c0587afd7469280f0debd31", "class_name": "RelatedNodeInfo"}}, "text": "Task 1: Data Collection and Storage\nIdentify data sources:\nResearch and list potential sources such as news websites, blogs, social media platforms, online forums, and digital libraries.\nPrioritize sources based on the quality and volume of content available.\nSet Up Development Environment\nInstall necessary tools and libraries (e.g., Scrapy, BeautifulSoup, Selenium).\nSet up version control with Git and create a repository on GitHub.\nBy using Scrapy, BeautifulSoup, and Selenium set up web scraping scripts\nDesign an SQL/NoSQL schema for the raw data.\nStore raw data.\n\nTask 2: Data processing \nRetrieve the raw text data stored in PostgreSQL/MongoDb\nNormalize Text\nRemove punctuation, and normalize whitespace.\nRemove HTML Tags and Special Characters\nStore the cleaned data.\nTask 3: API and Frontend Development for Data Access\nDevelop APIs and a frontend interface to provide easy access to the collected and processed data.\nDesign API Endpoints\nDefine the endpoints needed for data access (e.g., search, filter, retrieve specific data).\nImplement API Using Flask/FastAPI\nCreate the API application with Flask or FastAPI.\nDevelop the Frontend Interface Using React\nCreate a React application to interact with the API and display the data.\nCreate components for displaying data\nImplement functionalities like\nSearch\nFilter\nRetrieve specific data\nTask 4: Data Annotation\nAnnotate the subset of the data to create labeled datasets for various Natural Language Processing (NLP) tasks and Supervised LLM finetuning, such as named entity recognition, sentiment analysis, and text classification.\nSet up annotation tools\nPrepare data for annotation\nPrepare the cleaned and preprocessed text data for annotation.\nFormat the data appropriately for the annotation tool.\nPerform annotations\nUse Prodigy to annotate the text data.\nValidate annotations\nEnsure the quality and consistency of annotations.\nStore Annotated Data\nStore the annotated data for use in NLP model training.\nTask 5: Automation\nSet Up Apache Airflow for workflow automation\nInstall and configure Apache Airflow to manage and orchestrate the different stages of the pipeline.\nDefine workflow DAG (Directed Acyclic Graph)\nCreate a DAG to define the sequence of tasks for data collection, cleaning, processing, and storage.\nContainerize the Workflow with Docker\nCreate Dockerfiles for each component of the pipeline (data collection, cleaning, API).\nCreate a docker-compose.yml file for orchestration\nUse Docker Compose to define and manage the multi-container application.\nImplement CI/CD Pipeline with GitHub Actions\nSet up GitHub Actions to automate testing, building, and deployment of the Docker images.\nAutomate Deployment and Monitoring\nSet up monitoring tools like Prometheus and Grafana to track the performance and health of the pipeline.\nTask 6: Blog Reporting\nWrite a blog-like report that details the process followed, the challenges faced, and lessons learned from this week\u2019s challenge.\nN.B for reporting\nYour report should start with the Introduction, the overall body of your report, and then a Conclusion.\n\n\nTutorials Schedule\nIn the following, the colour purple indicates morning sessions, and blue indicates afternoon sessions.\nMonday: Project Overview and Data Collection Techniques\nIntroduction to Week Challenge (Yabebal)\nWeb Scraping and  Data Collection Techniques(Emtinan)\n\nKey Performance Indicators:\n\nUnderstanding week\u2019s challenge\nUnderstanding Data Collection Techniques\nUnderstanding web scraping\nAbility to reuse previous knowledge\nTuesday: Data Cleaning and Annotation\nData preprocessing and cleaning for Amharic, Swahili, and Yoruba text.(Rehmet)\nData annotation.(Rehmet)\nWednesday: API and Frontend Development \nDevelop APIs for data access.(Rehmet)\nFrontend development using react.(Rehmet)\nThursday: Automation and Orchestration\nAutomating the entire pipeline(Emtinan)\nContainerizing the application using Docker.(Emtinan)\nFriday: Monitoring\nIntroduction to monitoring tools (Prometheus and Grafana).(Emtinan)\nSubmission \nInterim: Due Wednesday 22 May 20:00 UTC\nA PDF report with an overview of methodologies and tools used for data collection, including: \nDetailed comparison between web scraping tools used.\n\nGithub link submission that demonstrates:\nA working data collection script.\nDatabase schema design.\nData loading to the database.\nWork in progress for the front end.\nWork in progress for backend API.", "start_char_idx": 8115, "end_char_idx": 12522, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4fd96d3c-91ba-4f7d-9795-3ddc3c7545dc": {"__data__": {"id_": "4fd96d3c-91ba-4f7d-9795-3ddc3c7545dc", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 5.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd04ce89-ee65-4c26-8cee-eb0ed9120c40", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 5.docx"}, "hash": "c9737cde691d828cfb9b248a4999ba1bb2edf1a551f4a3c0f086dc538b16b6e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "045f4922-71e0-4de9-acca-71a3b5657f00", "node_type": "1", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 5.docx"}, "hash": "ee31bd1863a68f4c881dd3940eb854fd49a5ce5cb4b5fcdbc2d879704a2eae3a", "class_name": "RelatedNodeInfo"}}, "text": "Feedback\nYou may not receive detailed comments on your interim submission but will receive a grade.\nFinal: Due Saturday 25 May 20:00 UTC\nPDF document (to be published) or a published Blog link detailing the process followed. This should include:\nThe business objective of the project\nThe project design \nThe tech-stack used\nThe methodologies followed\nThe challenges faced\nThe results obtained\nThe lessons learned\nLimitations and future plans\n\nGithub link submission that includes the following\nData Collection Scripts\nData Cleaning and Preprocessing Code\nAPI and Frontend Code\nExample annotations and how they are stored back into the database.\nDockerfile(s) for containerizing the different components of the project.\ndocker-compose.yml for orchestrating multi-container applications.\nBash/Python/Makefile scripts for automating the installation and setup process.\nContinuous Integration and Continuous Deployment (CI/CD)\nLogging\nFeedback\nYou will receive comments/feedback in addition to a grade.\n\n\nReferences\nKey References\nhttps://www.selenium.dev/\nhttps://scrapy.org/\nhttps://beautiful-soup-4.readthedocs.io/en/latest/\nhttps://abe2g.github.io/am-preprocess.html\nhttps://data.mendeley.com/datasets/p74pfhz3yx/1\nhttps://prometheus.io/\nhttps://airflow.apache.org/\nhttps://prodi.gy/\nhttps://medium.com/@swalperen3008/what-is-dockerize-and-dockerize-your-project-a-step-by-step-guide-899c48a34df6\nhttps://grafana.com/docs/grafana/latest/setup-grafana/set-up-grafana-monitoring/", "start_char_idx": 12525, "end_char_idx": 14002, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "197e6528-11b7-45ed-80d2-ee6800bc02ac": {"__data__": {"id_": "197e6528-11b7-45ed-80d2-ee6800bc02ac", "embedding": null, "metadata": {"source": "cB - 10 Academy - Careers - Week 2 - Tools for Remote Work.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99eb7832-16cf-4b6e-8300-fb674fb0651d", "node_type": "4", "metadata": {"source": "cB - 10 Academy - Careers - Week 2 - Tools for Remote Work.docx"}, "hash": "3987ed39b1e977fdd64b03d4de25a70c7b672fff7c1279192cfb6d2f8ac9c16a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fa8dda1-b3c5-4ded-9f93-2ded8dc0afce", "node_type": "1", "metadata": {}, "hash": "cf878f287d2954ba9013f89116d326581a7b4c04f3579018aef84298cedac4cb", "class_name": "RelatedNodeInfo"}}, "text": "10 Academy cB : Week 2\n\nCareers - Challenge 1\n\nTools for Remote Work\n\nSubmission Deadline: Saturday, 04th May 2024, 8pm UTC\n\nBackground\nWhat is Remote work ?\nRemote work is the practice of having employees carry out their duties away from a central office. Examples of such places include:\nA worker's home,\nPrivate office, \nShared workspace, or \nany other location besides the typical corporate campus or office building.\nOnboarding: The term \"onboarding\" describes the procedures used to integrate new hires and employees into the company. In addition to learning about the organisation's structure, culture, vision, mission, and values, it also contains activities that help new hires complete the first new-hire orientation process.\nWhy remote work?\nEmployees can completely customise their working setup. \nMay assist employees with disabilities.\nGreater talent pool for corporations, with nearly no regional restrictions. \nEmployees (as well as employers) save money and time travelling.\nEmployee productivity can increase with higher autonomy and fewer interruptions at work. \nThe working environment can be entirely customised by employees.\nCompanies and organisations are utilising digital management tools and services to assist them manage their work as remote work becomes more prevalent on a global scale. These are a handful of the most common remote work tools and applications:\nGoogle calendar\nSlack\nNotion\nTrello\nGoogle Calendar: Google Calendar is used for scheduling and organising events, appointments, and tasks, facilitating efficient time management and coordination.\nNotion is a project-management and note-taking software. It was designed to help companies and organisations coordinate deadlines, objectives, and assignments for the sake of efficiency and productivity.\nSlack is termed a \u201cdigital headquarters\u201d. With remote work steadily becoming the norm, Slack assists teams by organising different groups/teams/projects into workspaces, or \u201cchannels\u201d. This helps users to move between tasks, projects, and groups easily, negating the confusion of having all communication in one workspace. Slack also allows users to send direct messages to one another, and users can send files, audio/voice messages, images, gifs, videos, and can access internet calls.\nTrello is a web-based project management and collaboration tool that uses a card-based system to help individuals and teams organize tasks, projects, and workflows in a visual and flexible way.\n\nTask:\nYour company has moved to a different location and your team manager has tasked you with setting up the office workflow tools to assist with onboarding a new colleague in your team who has never used these tools before. The workflow tools you have to set up are: Slack, Notion, Google Calendar, and Trello.\n\nTask 1: Imagine that Pascaline is the new member joining your team.  Follow the below instructions and answer the question. \n\nOn Google calendar:\nAccess your Google Calendar.\nCreate a new calendar specifically for \"Onboarding Meeting - Pascaline\"\nSchedule a weekly check in team meeting, and invite the new team member. The team member\u2019s Email is: pascaline@10academy.org\nIn the description part, add an important description of what that weekly meeting will be about in general. \nSet up reminders for that weekly team meeting. \nTake a screenshot of your Google Calendar showing the scheduled events. \n\nOn Slack:\nCreate a new Slack workspace named \u201cOnboarding New Member\u201d, and create 3 new channels, in addition to the ones which Slack auto-creates. \nSend Pascaline an invite link, inviting them to join that Slack workspace. \nPost a welcome message in the channel introducing the purpose of the slack workspace and its objectives. \nInitiate a discussion about their experience so far in your company and ask for their input.\nTake a screenshot of the Slack channel with the introductory message and discussion visible.\n\nOn Notion:\nSet up a task board in Notion using the Kanban format. On this task board, you should list 5 key exercises you have completed for the training at 10 Academy in Week 0 and Week 1, along with the key feedback you have received from your tutors. \nLink your Notion task board to the Slack workspace you have created. You can do this using the Slack app directory. Please choose the channel that Pascaline has joined, to link your Notion account to. \nUsing your Notion account, create a public page for the task view, and provide this link in the Slack channel. \nTake a screenshot of the Task board you created above. \n\nTrello:\nCreate a new Trello board named \"Onboarding process\"\nSet up columns on the board for \"To Do,\" \"In Progress,\" \"Review,\" and \"Completed.\"\nCreate task cards for the onboarding week and tasks in the \"To Do\" column.", "start_char_idx": 0, "end_char_idx": 4754, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9fa8dda1-b3c5-4ded-9f93-2ded8dc0afce": {"__data__": {"id_": "9fa8dda1-b3c5-4ded-9f93-2ded8dc0afce", "embedding": null, "metadata": {"source": "cB - 10 Academy - Careers - Week 2 - Tools for Remote Work.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99eb7832-16cf-4b6e-8300-fb674fb0651d", "node_type": "4", "metadata": {"source": "cB - 10 Academy - Careers - Week 2 - Tools for Remote Work.docx"}, "hash": "3987ed39b1e977fdd64b03d4de25a70c7b672fff7c1279192cfb6d2f8ac9c16a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "197e6528-11b7-45ed-80d2-ee6800bc02ac", "node_type": "1", "metadata": {"source": "cB - 10 Academy - Careers - Week 2 - Tools for Remote Work.docx"}, "hash": "6bcad876d091dc104f144ca0de44b2d9a224a5b9a39ebc1f2eced8d1736e06a5", "class_name": "RelatedNodeInfo"}}, "text": "On this task board, you should list 5 key exercises you have completed for the training at 10 Academy in Week 0 and Week 1, along with the key feedback you have received from your tutors. \nLink your Notion task board to the Slack workspace you have created. You can do this using the Slack app directory. Please choose the channel that Pascaline has joined, to link your Notion account to. \nUsing your Notion account, create a public page for the task view, and provide this link in the Slack channel. \nTake a screenshot of the Task board you created above. \n\nTrello:\nCreate a new Trello board named \"Onboarding process\"\nSet up columns on the board for \"To Do,\" \"In Progress,\" \"Review,\" and \"Completed.\"\nCreate task cards for the onboarding week and tasks in the \"To Do\" column. In your to do column you should have the following 3 tasks: Introduction to the company and job description, Company platform tour, Introduction to current projects. \nMove one task card to the \"In Progress\" column.\nMove another task card to the \u201cReview\u201d column.\nTake a screenshot of the Trello board with the task cards visible.\n\nAfter completing those tasks on Google calendar, slack, notion and trello, create a google slide or PPT with the Guide on how you did each step on each of those platforms, and add a final result screenshots (The ones you took after every task) at the end of each Guide. \n\nNote: \nThe Guide should be clear that a new member, Pascaline, can follow to do the same tasks as you did.\nThe google slides or PPT title should be \u201cGuidance on Remote work Tools\u201d. The slides  should be simple, clean and professional - bullet points are fine.  The goal would be for this slide deck to be ready to use in a real-world work environment. \nUse a maximum of 6 slides for this Task 1.\n\nTask 2: \nWrite a Step-by-step guide showing your new members how to embed a new task in your Notion kanban board, and move it from backlog, to active to complete. Add the guide in the Google slide you created in Exercise 1. \nNote: Maximum slides for this second exercise is 3. \n\nTask 3:\nWrite a brief guide explanation showing the new members how to add a new member to their workspace on Slack. Add the guide in the Google slide you created in Exercise 1. \nNote: Maximum slides for this third question is 1.\n\nTask 4: \nProvide a guide on how to embed apps into Notion, such as Google, Figma, or Youtube, focussing on those that you think would be useful to 10 Academy training.\nNote: Maximum slides for this third question is 1.\n\nSubmission: On Tenx.\nFor this exercise, you have been provided with links which will direct you to the information you need in order to complete the above exercise. Please note, however, that the exercise cannot be completed based on the information in these links alone.\n\nUsefulness in real life:\nThis exercise prepares you for remote work, as you will have to create a realistic onboarding solution for possible future colleagues who have never worked remotely before. As more and more people start shifting to remote work completely, it will be important for you to understand the most common remote tools, how they work, and how to easily explain their uses to others\n\nMarking Rubric:\n\nGoogle Calendar Setup (40 points): You accurately created a new calendar and scheduled a weekly check-in meeting with reminders. Clear documentation with screenshots was provided.\nSlack Workspace Setup (10 points): You successfully created a new Slack workspace, invited new members, and introduced the workspace's objectives with an engaging discussion.\nNotion Task Board (10 points): You effectively set up a Notion task board with Kanban format, listed exercises and feedback, integrated it with Slack, and provided a public link within the Slack channel.\nTrello Board Setup (10 points): You created a Trello board with the required columns and task cards, correctly managed task cards, and provided a clear screenshot of the board.\nGoogle Slides Presentation (10 points): The Google Slides presentation was well-structured, clear, and professional. It included step-by-step guidance for each platform and final result screenshots.\nTask 2 - Notion Task Management Guide (20 points): The guide for Notion task management was clear and accurate, integrated into the Google Slides presentation.\nTask 3: Adding New Members to Slack (20 points): The guide for adding new members to Slack was clear, concise, and integrated into the Google Slides presentation.\nTask 4: The guide on how to embed apps into Notion. (20 points)", "start_char_idx": 3976, "end_char_idx": 8495, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "535fb58a-bbef-423f-972c-3592d270b0f7": {"__data__": {"id_": "535fb58a-bbef-423f-972c-3592d270b0f7", "embedding": null, "metadata": {"source": "W0 - Careers Exercise - CV Submission-day2.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8706c933-385c-4f7a-8d1b-0f7c40870e61", "node_type": "4", "metadata": {"source": "W0 - Careers Exercise - CV Submission-day2.docx"}, "hash": "3fa2505ab0169fcb5f9e2a44b2b990d3f803afc36f3ec4c4edfd5eff339d84da", "class_name": "RelatedNodeInfo"}}, "text": "10 Academy cB : Week 0\nCareers - Exercise 1\n\nCV\n\nFirst submission deadline: Tuesday, 9th April 2024, 8 pm UTC\n\nSecond resubmission deadline: Saturday, 13th April 2024, 8pm UTC\nExercise\nSubmit your CV by carefully following the instructions and requirements\nlisted below. \n\nThe following link provides more information and tips on constructing a CV: CV writing tips.\n\nRequirements: \n1 page CV in PDF format \n\nYour email address, phone number, and address should be listed right at the top. \n\nProvide a 50-word summary of yourself. This concise introduction provides the hiring team with information about you, your key qualifications, accomplishments, and experience. It must be brief and pertinent to the job for which you are seeking. \n\nList all of your most important technical skills and soft skills.\nBe careful not to list too many skills. \nLimit yourself to skills most relevant to your application.\nExamples of technical skills are proficiency in Python, SQL, C++, etc.\nExamples of soft skills include Communication,  Teamwork, Problem-solving, Time management, Critical thinking, Decision-making and Organisational skills.   \nConsider how best to make this easy for an employer to skim and understand what you are capable of delivering, by organising into relevant sections.\n\nList your education in chronological order (starting with the most recent educational achievement attained).  For each educational experience, include the name and location of the institution, year(s) of study, expected date of completion (if applicable) and title or certificate attained. If there are more than 3 educational experiences, highlight only those which are relevant to the position you are applying for. These might include courses in Mathematics, design, algorithms, statistics, Data Science,etc. \n\nUse digits instead of spelled-out numbers where necessary.\n\nUsing Bock\u2019s rule, describe your experience well. \n\nSee CV template example here: Sample CV Template\n\nOther Checklist Includes \nAccurate contact information (e.g.: include country code to your phone number)\nInclude a link to an online portfolio that expands on your experiences, projects, achievements, etc. \nEnsure all links (LinkedIn, portfolio, email, etc.) are clickable\nTake note of international naming conventions (first name last name)\nRelevant and targeted experience \nProofread the document to confirm there are no spelling mistakes\nUse a consistent and readable font type and size\nDo not include any extra fancy formatting\nMinimal use of colours\nBe clear and concise; keep your CV to 1 page, since recruiters may not have time to review more than 1 page. \nSaved and submitted as a PDF.\n\nSupport, Meeting and Tutorials\nOn Tuesday, 9th April 2024, there will be a tutorial for this exercise. Please get in touch with Pascaline Iyodusenga on Slack or during the tutorial session if you need assistance or have any queries. \nMarking Rubric\nContent Relevance and Clarity:\nPersonal Information: Check if the CV includes the candidate's full name, phone number, email address, and physical address (optional).\nProfessional Summary: Evaluate the professional summary is a 50 words brief description of themselves in relation to what they can do and concepts and tools they are familiar with. The professional summary should serves a an introduction to the trainee's CV and shows an overview of a trainees\u2019 background as well career aspirations. \nSkills: Ensure that the CV lists relevant skills, both technical and soft skills.\nWork Experience: Verify the presence of detailed descriptions for each job position held, including job title, company name, dates of employment, and a concise overview of key responsibilities and achievements (quantify with data whenever possible). The list of experiences should be in choronological order.\nEducation: Check if the CV includes relevant educational qualifications, a chronological list of degrees, including institution name, degree name, major/minor (if applicable), and graduation year. Note that Expected graduation date can be included if applicable.\nLicence & Certifications. This is optional, but check a if relevant certifications are listed in reverse-chronological order\n\n\n\nProfessionalism & Formating \nLayout and Structure: Evaluate the CV's organization, including clear section headers (e.g., Education, Work Experience, Skills) and a logical flow of information.\nFont and Style: Check for consistency in font type, size, and formatting throughout the document.\nSpelling and Grammar: Identify and flag any spelling or grammatical errors, ensuring a polished final product.\nLength: Assess if the CV is appropriately concise while still providing sufficient detail, typically aiming for one to two pages in length.\nProfessional Appearance: Look for the Overall layout and design that convey professionalism and attention to detail.\n\n\nNote: Individual feedback given.\nUsefulness in real life\nA good CV is the foundation of a successful career launch. A properly outlined CV shows both your past accomplishments and your potential for future success.\nSubmission\nUpload a PDF on Tenx.", "start_char_idx": 0, "end_char_idx": 5096, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93e4ae38-a046-457f-9d23-96f5155bb1c8": {"__data__": {"id_": "93e4ae38-a046-457f-9d23-96f5155bb1c8", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Week 1 Challenge.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d3f5a848-f1f9-4ba3-b78b-f2246d4186aa", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Week 1 Challenge.docx"}, "hash": "8e38a22c0397fbac740fd72141036d40a5edc99d129ba417773c3b70ed0989ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5086421-dd73-499c-9157-71e45a9463ea", "node_type": "1", "metadata": {}, "hash": "ab9d43e9f33cec5e8f3b37ed3b8394825a18c6a21bdc6e984bf1788a8bd3430b", "class_name": "RelatedNodeInfo"}}, "text": "10 Academy Cohort B - Weekly Challenge: Week 1 \nUser Analytics in the Telecommunication Industry - Overview\nSituational Overview (Business Need)\nYou are working for a wealthy investor that specialises in purchasing assets that are undervalued.  This investor\u2019s due diligence on all purchases includes a detailed analysis of the data that underlies the business, to try to understand the fundamentals of the business and especially to identify opportunities to drive profitability by changing the focus of which products or services are being offered.\n\nYour last role with this investor saw you do a rich analysis of a delivery company and you helped to identify that delivery to university students was the most profitable route to follow, and your analysis helped the investor purchase this delivery company and ramp up profits by 25% within 6 months through focussing on the most profitable aspect of the business.  This was driven by university students always being hungry, awake at all hours, willing to purchase from a limited food menu and tending to live within a small geographical area.\n\nThe investor is interested in purchasing TellCo, an existing mobile service provider in the Republic of Pefkakia.  TellCo\u2019s current owners have been willing to share their financial information but have never employed anyone to look at their data that is generated automatically by their systems.\n\nYour employer wants you to provide a report to analyse opportunities for growth and make a recommendation on whether TellCo is worth buying or selling.  You will do this by analysing a telecommunication dataset that contains useful information about the customers & their activities on the network. You will deliver insights you managed to extract to your employer through an easy to use web based dashboard and a written report. \nData\nWe have sourced the data from a month's aggregation of xDR records. \nThe description for attributes can be found here\nThis data should be extracted from a PostgreSQL database, and we've included both the database schema and the corresponding SQL file here.  \nLearning Outcomes\nUnderstanding and reasoning the business context. Thinking about suitable analysis that matches the business need. Thinking about clients and their interests.\nUnderstanding the data provided and extract insight. You will have to explore different techniques, algorithms, statistical distributions, sampling, and visualisation techniques to gain insight.\nUnderstand the data structure and algorithms used in EDA and machine learning pipelines\nBuilding a dashboard to explore data as well as to communicate insight. Advanced use of modules such as plotly, seaborn, matplotlib etc. to build descriptive visualisations. Reading through the modules documentation to expand your skill set.\nThinking about statistical distributions, sampling, bias, overfitting, correlations.\nModular and object oriented python code writing. Python package building. \nCompetency Mapping\nThe tasks you will carry out in this week\u2019s challenge will contribute differently to the 11 competencies 10 Academy identified as essential for job preparedness in the field of Data Engineering, and Machine Learning engineering. The mapping below shows the change (lift) one can obtain through delivering the highest performance in these tasks.   \n\n\n\nTeam\nTutors: \nYabebal\nEmtinan \nRehmet\nNatnael\n\nBadges\nEach week, one user will be awarded one of the badges below for the best performance in the category below.\n\nIn addition to being the badge holder for that badge, each badge winner will get +20 points for the leaderboard score.\n\nVisualisation - quality of visualisations, understandability, skimmability, choice of visualisation\nQuality of code - reliability, maintainability, efficiency, commenting - in future this will be CICD\nInnovative approach to analysis -using latest algorithms, adding in research paper content and other innovative approaches\nWriting and presentation - clarity of written outputs, clarity of slides, overall production value\nMost supportive in the community - helping others, adding links, tutoring those struggling\n\nThe goal of this approach is to support and reward expertise in different parts of the Machine Learning Engineer toolbox.\nGroup Work Policy\nThis submission is to be done individually. Collaborative learning is encouraged, but each person must have his or her own submissions.\nLate Policy\nOur goal is to prepare successful trainees for the work and submitting late, when given enough notice, shouldn\u2019t be necessary.\n\nFor interim submissions, those submitted 1-6 hours late will receive a maximum of 50% of the total possible grade.  Those submitted >6 hours late may receive feedback, but will not receive a grade.\n\nFor final submissions, those submitted 1-24 hours late, will receive a maximum of 50% of the total possible grade. Those submitted >24 hours late may receive feedback, but will not receive a grade.\n\nWhen calculating the leaderboard score:\nFrom week 4 onwards, your lowest week\u2019s score will not be considered.\nFrom week 8 onwards, your two lowest weeks\u2019 scores will not be considered.", "start_char_idx": 0, "end_char_idx": 5119, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5086421-dd73-499c-9157-71e45a9463ea": {"__data__": {"id_": "c5086421-dd73-499c-9157-71e45a9463ea", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Week 1 Challenge.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d3f5a848-f1f9-4ba3-b78b-f2246d4186aa", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Week 1 Challenge.docx"}, "hash": "8e38a22c0397fbac740fd72141036d40a5edc99d129ba417773c3b70ed0989ba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93e4ae38-a046-457f-9d23-96f5155bb1c8", "node_type": "1", "metadata": {"source": "10 Academy Cohort B - Week 1 Challenge.docx"}, "hash": "32467f76b4b9da84666b7735add459fa1f9d63cb7498a2fe108003d19afb7c75", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5cba45a4-5a27-404b-ba56-9767f5f47759", "node_type": "1", "metadata": {}, "hash": "0362d073ab40dbe7d734a4cecaa9b364d2571adee0d5d944992064748f89baf0", "class_name": "RelatedNodeInfo"}}, "text": "Group Work Policy\nThis submission is to be done individually. Collaborative learning is encouraged, but each person must have his or her own submissions.\nLate Policy\nOur goal is to prepare successful trainees for the work and submitting late, when given enough notice, shouldn\u2019t be necessary.\n\nFor interim submissions, those submitted 1-6 hours late will receive a maximum of 50% of the total possible grade.  Those submitted >6 hours late may receive feedback, but will not receive a grade.\n\nFor final submissions, those submitted 1-24 hours late, will receive a maximum of 50% of the total possible grade. Those submitted >24 hours late may receive feedback, but will not receive a grade.\n\nWhen calculating the leaderboard score:\nFrom week 4 onwards, your lowest week\u2019s score will not be considered.\nFrom week 8 onwards, your two lowest weeks\u2019 scores will not be considered. \n\nInstructions\n\nAt the end of this week you are expected to to have a complete project that has\nReusable code for data preparation and cleaning.\nEstimated code complexity (running time and memory size order of magnitude estimate) for the key part of your code. To do so you would need to explicitly identify and profile the data structures and algorithms in your code.\nCode connected using scikit pipeline or other form of chaining multiple EDA steps  \nBeautiful and insightful Streamlit dashboard that shows your findings.\nSQL database as feature store which can be used to store selected features for dashboard visualisation and model training.\nYour project folder should mirror as close as possible the example here. In particular\nCode is installable via pip\nHas unit tests with good test coverage\nHas CI/CD setup - using Github Actions\nHas Dockerfile to build it as docker image\nPython codes are written in the style and structure of Streamlit python source code - reading and understanding source codes of well-known open source packages will help you learn advanced python programming.\n\nThe global objective is divided into 4 sub-objectives \nUser Overview analysis\nUser Engagement analysis\nUser Experience analysis\nUser Satisfaction analysis\nTask 1 - Report on Streamlit understanding\nStreamlit Source Code Overview: Provide a one-page comprehensive overview of Streamlit source code structure. Your report should emphasise on \nHow the python source code is structured\nThe different advanced python programming techniques used in the code base \nHow unit and integration tests are written\nWhat extra packages are used - check the requirements.txt file\nHow doc strings, logging,  and error handling  are written\nHow @property and other builtin and custom decorators are used in the source code\nAny other observations that interested you.\nIdentifying the data structure and algorithm used in the key component of the data loading part of the streamlit package. \nTask 2 - User Overview analysis \n\nThe lifeblood of any business is its customers. Businesses are always finding ways to better understand their customers so that they can provide more efficient and tailored solutions to them. \nExploratory Data Analysis is a fundamental step in the data science process. It involves all the processes used to familiarise oneself with the data and explore initial insights that will inform further steps in the data science process.\n\nIt is always better to explore each data set using multiple exploratory techniques and compare the results. The goal of this step is to understand the dataset, identify the missing values & outliers if any using visual and quantitative methods to get a sense of the story it tells. It suggests the next logical steps, questions, or areas of research for your project.\n\nStart by applying your streamlite understanding to prepare your code with modular design, best practices in coding.\n\nFor the actual telecom dataset, you\u2018re expected to conduct a full User Overview analysis & the following sub-tasks are your guidance: \nStart by identifying the top 10 handsets used by the customers.\nThen, identify the top 3 handset manufacturers\nNext, identify the top 5 handsets per top 3 handset manufacturer\nMake a short interpretation and recommendation to marketing teams\n\nIn telecommunication, CDR or Call Detail Record is the\u00a0voice\u00a0channel and XDR is the\u00a0data\u00a0channel equivalent. So here, consider xDR as data sessions Detail Record. In xDR, user behaviour can be tracked through the following applications:  Social Media, Google, Email, Youtube, Netflix, Gaming, Other. \n \nTask 2.1 - Your employer wants to have an overview of the users\u2019 behaviour on those applications.   \nAggregate per user the following information in the column  \nnumber of xDR sessions\nSession duration\nthe total download (DL) and upload (UL) data\nthe total data volume (in Bytes) during this session for each application\n\nTask 2.2 - Conduct an exploratory data analysis on those data & communicate useful insights. Ensure that you identify and treat all missing values and outliers in the dataset by replacing by the mean of the corresponding column.", "start_char_idx": 4243, "end_char_idx": 9268, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5cba45a4-5a27-404b-ba56-9767f5f47759": {"__data__": {"id_": "5cba45a4-5a27-404b-ba56-9767f5f47759", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Week 1 Challenge.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d3f5a848-f1f9-4ba3-b78b-f2246d4186aa", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Week 1 Challenge.docx"}, "hash": "8e38a22c0397fbac740fd72141036d40a5edc99d129ba417773c3b70ed0989ba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5086421-dd73-499c-9157-71e45a9463ea", "node_type": "1", "metadata": {"source": "10 Academy Cohort B - Week 1 Challenge.docx"}, "hash": "6d89461f667af01611809fcfc34d0f95c27b3ca7540c06dee0ee4baf0cd4a12a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd027211-1b03-4652-a363-f601de8f98e9", "node_type": "1", "metadata": {}, "hash": "98129adc6ee47db984f9b9e90c795e7fddc1c0b8619d639a5e1375e12d4b1b0d", "class_name": "RelatedNodeInfo"}}, "text": "So here, consider xDR as data sessions Detail Record. In xDR, user behaviour can be tracked through the following applications:  Social Media, Google, Email, Youtube, Netflix, Gaming, Other. \n \nTask 2.1 - Your employer wants to have an overview of the users\u2019 behaviour on those applications.   \nAggregate per user the following information in the column  \nnumber of xDR sessions\nSession duration\nthe total download (DL) and upload (UL) data\nthe total data volume (in Bytes) during this session for each application\n\nTask 2.2 - Conduct an exploratory data analysis on those data & communicate useful insights. Ensure that you identify and treat all missing values and outliers in the dataset by replacing by the mean of the corresponding column.\nYou\u2019re expected to report about the following using  python script and slide  :\nDescribe all  relevant variables and associated data types (slide). \nAnalyze the basic metrics (mean, median, etc) in the Dataset (explain) & their importance for the global objective.\nConduct a Non-Graphical Univariate Analysis by computing dispersion parameters for each quantitative variable and provide useful interpretation. \nConduct a Graphical Univariate Analysis by identifying the most suitable plotting options for each variable and interpret your findings.\nBivariate Analysis \u2013 explore the relationship between each application & the total DL+UL data using appropriate methods and interpret your findings. \nVariable transformations \u2013 segment the users into top five decile classes based on the total duration for all sessions and compute the total data (DL+UL) per decile class. \nCorrelation Analysis \u2013 compute a correlation matrix for the following variables and interpret your findings: Social Media data, Google data, Email data, Youtube data, Netflix data, Gaming data, Other data \nDimensionality Reduction \u2013 perform a principal component analysis to reduce the dimensions of your data and provide a useful interpretation of the results (Provide your interpretation in four (4) bullet points-maximum). \nTask 3 - User Engagement analysis\nAs telecom brands are the data providers of all online activities, meeting user requirements, and creating an engaging user experience is a prerequisite for them. Building & improving the QoS (Quality of Service) to leverage the mobile platforms and to get more users for the business is good but the success of the business would be determined by the user engagement and activity of the customers on available apps.\u00a0\n\nIn telecommunication, tracking the user activities on the database sessions is a good starting point to appreciate the user engagement for the overall applications and per application as well. If we can determine the level of engagement of a random user for any application, then it could help the technical teams of the business to know where to concentrate network resources for different clusters of customers based on the engagement scores.\n\nIn the current dataset you\u2019re expected to track the user\u2019s engagement using the following engagement metrics: \nsessions frequency \nthe duration of the session \nthe sessions total traffic (download and upload (bytes))\n\nTask 3.1 - Following the above, perform the following tasks:\nAggregate the above metrics per customer id (MSISDN) and report the top 10 customers per engagement metric \nNormalize each engagement metric and run a k-means (k=3) to classify customers in three groups of engagement. \nCompute the minimum, maximum, average & total non-normalized metrics for each cluster. Interpret your results visually with accompanying text explaining your findings.\nAggregate user total traffic per application and derive the top 10 most engaged users per application\nPlot the top 3 most used applications using appropriate charts.  \nUsing k-means clustering algorithm, group users in k engagement clusters based on the engagement metrics: \nWhat is the optimized value of k (use elbow method for this)?  \nInterpret your findings. \n\nTask 4 - Experience Analytics\nThe Telecommunication industry has experienced a great revolution since the last decade. Mobile devices have become the new fashion trend and play a vital role in everyone's life. The success of the mobile industry is largely dependent on its consumers. Therefore, it is necessary for the vendors to focus on their target audience i.e. what are the needs and requirements of their consumers and how they feel and perceive their products. Tracking & evaluation of customers\u2019 experience can help the organizations to optimize their products and services so that it meets the evolving user expectations, needs, and acceptance.\n\nIn the telecommunication industry, the user experience is related, most of the time, to network parameter performances or the customers\u2019 device characteristics.  \n\nIn this section, you\u2019re expected to focus on network parameters like TCP retransmission, Round Trip Time (RTT), Throughput, and the customers\u2019 device characteristics like the handset type to conduct a deep user experience analysis. The network parameters are all columns in the dataset. The following questions are your guidance to complete the task. For this task you need a python script that includes all solutions to tasks.", "start_char_idx": 8524, "end_char_idx": 13744, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd027211-1b03-4652-a363-f601de8f98e9": {"__data__": {"id_": "cd027211-1b03-4652-a363-f601de8f98e9", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Week 1 Challenge.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d3f5a848-f1f9-4ba3-b78b-f2246d4186aa", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Week 1 Challenge.docx"}, "hash": "8e38a22c0397fbac740fd72141036d40a5edc99d129ba417773c3b70ed0989ba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5cba45a4-5a27-404b-ba56-9767f5f47759", "node_type": "1", "metadata": {"source": "10 Academy Cohort B - Week 1 Challenge.docx"}, "hash": "5e96bdf192ec294e5f65de6232e9c483ebe76ed6766f66580b540057ba7d153a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d21f329-2c8d-453c-90d0-b18559722131", "node_type": "1", "metadata": {}, "hash": "0aa2819973bfef188a65d96c66282b6ee8e8483b2d9c79f1e2a3311c6636f97b", "class_name": "RelatedNodeInfo"}}, "text": "The success of the mobile industry is largely dependent on its consumers. Therefore, it is necessary for the vendors to focus on their target audience i.e. what are the needs and requirements of their consumers and how they feel and perceive their products. Tracking & evaluation of customers\u2019 experience can help the organizations to optimize their products and services so that it meets the evolving user expectations, needs, and acceptance.\n\nIn the telecommunication industry, the user experience is related, most of the time, to network parameter performances or the customers\u2019 device characteristics.  \n\nIn this section, you\u2019re expected to focus on network parameters like TCP retransmission, Round Trip Time (RTT), Throughput, and the customers\u2019 device characteristics like the handset type to conduct a deep user experience analysis. The network parameters are all columns in the dataset. The following questions are your guidance to complete the task. For this task you need a python script that includes all solutions to tasks.\n\nTask 4. 1 - Aggregate, per customer, the following information (treat missing & outliers by replacing by the mean or the mode of the corresponding variable):\nAverage TCP retransmission\nAverage RTT\nHandset type\nAverage throughput\nTask 4.2 - Compute & list 10 of the top, bottom and most frequent:\nTCP values in the dataset. \nRTT values in the dataset.\nThroughput values in the dataset.\nTask 4.3 - Compute & report:\nThe distribution of the average throughput  per handset type and provide interpretation for your findings.\nThe average TCP retransmission view per handset type and provide interpretation for your findings.\nTask 4.4 - Using the experience metrics above, perform a k-means clustering (where k = 3) to segment users into groups of experiences and provide a brief description of each cluster. (The description must define each group based on your understanding of the data)\nTask 5 - Satisfaction Analysis\nAssuming that the satisfaction of a user is dependent on user engagement and experience, you\u2019re expected in this section to analyze customer satisfaction in depth. The following tasks will guide you: \n\nBased on the engagement analysis + the experience analysis you conducted above ,\nTask 5. 1 - Write a python program to assign:\nengagement score to each user. Consider the engagement score as the Euclidean distance between the user data point & the less engaged cluster (use the first clustering for this) (Euclidean Distance)\nexperience score to each user. Consider the experience score as the Euclidean distance between the user data point & the worst experience\u2019s cluster. \nTask 5.2 - Consider the average of both engagement & experience scores as  the satisfaction score & report the top 10 satisfied customer \nTask 5.3 - Build a regression model of your choice to predict the satisfaction score of a customer. \nTask 5.4 - Run a k-means (k=2) on the engagement & the experience score . \nTask 5.5 - Aggregate the average satisfaction & experience score per cluster. \nTask 5.6 - Export your final table containing all user id + engagement, experience & satisfaction scores in your local MySQL database. Report a screenshot of a select query output on the exported table. \nTask 5.7 Model deployment tracking- deploy the model and monitor your model. Here you can use Docker or other MlOps tools which can help you to track your model\u2019s change.  Your model tracking report includes code version, start and end time, source, parameters, metrics (loss convergence) and artefacts or any output file regarding each specific run. (CSV file, screenshot)\n\n \n\nTutorials Schedule\nOverview\nMonday: Understanding week 1 challenge and Data Pre-processing\nTuesday: Data Exploration and Insights communication\nWednesday: Data modelling\nThursday: Dashboard Development\n\nIn the following, the colour purple indicates morning sessions, and blue indicates afternoon sessions.\nMonday: Data Pre-processing\nHere, students will understand this week's challenge and learn how to prepare data for modelling.\n\nIntroduction to week1 challenge(YF)\nGoing through the Streamlit source code (Rehmet)\n\nKey Performance Indicators:\n\nUnderstanding advanced code base\nEfficient and modular coding technique\nUnderstanding data in DB \u2013 connecting with DB \nAbility to help others \nTuesday: EDA\nHere, students will learn how to understand data and communicate insights from the data.\n\nData Extraction, Cleaning, Transforming and formatting using modular python (Emtinan)\nWorking with PostgreSQL DB using SQL, Pandas, and SQLAlchemy (Emtinan And Rehmet)\n\nKey Performance Indicators:\n\nProactivity to self-learn - sharing references\nIntermediate to advanced SQL techniques \nData Understanding and Exploration\nWednesday: Data Modelling\nHere, students will learn how to model the data.", "start_char_idx": 12708, "end_char_idx": 17503, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d21f329-2c8d-453c-90d0-b18559722131": {"__data__": {"id_": "5d21f329-2c8d-453c-90d0-b18559722131", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Week 1 Challenge.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d3f5a848-f1f9-4ba3-b78b-f2246d4186aa", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Week 1 Challenge.docx"}, "hash": "8e38a22c0397fbac740fd72141036d40a5edc99d129ba417773c3b70ed0989ba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd027211-1b03-4652-a363-f601de8f98e9", "node_type": "1", "metadata": {"source": "10 Academy Cohort B - Week 1 Challenge.docx"}, "hash": "295a4b8deee96d18b949dbd6f1a3a3f2f85122ff984eadf89abaac0a51b1aacf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2c40a1d-2be6-42e1-80a5-6cb208c16cff", "node_type": "1", "metadata": {}, "hash": "9131137ba46b46ef49234b82ee2079e9eb1676c06218e9b47f283b9c50c27301", "class_name": "RelatedNodeInfo"}}, "text": "Monday: Data Pre-processing\nHere, students will understand this week's challenge and learn how to prepare data for modelling.\n\nIntroduction to week1 challenge(YF)\nGoing through the Streamlit source code (Rehmet)\n\nKey Performance Indicators:\n\nUnderstanding advanced code base\nEfficient and modular coding technique\nUnderstanding data in DB \u2013 connecting with DB \nAbility to help others \nTuesday: EDA\nHere, students will learn how to understand data and communicate insights from the data.\n\nData Extraction, Cleaning, Transforming and formatting using modular python (Emtinan)\nWorking with PostgreSQL DB using SQL, Pandas, and SQLAlchemy (Emtinan And Rehmet)\n\nKey Performance Indicators:\n\nProactivity to self-learn - sharing references\nIntermediate to advanced SQL techniques \nData Understanding and Exploration\nWednesday: Data Modelling\nHere, students will learn how to model the data. \n\nModelling (Emtinan)\n\nKey Performance Indicators:\n\nModelling\nMLOps\nProactivity to self-learn - sharing references\nThursday: Dashboard Development\nHere, students will learn how to visually communicate data and insights using dashboards. \n\nDashboard development (Rehmet)\n\nKey Performance Indicators:\n\nDesign Thinking\nDashboard Design\nProactivity to self-learn - sharing references\n\n\nDeliverables\nMAKE SURE YOUR SUBMISSION IS IN A PDF FORMAT WHEN ASKED TO UPLOAD A WORD/PPT FILE.\nInterim Submission (Due Wednesday 24.04.2024 2000 UTC)\nA PDF containing concise and comprehensive descriptions of the key data structure and algorithm you used in your code to address Task 2 and Task 3 in 1 or 2 pages. If you use pandas, scikit-learn etc. in your code, report on the data structure and algorithm used in the part of those packages you used. Comment on how long (time) and how much memory (RAM size) will it take your code if the data size grows by 10x, 1000x, 100000x, 1000000x.  Comment on the suitability of the data structure and algorithm you used for large data sets.\nLink to your GitHub repository. \nFeedback\nYou may not receive detailed comments on your interim submission, but will receive a grade.\nFinal Submission (Due Sat 27.04.2024 2000 UTC)\nReport for Task 1 - Streamlit source code review (1-3 pages).\nSummarise your findings from Task 2 to 5 (Customers Overview, User Engagement, Experience and Satisfaction Analysis).  Your employer demands no more than 30 slides - 1hr presentation.\n  \nEnsure that you include a title slide in the beginning and a reference slide at the end.\nEnsure that you make a recommendation to your employer on the growth potential of the company (positive or negative) based on the data.  \nEnsure that you share the data and slides with justifying your recommendation with data and graphs\nEnsure that you outline the limitations of your analysis.\nEnsure that you make a recommendation on whether your employer should purchase this company.\n\nA Github link to all your codes and a screenshot of your dashboard. Make sure to include as many screen shots as possible so that we are able to see all the functionality of your dashboard - your evaluation for the dashboard wireframe and functionality will be entirely based on the screenshots (or deployed version) you submitted. Make an effort to deploy your dashboard live in Streamlit Cloud Heroku, Netlify or any one of the free web app hosting services you can find here. \n\nFeedback\nYou will receive comments/feedback in addition to a grade.\n\nLeaderboard for the week\nThere are 100 points available for the week.\n\n20 points - community growth and peer support.  \nThis includes supporting other learners by answering questions (Slack), asking good questions (Slack), participating (not only attending) daily standups (GMeet) and sharing links and other learning resources with other learners.\n\n25 points - presentation and reporting.\n\t5 points - interim submission\n\t\t2 - Evidence of clear understanding of the business context and data\n\t\t2 - Evidence of clear plan to complete the project and what is done\n       till date\n\t\t1 - Style of report \n20 points for the final submission.  This is measured through:\n4 - Style and quality of report (e.g. error free, font and format consistency)\n4 - Creative articulation, clarity of content, and objective communication  \n10 - Clear sections on \n1 - objectives of the project and the intended business value\n1 - data size, type, format and other details (e.g.", "start_char_idx": 16620, "end_char_idx": 20988, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2c40a1d-2be6-42e1-80a5-6cb208c16cff": {"__data__": {"id_": "f2c40a1d-2be6-42e1-80a5-6cb208c16cff", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Week 1 Challenge.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d3f5a848-f1f9-4ba3-b78b-f2246d4186aa", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Week 1 Challenge.docx"}, "hash": "8e38a22c0397fbac740fd72141036d40a5edc99d129ba417773c3b70ed0989ba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d21f329-2c8d-453c-90d0-b18559722131", "node_type": "1", "metadata": {"source": "10 Academy Cohort B - Week 1 Challenge.docx"}, "hash": "efb4d2295b91d892ef9848243be43ca3d923c9d6f0733760ec13d6eedb0f51b1", "class_name": "RelatedNodeInfo"}}, "text": "20 points - community growth and peer support.  \nThis includes supporting other learners by answering questions (Slack), asking good questions (Slack), participating (not only attending) daily standups (GMeet) and sharing links and other learning resources with other learners.\n\n25 points - presentation and reporting.\n\t5 points - interim submission\n\t\t2 - Evidence of clear understanding of the business context and data\n\t\t2 - Evidence of clear plan to complete the project and what is done\n       till date\n\t\t1 - Style of report \n20 points for the final submission.  This is measured through:\n4 - Style and quality of report (e.g. error free, font and format consistency)\n4 - Creative articulation, clarity of content, and objective communication  \n10 - Clear sections on \n1 - objectives of the project and the intended business value\n1 - data size, type, format and other details (e.g. missing values) \n1 - methods and algorithms used\n1 - details on pipeline, automation, (code, data, model) versioning\n2 - well produced supporting figures and graphics\n2 - result and discussion\n2 - summary of what has been achieved, its implication, and weather\n     objectives of the project are met or not and why   \n2 - Balance between being \u2018full of information\u2019 and \u2018easy to understand\u2019 \n\n20 points - dashboard code, screenshot, and cloud deployment\n\t10 points - screenshot & dashboard code submission \n\t\t2 - functionality of dashboard to address business need\n\t\t2 - multiple screenshot or live access \n\t\t2 - proof of wireframe design\n\t\t2 - CI/CD deployment\n\t10 points - dashboard code and deployment \n2 - Dockerfile to build the dashboard \n2 - Github actions and unit test\n2 - Quality and structure of code\n2 - incorporating javascript/css to enhance dashboard\n2 - sql schema of feature store database\n \n40 points - data analysis and coding\n\t15 points - interim submission\n\t\t4 - Preprocessing &  EDA  \n2 - Generating novel plots - insightful and quality plots\n2 - Frequent github commits, multiple branching, and pull request \n3 - Modularity and quality of code (including readability) \n4 - use of scikit pipeline or other pipeline approach\n\n30 points - final submission\n\t10 -  Depth of preprocessing &  EDA\n\t5   - Pipeline driven analysis \n5   - plots and notebook - insightful plots and well commented sections  \n6   - Advanced github use, modularity, and quality of code \n4   - Github actions and unit test \n\n\nReferences\nGeneral references\nExploratory Data Analysis In Python\nNon Graphical Univariate Analysis 1\nNon Graphical Univariate Analysis 2\nUnivariate and Bivariate Analysis\nHow to define an outlier\nHow to Correlation Analysis\nHow to do PCA (Video)\nDefine telecoms QoS\nAn Oracle Data Science Case Study in Telecom\nhttps://www.statology.org/deciles-in-python/\nUse cases and challenges in telecom big data analytics paper (PDF) Use cases and challenges in telecom big data analytics\nhttps://github.com/10-Academy-Self-Learning-Resources/Data-Understanding\nhttps://github.com/10-Academy-Self-Learning-Resources/DataVisualization\nhttps://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html\nhttps://strategyanalytics.medium.com/pandas-read-excel-removed-support-for-xlsx-files-426e4acfde89\nhttps://www.mlflow.org/docs/latest/tutorials-and-examples/tutorial.html\nDesign\nApp Layout & Style Tips | Designing Apps for User (Part II) (streamlit.io)\nMarvel | Create your free account (marvelapp.com)", "start_char_idx": 20101, "end_char_idx": 23521, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ed6b151-b867-4ba5-9f5e-4ba929044d23": {"__data__": {"id_": "8ed6b151-b867-4ba5-9f5e-4ba929044d23", "embedding": null, "metadata": {"source": "Teamwork.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "485e9781-7c77-4015-a0e4-1f399abd0e2d", "node_type": "4", "metadata": {"source": "Teamwork.docx"}, "hash": "5eacfb6e5da64dad22c8c9de84a71b68bf83578cbefbe5e837ae08026bbc03a3", "class_name": "RelatedNodeInfo"}}, "text": "10 Academy cB : Week 5\n\nCareers - Exercise 2\n\nTeamwork\n\nDue Date: June 1st , 2024. 8:00 PM UTC\n\n\nIntroduction\n\nWorking as a team is an essential aspect of success in many fields, especially in the dynamic and complex world of data and AI engineering. Effective teamwork brings together individuals with unique skills, perspectives, and ideas, creating a powerful force for innovation and problem-solving.\n\nIn your future careers as data, ML, and AI engineers, you will likely collaborate with various experts, including software developers, data scientists, project managers, and domain specialists. Embracing teamwork will enable you to tackle complex projects, share knowledge, and achieve results that exceed what any individual could accomplish alone. \n\nThrough this exercise, you will;\nExplore the fundamentals of teamwork.\nReflect on your collaborative experiences.\nDevelop insights into maximizing team potential. \n\nRemember, employers highly value individuals who can work effectively in a team, contribute positively to the group dynamic, and inspire those around them.\n\n\nExercise\nDuring weeks 3-4 at 10 Academy, you worked on a group work challenge to build a Redash chatbot add-on. For this exercise, you are required to reflect on your team\u2019s participation in tackling the challenge, your experiences, and your contributions to the final team report/project.\n\nNote: You can choose to use any of the team projects from wk 3-6.\n\nThis reflective exercise is designed to assess your team's performance and collaboration. By evaluating your past experiences, you can gain valuable insights into your team's strengths and areas for improvement, which will ultimately enhance your future project undertakings.\n\n\nNote: There are no penalties for honest reflections. Your feedback is important as it helps foster a collaborative and growth-oriented mindset within the team and beyond.\n\n\n\nQuestions to reflect on:\n1. Roles, Distributions, and Contributions of Team Members\nDescribe the specific roles and responsibilities of each team member in the project. \nWhat unique skills, expertise, or perspectives did they bring to the team? How did these contributions benefit the project\u2019s outcome?\nWere there any imbalances in the distribution of tasks? If so, how did this affect the team\u2019s dynamics and overall performance?\n2. Communication within the team\nWhat communication strategies did your team employ to stay aligned? For example, did you have daily meetings, and communicate frequently on Slack, or Github?\nHow effective were these strategies in ensuring everyone was on the same page? Were there any challenges or miscommunications, and if so, how were they addressed?\nReflect on the frequency and quality of your team\u2019s communication. Were there any improvements or adjustments made throughout the project?\n3. Balancing strengths and weaknesses of teammates\nHow did your team identify and address the strengths and weaknesses of individual members? For example, did you have a discussion about preferences and challenges during your initial meeting?\nWere tasks distributed considering varying skills within your team that played to every individual strengths? Were there opportunities for skill development in areas of weakness, and if so, how were these facilitated? \n4. Challenges and conflicts and how they fixed them\nWhat challenges arose within your team? For example, were there any disagreements, scheduling issues, or unexpected obstacles?\nDescribe the strategies employed to resolve these challenges effectively. How did your team adapt and find solutions?\nIf there were any unresolved challenges, reflect on why they persisted and propose potential solutions for future projects.\n5. Reflect on their team\u2019s work\nAre you satisfied with the overall outcome of your previous group project? Explain why, providing specific examples of factors that contributed to your success or shortcomings. \nWhat insights have you gotten from this reflective process?\nHow can these insights be applied to improve your team\u2019s dynamics, communication, and collaboration in future projects?\n\nInstructions for submission:\nEach team member should submit their reflective report individually, ensuring confidentiality in their responses.\n\nAs a team, come together to discuss the common themes, insights, and areas for improvement that emerged from your reflections.\n\nCollaboratively create an action plan, outlining specific steps and strategies to enhance your team's performance in your current team project. This plan should be concise, and practical, and reflect your collective commitment to continuous improvement.\n\n\n\nDeliverables\nReflective report\n\nEach team member will submit a reflective report answering the questions above. The report should be structured according to the topics provided.\n\nAction Plan\n\nAs a team, you will collectively create an action plan based on your reflections. This plan will outline specific strategies and improvements for your next group project, ensuring a more efficient and effective collaboration. \n\n\n\nDue Date:7\nReflective slides report and action plan: June 1st 8 PM UTC", "start_char_idx": 0, "end_char_idx": 5111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9836a060-fb0f-41c3-b0cc-39c736b961dc": {"__data__": {"id_": "9836a060-fb0f-41c3-b0cc-39c736b961dc", "embedding": null, "metadata": {"source": "Asking Good Questions Challenge - W5 - cB.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "22bb4129-e78e-48d3-921a-fa73a3d1f7df", "node_type": "4", "metadata": {"source": "Asking Good Questions Challenge - W5 - cB.docx"}, "hash": "f1c3ec9c1d016a8d388efdaffec5901f6800636be87ea7874dd1820469383e70", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8485f53e-4366-4925-94db-1f1f0f1db64a", "node_type": "1", "metadata": {}, "hash": "4e0dd89dfda701efd678785a45f313a08ec50a1b9ea4888f3d6750b155820211", "class_name": "RelatedNodeInfo"}}, "text": "10 Academy cB : Week 5\n\nCareers Exercise \n\nAsking Good Questions \n\nDeadline: 25th May, 2024. 8PM UTC\n\nIntroduction\nUnderstanding how to ask the right questions is crucial in any professional setting. This skill enhances your ability to gather essential information, solve problems effectively, and engage meaningfully with colleagues and stakeholders. The 'right' question often is not about finding a direct answer but about exploring possibilities and understanding deeper undercurrents of business operations. \n\nIn this challenge, you will analyze the given various workplace-related scenarios and decide whether further questions are needed. If questioning is required, you must formulate specific, strategic questions that would be appropriate to ask in each situation.\n\nObjective:\nThe goal is to develop the trainees' ability to discern when additional information is necessary and to craft questions that lead to clarity, facilitate decision-making, or reveal deeper insights into the issues presented.\n\n\nInstructions:\nThe scenarios below are made of:\n\nScenarios where you need to ask further questions to understand the situations and actions that need to be taken. \nScenarios where you may be curious to know more even though there are no actions that need to be taken. \nScenarios where the information given are satisfying and you don\u2019t need to ask any further questions. \n\n\nTask\nFor each scenario provided below, answer the following:\n\nDetermine whether further questions need to be asked to better understand the situation and to assist in decision-making, or to satisfy your curiosity, or not.\n\nIf you decide that no further questions are needed, give us 3 reasons why. \n\nIf you decide that further action questions or curiosity questions are needed, write down 5 specific questions you would ask. Aim to formulate questions that are clear, purposeful, and relevant to the context of the scenario.\n\n\nNOTE: DO NOT MAKE ANY ASSUMPTIONS IN THE SCENARIOS. Consider the information provided as the only information you have to make your decision.\n\n\nScenarios\n\nScenario 1:\nDuring a team meeting, it is announced that your objective is to enhance the performance of an existing machine learning model. The meeting ended with the team discussion only focusing on general goals for improving model operations within the next quarter. \n\nScenario 2:\nThe HR department sends out a memo detailing a new company policy on remote work, which includes eligibility criteria, steps for application, tools provided by the company, expectations for remote workers, and resources for technical support. The memo is thorough and includes a FAQ section that addresses common queries and concerns. \n\nScenario 3:\nYour team receives an email about upcoming IT maintenance that will affect several systems you use regularly. The email lists the systems but does not mention the duration of the downtime and other necessary informations.\n\nScenario 4:\nYou are assigned to integrate a new natural language processing (NLP) feature into an existing application. The project brief describes the end goals for user interaction improvements and enhanced data analysis capabilities through the NLP feature.", "start_char_idx": 1, "end_char_idx": 3185, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8485f53e-4366-4925-94db-1f1f0f1db64a": {"__data__": {"id_": "8485f53e-4366-4925-94db-1f1f0f1db64a", "embedding": null, "metadata": {"source": "Asking Good Questions Challenge - W5 - cB.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "22bb4129-e78e-48d3-921a-fa73a3d1f7df", "node_type": "4", "metadata": {"source": "Asking Good Questions Challenge - W5 - cB.docx"}, "hash": "f1c3ec9c1d016a8d388efdaffec5901f6800636be87ea7874dd1820469383e70", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9836a060-fb0f-41c3-b0cc-39c736b961dc", "node_type": "1", "metadata": {"source": "Asking Good Questions Challenge - W5 - cB.docx"}, "hash": "97df0f8d31f2c6acc32e22203fce7e0b66de8b7718bbf541549081934cafdb94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8f7cca3-b9d9-4703-8274-93bdbd62048b", "node_type": "1", "metadata": {}, "hash": "6f194647a9e0ea3dcbb12b87b3005fe65f036716b190887c139aecc7a476aa5e", "class_name": "RelatedNodeInfo"}}, "text": "Scenario 5:\nYou are given access to a well-documented API for a machine learning model, including detailed usage examples, a list of endpoints, expected input/output formats, and error handling instructions. The documentation also includes a changelog with updates from previous versions. \n\nScenario 6:\nThe project lead shares a detailed plan for deploying a new data pipeline. The plan includes a timeline, resource allocation, tools to be used, and a step-by-step guide for the deployment process. Additionally, there's a support contact list and a troubleshooting section addressing common issues from past deployments.\n\nScenario 7:\nOn your first day, you're given a rapid tour of the office and briefly introduced to different digital tools and platforms that the company uses\u2014each with distinct roles in operations, communication, and project management. Your orientation session includes a quick overview of each tool by various team members, but it lacks in-depth training or guidance on how to effectively use these tools in your specific role. You're expected to start working on projects from the next day, which involves interacting with many of these platforms.\n\nScenario 8:\nYou are tasked with organizing a small departmental conference. The administration has provided you with a conference plan, session plan, timelines, clear budget, a list of preferred vendors for catering and equipment, past event feedback to guide your planning, and a checklist of logistical steps to follow, approved by the previous event coordinators.\n\nScenario 9:\nYou're deeply involved in several critical tasks: rolling out a new software system, updating major data tools, organizing a company-wide training session on data security, finalizing quarterly performance reports, and overseeing the integration of a newly acquired company's technology into existing systems. With each project demanding thorough attention and varying skills, from technical knowledge to project management, the combined pressure is immense. Recognizing the need to maintain high standards of work, you're considering the best approach to seek additional help.\n\nScenario 10:\nThe company has just announced that it will be buying another company to become bigger and offer more products. When one company buys another, they usually have to combine their teams and figure out the best way to work together. This process can affect many parts of the company, from the jobs people do to the new opportunities that might come up. During a brief meeting, the leaders explained that this acquisition means our company will have more employees and a broader range of products. They talked about how this would help us compete better in the market. \n\n\nSubmission\nMake a PPT / Google slides with 12 slides maximum and convert it into PDF, then submit on Tenx. \n\nUsefulness in real life\nThis exercise allows you to develop the art of being curious and the skill of asking questions. These two concepts are vital skills necessary for navigating the world of work and in your personal life.", "start_char_idx": 3191, "end_char_idx": 6240, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8f7cca3-b9d9-4703-8274-93bdbd62048b": {"__data__": {"id_": "e8f7cca3-b9d9-4703-8274-93bdbd62048b", "embedding": null, "metadata": {"source": "Asking Good Questions Challenge - W5 - cB.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "22bb4129-e78e-48d3-921a-fa73a3d1f7df", "node_type": "4", "metadata": {"source": "Asking Good Questions Challenge - W5 - cB.docx"}, "hash": "f1c3ec9c1d016a8d388efdaffec5901f6800636be87ea7874dd1820469383e70", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8485f53e-4366-4925-94db-1f1f0f1db64a", "node_type": "1", "metadata": {"source": "Asking Good Questions Challenge - W5 - cB.docx"}, "hash": "8b45cb03baaaa99fd9f8a2815bf0208a139ad66a25f4838f019681d3997ba800", "class_name": "RelatedNodeInfo"}}, "text": "Submission\nMake a PPT / Google slides with 12 slides maximum and convert it into PDF, then submit on Tenx. \n\nUsefulness in real life\nThis exercise allows you to develop the art of being curious and the skill of asking questions. These two concepts are vital skills necessary for navigating the world of work and in your personal life. \n\n\nRubrics:\nScenario 1 (10 points): You need to decide whether further questions are necessary or not necessary to understand the situation and improve the model's performance. Grading will focus on the clarity of your decision and reasoning, and the formulation of good questions if required. \n\nScenario 2  (10 points):Assess whether additional questions are needed or not needed to understand the new remote work policy fully. Grading will consider the completeness of your analysis, the justification for not needing further questions, or the relevance and clarity of the questions if you decide more information is needed.\n\nScenario 3  (10 points): Evaluate whether additional questions are required or not required to gather necessary information about the upcoming IT maintenance. Grading will focus on the clarity of your decision-making process and the formulation of relevant questions if deemed necessary.\n\nScenario 4  (10 points): Decide if further questions are needed or not needed to understand the scope and objectives of integrating the NLP feature. Grading will consider the justification provided for your decision and the clarity and relevance of any additional questions proposed.\n\nScenario 5  (10 points): Assess whether additional questions are necessary or not necessary to fully comprehend the provided API documentation. Grading will focus on the completeness of your analysis and the formulation of purposeful questions if required.\n\nScenario 6  (10 points): Analyze if further questions are needed or not needed to understand the deployment plan for the new data pipeline. Grading will consider the clarity of your reasoning and the effectiveness of any proposed questions to gain additional insights.\n\nScenario 7  (10 points): Determine whether additional questions are required to address the lack of in-depth training on digital tools during orientation. Grading will focus on the thoroughness of your analysis and the formulation of clear and relevant questions to enhance understanding.\n\nScenario 8  (10 points): Evaluate if further questions are necessary or not necessary to organize the departmental conference effectively. Grading will consider the justification provided for your decision and the clarity and relevance of any additional questions proposed. \n\nScenario 9  (10 points): Analyze whether additional help is needed or not needed to manage multiple critical tasks effectively. Grading will focus on the clarity of your reasoning and the formulation of purposeful questions or strategies for seeking assistance if deemed necessary.\n\nScenario  10  (10 points): Determine if further questions are needed or not needed to understand the implications of the company acquisition announcement. Grading will consider the completeness of your analysis and the formulation of clear and relevant questions to gain insights into the situation.", "start_char_idx": 5906, "end_char_idx": 9119, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e46ee6f-6e1e-4890-afde-dbe518f9fdfc": {"__data__": {"id_": "9e46ee6f-6e1e-4890-afde-dbe518f9fdfc", "embedding": null, "metadata": {"source": "How to use Tenx.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f1cd40ef-11ea-4e7a-a88a-05c774a32226", "node_type": "4", "metadata": {"source": "How to use Tenx.docx"}, "hash": "a1209b4543d5136e6874f63adaaf96c6d77eaea1b3975a1a482583441b7fc4b2", "class_name": "RelatedNodeInfo"}}, "text": "How to use Tenx\n\nWelcome to tenx, the platform designed specifically for trainees enrolled in 10 Academy's online education. This guide aims to assist you in navigating through the essential steps to effectively utilize the features and functionalities of tenx.\nLogging In\nBegin your journey by visiting the login page here. Enter the email you used to register for 10 Academy as both the username and password into the provided fields.\n\nAfter, make sure to hit \u2018forgot password\u2019 to reset your login credentials, reset link will be sent to your email.\n\nIf you do not have login info, contact cohort manager Makida Shimeles.\n\nResetting Your Password\n\nUpon login, expect a notification stating that an email has been sent to facilitate a password change. Head to your email inbox and open the email you received from 10 Academy.\n\nInside the email, you will find a link designed for resetting your password. Click on this link to proceed.\n\nYou'll be directed to a new page where you can input your desired new password. Enter it and confirm by typing it again.\n\n\n\nAccessing Your Dashboard\n\nRe-login using your newly updated password here. Once logged in, your personalized dashboard awaits you.\n\nExplore the left sidebar of your dashboard to find various links leading to different functionalities. Discover sections such as challenges, performance leaderboard, assignments, and more.\n\n\nSubmitting Assignments\n\nTo submit your assignments, navigate to the 'Assignments' section through the sidebar. Choose the specific assignment you wish to complete.\n\n\nFollow the prompts and instructions provided within the assignment section to submit your work as requested.", "start_char_idx": 0, "end_char_idx": 1658, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e2f842f-f73d-42d0-a9c6-26735788256e": {"__data__": {"id_": "3e2f842f-f73d-42d0-a9c6-26735788256e", "embedding": null, "metadata": {"source": "Payment Processing Manual_.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d97e69df-0a2f-4b0a-a3d4-3634e73dbab4", "node_type": "4", "metadata": {"source": "Payment Processing Manual_.docx"}, "hash": "aef7a8bd73dd6448eca921c357ece3f8fd92d0bb1881355b5421c015e8d5e2ef", "class_name": "RelatedNodeInfo"}}, "text": "Payment Processing Manual for 10 Academy Tuition Fees\n\nThis manual provides detailed instructions for making tuition fee payments using various methods, including PayPal, US Bank Transfer, Nigerian Bank Transfer, Ethiopian Bank Transfer, International Bank Transfer, and Credit Card Transfer. Please follow the steps carefully for successful processing.\n        PayPal Payment Process\nGo to 10 Academy's PayPal profile here.\nLog in to your PayPal account.\nEnter the amount of the tuition fee and select the currency.\nReview the details and click \"Send.\"\n        US Bank Transfer Process\nBeneficiary Bank Details\n\nBeneficiary Name: 10 Academy Corp\nAccount Number: 9801172556\nType of Account: Checking\nBeneficiary Address: 1450 El Camino Real, 208, Santa Clara, CA 95050\n\nReceiving Bank Details\n\nABA Routing Number: 084106768\nBank Name: Evolve Bank & Trust\nBank Address: 6000 Poplar Ave, Suite 300, Memphis, TN 38119\n        International Bank Transfer Process\nInternational Wire Details\n\nReceiving Bank Details\n57D Account with institution \n\nSWIFT/BIC Code: FRNAUS44\nBank Name: First National Bankers Bank\nBank Address: 7813 Office Park Blvd, Baton Rouge, LA, 70809, USA\n\nBeneficiary Bank Details\n59 Beneficiary customer name & address\n\nIBAN/Account Number:9801172556\nBeneficiary Name: 10 Academy Corp\nBeneficiary Address: 1450 El Camino Real, 208 Santa Clara, CA 95050 USA\n\nReference Field\n70 Remittance information\n            084106768 with Evolve Bank and\n            Trust, 6000 Poplar Ave Ste. 300\n             Memphis, TN 38119\n         Credit Card Transfer Process\nEmail us at train@10academy.org and we will send you the details on how you can process the payment.\nEthiopian Bank Transfer Process (ETB Payments)\n            Beneficiary Name - 10 Academy\n             Bank Name - Commercial Bank of Ethiopia\n             Beneficiary Account Number - 1000589619076\nNigerian Bank Transfer Process (NGN Payments)\nBank Name - Guaranty Trust Bank\nBeneficiary Name - Adeyemo Oluwatosin Maryam\nBeneficiary Account Number - 0152858905\n        Important Note\nPlease keep a record of your transaction for your reference.\n\nTo ensure smooth processing and confirmation of your payment, we kindly request that you send a payment confirmation screenshot/link (for PayPal or Credit Card transfers) or a bank payment slip with a stamp (for bank transfers) if available. This will help us verify your payment promptly.\n\nIf you have any further requests or questions, feel free to let us know!", "start_char_idx": 2, "end_char_idx": 2484, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c718f1af-8ad9-4372-9b32-7b285523ef03": {"__data__": {"id_": "c718f1af-8ad9-4372-9b32-7b285523ef03", "embedding": null, "metadata": {"source": "10 Academy Intensive Training_ Week-0 Challenge Document.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "05f3a61d-ded7-4eb3-97e0-9b624ba9e052", "node_type": "4", "metadata": {"source": "10 Academy Intensive Training_ Week-0 Challenge Document.docx"}, "hash": "5d2c1b73b648d2e3d32c9253536db534423d4df28f7c0d2a19df6b99ff869b1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "944464a4-f05f-4d22-90bf-e51df3ec7572", "node_type": "1", "metadata": {}, "hash": "fffea217e98a8151102f01b5b5beffe15b843a4a9bb38947c19bd87cc1d992c5", "class_name": "RelatedNodeInfo"}}, "text": "10 Academy Intensive Training Week-0 Challenge Document\nPre-training Assessment \nDate: 08 Apr - 12 Apr 2023\nChallenge Overview\nThis week\u2019s challenge focuses on the sentiment, topic, and reporting correlation among various global media agencies. The challenge aims to evaluate candidates for the 12-week training program in Data Engineering (DE), Generative AI Engineer (AIE), and Machine Learning Engineering (MLE). It provides applicants with a real-world experience of tasks in these domains and helps 10 Academy select suitable candidates\nApplicants who proceed to the next level by attaining an excellent performance in this week's challenge will have a clear picture of the required discipline, resilience, proactivity, talent diversity, and other essential elements of the 10 Academy training. Those that can not make it to the limited spots available will gain a clear understanding of the direction they should improve to prepare for AIE, DE & MLE job positions in the future.  Everyone will gain project experience to showcase in their professional profile.\n\nThis week, therefore, is a win-win for everyone. We advise you to put your best effort to complete as many tasks as possible. We know that the number of tasks you are required to complete are a lot, and you will not have time to build intuition or to be comfortable with the new concepts and skills you are exposed to with this week\u2019s challenge. Please note that building a deeper understanding is not the purpose for this week's project. Moreover, you may have never done or attempted to do some of the tasks before this training. If you are confused and overwhelmed, know that it is expected. \n\nThe tutors, community managers, and all other teams are there to support you as best as they can. Be proactive to ask questions, provide resources that may help others, and above all persist!\nDataset Overview: Importance of News Headline Analysis\nThe data for this week's challenge is Global News Dataset. This dataset comprises news articles collected over the past few months using the NewsAPI\nYou can download the global news data here. The structure of the data is as follows\n\ndata.csv: \narticle_id: Unique article id  \nsource_id\nsource_name: Source name\nauthor: The author of the article\ntitle: The headline or title of the article.\ndescription: A description or snippet from the article. \nurl: The direct URL to the article. \nurl_to_image: The URL to a relevant image for the article.\npublished_at: The date and time that the article was published, in UTC\ncontent: The unformatted content of the article, where available. This is truncated to 200 chars\ncategory: Search query used to fetch data\narticle: Full content of that article\ntitle_sentiment: Sentiment of the title\n\nYou can download the domain\u2019s location (the country where the news media headquarter that owns the domain/website is located) data here. The structure is as follows\n\ndomains_location.csv:\nSourceCommonName: Common  Domain Name (bbc.co.uk, cnn.com)\nlocation: Country short code (US, UK, CA)\nCountry: Country name\n\nYou can download the 1million global website traffic data here. The structure is as follows\n\ntraffic_data.csv:\nGlobalRank: Rank of the domain globally\nTldRank: Rank of the TLD (Top-Level Domain [.com, .org\u2026..etc]) among other similar TLD (Top-Level Domain [.com, .org\u2026..etc])\nDomain: \nTLD: TLD (Top-Level Domain [.com, .org\u2026..etc])\nRefSubNets: The number of Referring Subnets found for this domain in the Fresh Index.\nRefIPs: The number of Referring IPs found for this domain in the Fresh Index.\nIDN_Domain: Internationalized Domain Name\nIDN_TLD: Internationalized Domain Name Top-Level Domain.\nPrevGlobalRank: Previous Global Rank.\nPrevTldRank: Previous TLD Rank.\nPrevRefSubNets: Previous number of referring subnets.\nPrevRefIPs: Previous number of referring IPs.\nData Handling Instructions\nDo not push the data to GitHub. Add the data path to your .gitignore file to prevent accidental uploads.\nEnsure compliance with data privacy and confidentiality standards when working with the dataset \nThe data for the project is in the public domain, so feel free to share your exciting findings in your preferred social media.\nGuidance for Applicants\nWin-Win Situation: This week's challenge is designed for mutual benefit. Excellent performance can lead to selection for the training program, while others gain valuable insights for future improvement.\nTask Completion: Focus on completing as many tasks as possible. The goal is exposure to various aspects, not necessarily deep understanding.\nSupport System: The 10 Academy team, including tutors and community managers, is here to support you.", "start_char_idx": 3, "end_char_idx": 4650, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "944464a4-f05f-4d22-90bf-e51df3ec7572": {"__data__": {"id_": "944464a4-f05f-4d22-90bf-e51df3ec7572", "embedding": null, "metadata": {"source": "10 Academy Intensive Training_ Week-0 Challenge Document.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "05f3a61d-ded7-4eb3-97e0-9b624ba9e052", "node_type": "4", "metadata": {"source": "10 Academy Intensive Training_ Week-0 Challenge Document.docx"}, "hash": "5d2c1b73b648d2e3d32c9253536db534423d4df28f7c0d2a19df6b99ff869b1e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c718f1af-8ad9-4372-9b32-7b285523ef03", "node_type": "1", "metadata": {"source": "10 Academy Intensive Training_ Week-0 Challenge Document.docx"}, "hash": "334e804aef8b2c1a3f034691b29ab5f6230bc177d56290e5bc8205a2f8f4f3bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "200ae9be-35b3-4dce-b978-78a9ab45051c", "node_type": "1", "metadata": {}, "hash": "92bdf4a09e1a65f53ac6532345bceed13ddd6d18309c7919c09774642a49ec4e", "class_name": "RelatedNodeInfo"}}, "text": "PrevGlobalRank: Previous Global Rank.\nPrevTldRank: Previous TLD Rank.\nPrevRefSubNets: Previous number of referring subnets.\nPrevRefIPs: Previous number of referring IPs.\nData Handling Instructions\nDo not push the data to GitHub. Add the data path to your .gitignore file to prevent accidental uploads.\nEnsure compliance with data privacy and confidentiality standards when working with the dataset \nThe data for the project is in the public domain, so feel free to share your exciting findings in your preferred social media.\nGuidance for Applicants\nWin-Win Situation: This week's challenge is designed for mutual benefit. Excellent performance can lead to selection for the training program, while others gain valuable insights for future improvement.\nTask Completion: Focus on completing as many tasks as possible. The goal is exposure to various aspects, not necessarily deep understanding.\nSupport System: The 10 Academy team, including tutors and community managers, is here to support you. Don't hesitate to ask questions, share resources, and persist in your efforts.\nWeek's Topics Covered\nPython and Javascript Programming:\nTask-specific programming assignments.\nGitHub Commands:\nContinuous committing and repository management.\nFrameworks, Processes, and Workflows:\nUtilising CRISP-DM methodology.\nDatabases:\nWorking with SQL and NoSQL databases.\nData Understanding and Exploration:\nApplying exploratory data analysis techniques.\nCI/CD:\nUnderstanding continuous integration and continuous deployment.\nMLOps:\nML system process design.\nModelling:\nTopic modelling and sentiment analysis.\nWeb App Development:\nBuilding dashboards.\nFull Stack Development:\nExposure to full-stack concepts.\nServer and Serverless Deployment:\nDeployment architecture and strategies.\nNote to Applicants\nOverwhelm is Expected: The challenge is designed to cover a broad range of topics. Feeling confused or overwhelmed is expected, and support is available.\nProactivity is Key: Be proactive in asking questions, sharing resources, and collaborating with others.\nPersistence is Valued: Persistence in tackling challenges is highly valued in this learning environment.\nStarter python package\nWe provide a starter python package that you may use to start working on the challenge. It has (at the time of writing this document) the following structure\n\nNetwork_analysis\n\u251c\u2500\u2500 .vscode\n\u2502   \u2514\u2500\u2500 settings.json\n\u251c\u2500\u2500 .github\n\u2502   \u2514\u2500\u2500 workflows\n\u2502       \u251c\u2500\u2500 flake8_check.yml\n\u2502       \u251c\u2500\u2500 unittests.yml\n\u2502       \u2514\u2500\u2500 docstring_tests.yml\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 .flake8\n\u251c\u2500\u2500 .pre-commit-config.yaml\n\u251c\u2500\u2500 setup.cfg\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 style_guide.md\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 view_tree.py\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 notebooks\n\u2502   \u251c\u2500\u2500 parse_slack_data.ipynb\n\u2502   \u2514\u2500\u2500 README.md\n\u251c\u2500\u2500 tests\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 config.py\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 utils.py\n    \u2514\u2500\u2500 loader.py\n\n\nHere's a breakdown of the different components (WARNING: we may add, edit or remove some files without updating this document. Also note that, code or config files may not be working - you should fix it by yourself or remove it if you don\u2019t need it):\n\n.flake8: Configuration file for the Flake8 tool, which checks for code style and quality.\nrequirements.txt: Lists the dependencies required to run the project. This file is commonly used with tools like pip to install project dependencies.\n.pre-commit-config.yaml: Configuration file for pre-commit hooks, which are checks that run before commits are allowed.\nMakefile: A Makefile for defining and running project tasks. It's a convenient way to encapsulate complex or repetitive tasks.\npyproject.toml: Configuration file for build tools and project metadata. It's commonly used for projects using poetry.\ntests/: A directory for storing unit tests. The __init__.py file indicates that this directory should be treated as a Python package.\ndocs/: A directory to store project documentation.\nstyle_guide.md: A markdown file containing the style guide for the project. Good documentation for coding styles can be useful for maintaining consistency.\nREADME.md: A README file containing essential information about the project. It serves as the first point of contact for anyone exploring the project.\n.gitignore: A file specifying which files and directories to ignore when pushing to version control (Git). This often includes files that are generated during runtime, build artifacts, and sensitive information.\n.github/workflows/: GitHub Actions workflow files for automating tasks like code formatting, unit testing, and docstring testing.\nsetup.cfg: Configuration file for the setuptools package, often used for packaging and distribution.", "start_char_idx": 3655, "end_char_idx": 8303, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "200ae9be-35b3-4dce-b978-78a9ab45051c": {"__data__": {"id_": "200ae9be-35b3-4dce-b978-78a9ab45051c", "embedding": null, "metadata": {"source": "10 Academy Intensive Training_ Week-0 Challenge Document.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "05f3a61d-ded7-4eb3-97e0-9b624ba9e052", "node_type": "4", "metadata": {"source": "10 Academy Intensive Training_ Week-0 Challenge Document.docx"}, "hash": "5d2c1b73b648d2e3d32c9253536db534423d4df28f7c0d2a19df6b99ff869b1e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "944464a4-f05f-4d22-90bf-e51df3ec7572", "node_type": "1", "metadata": {"source": "10 Academy Intensive Training_ Week-0 Challenge Document.docx"}, "hash": "a77b3eac9ce1d7ff9b919a1ebee7d6a9c2b289f71318a329fb5a5cdeff5706d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de101251-6a1b-45eb-862f-363b603f355a", "node_type": "1", "metadata": {}, "hash": "d8340b332509c9aad3975ff8b0f31442b53e4ad392f7354f94702c4b2b3efa03", "class_name": "RelatedNodeInfo"}}, "text": "pyproject.toml: Configuration file for build tools and project metadata. It's commonly used for projects using poetry.\ntests/: A directory for storing unit tests. The __init__.py file indicates that this directory should be treated as a Python package.\ndocs/: A directory to store project documentation.\nstyle_guide.md: A markdown file containing the style guide for the project. Good documentation for coding styles can be useful for maintaining consistency.\nREADME.md: A README file containing essential information about the project. It serves as the first point of contact for anyone exploring the project.\n.gitignore: A file specifying which files and directories to ignore when pushing to version control (Git). This often includes files that are generated during runtime, build artifacts, and sensitive information.\n.github/workflows/: GitHub Actions workflow files for automating tasks like code formatting, unit testing, and docstring testing.\nsetup.cfg: Configuration file for the setuptools package, often used for packaging and distribution.\n.vscode/settings.json: Configuration settings for Visual Studio Code, an integrated development environment (IDE).\nnotebooks/: A directory for storing Jupyter notebooks, which are often used for exploratory data analysis (EDA) and documenting code.\nview_tree.py: A Python script or module for viewing the project tree structure.\nsrc/: A directory containing the source code of the project.\nconfig.py: Configuration file for the project.\n__init__.py: Marks the src directory as a Python package.\nutils.py: A module containing utility functions.\nloader.py: A module for loading Slack data \nFeel free to adopt or alter the structure as you see fit. \n\nWork Plan\nThe following is the summary of the 5 tasks you are expected to complete this week. A step-by-step guideline will be provided during the week.\n\nIn all of the tasks below the following are required performance indicators\nAbility to help others.\nEfficient and modular coding technique.\nProactivity to self-learn - sharing references.\n\n Task 1: \nGit and GitHub\nTasks: \nSetting up Python environment\nGit version control \nCI/CD \nKey Performance Indicators (KPIs):\nDev Environment Setup.\nRelevant skill in the area demonstrated.\nProject Planning - EDA & Stats\nTasks: \nUnderstanding CRISP-DM Framework\nData Understanding\nExploratory Data Analysis (EDA)\nStatistical thinking\nKPIs:\nProactivity to self-learn - sharing references.\nEDA techniques to understand data and discover insights,\nDemonstrating Stats understanding by using suitable statistical distributions and plots to provide evidence to actionable insights gained from EDA.\nMinimum Essential To Do (if you do it better, don\u2019t worry about sticking with this guideline) :\nCreate a github repository that you will be using to host all the code for this week. You can name it like news_correlation_10ac_week0/\nCreate at least one new branch called \u201dtask-1\u201d for your analysis of day 1\nCommit your work at least three times a day with descriptive commit message\nIf you are using the starter code (which is prepared for a slack data analysis project), make sure to change every occurrence of slack (filename, function names, class names, etc.) to news. Moreover, make sure to update the code to be useful for the news data. For example \nChange SlackDataLoader to NewsDataLoader so that you can load the current data\nMove all non-plotting functions from notebooks/parse_slack_data.ipynb into src/loader.py and src/utils.py such that in the notebook you use the NewsDataLoader from src/loader.py and functions from src/utils.py for all your data loading needs. \nWrite new methods and new classes to help you explore the data better\nPerform EDA analysis to answer the following questions\nWho are the top and bottom 10 \nWebsites that have the largest count of news articles\nWebsites with the highest numbers of visitors traffic \nCountries with the highest number of news media organisations (represented by domains in the data)\nCountries that have many articles written about them - the content of the news is about that country\nWebsites that reported (the news content) about Africa, US, China, EU, Russia, Ukraine, Middle East? Note that you will need to group countries together to form the African, EU, and Middle East continents/regions.\nWebsites with the highest count of positive, neutral, and negative sentiment? To do this you will need to group the data by website domain and apply descriptive statistics such as mean, median, and variance\ncompare the impact of using mean/average and median, \ncheck the distribution of sentiments for a particular domain (select the top 10 domains by visitors traffic) amount of news reported or vs the global news sentiment distribution)\nCompare the content metadata across sites \nHow similar are the raw message lengths across sites? Check the distribution among sites\nHow similar are the number of words in the title across sites?", "start_char_idx": 7250, "end_char_idx": 12180, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de101251-6a1b-45eb-862f-363b603f355a": {"__data__": {"id_": "de101251-6a1b-45eb-862f-363b603f355a", "embedding": null, "metadata": {"source": "10 Academy Intensive Training_ Week-0 Challenge Document.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "05f3a61d-ded7-4eb3-97e0-9b624ba9e052", "node_type": "4", "metadata": {"source": "10 Academy Intensive Training_ Week-0 Challenge Document.docx"}, "hash": "5d2c1b73b648d2e3d32c9253536db534423d4df28f7c0d2a19df6b99ff869b1e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "200ae9be-35b3-4dce-b978-78a9ab45051c", "node_type": "1", "metadata": {"source": "10 Academy Intensive Training_ Week-0 Challenge Document.docx"}, "hash": "2bc67f1e7ca4c1bae3e64c5841abe512d6fe381e9ca7e587d298700c4b41de84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d1a4df3-e851-4dae-9dd2-07a0e1a8e5fa", "node_type": "1", "metadata": {}, "hash": "59d0060ad054b832fa475f6cfe3b7cf4f4ec5640ca01900487431ead3537e09c", "class_name": "RelatedNodeInfo"}}, "text": "Note that you will need to group countries together to form the African, EU, and Middle East continents/regions.\nWebsites with the highest count of positive, neutral, and negative sentiment? To do this you will need to group the data by website domain and apply descriptive statistics such as mean, median, and variance\ncompare the impact of using mean/average and median, \ncheck the distribution of sentiments for a particular domain (select the top 10 domains by visitors traffic) amount of news reported or vs the global news sentiment distribution)\nCompare the content metadata across sites \nHow similar are the raw message lengths across sites? Check the distribution among sites\nHow similar are the number of words in the title across sites? Check the distribution among sites\nWhat is the impact of frequent news reporting and sentiment to the website\u2019s global ranking? \nDo a 2D scatter plot where x-axis is the total number of reports by a website, y-axis is the global ranking of the site, and the color representing average/median sentiment. \n\nTask 2: \nData Science Component Building\nTasks: \nMLOps components\nAnalysis pipeline designs\nTime Series Analysis\nClassification of Headlines into the following tags\nBreaking News\nPolitics\nWorld News\nBusiness/Finance\nTechnology\nScience\nHealth\nEntertainment\nSports\nEnvironment\nCrime\nEducation\nWeather\nOther\nTopic Modelling & Sentiment Analysis.\nPredictive analysis and modelling  \nNetwork analysis\nKPIs:\nUnderstanding DS components.\nPipeline and process-centric thinking.\nAbility to understand basic requirements and translate them into codes.\nDemonstrating ML skills and knowledge\nML Engineering\nTasks: \nUnderstanding MLOps components\nFeature Store\nModel versioning\nModel monitoring, \nUnit Testing \nCI Implementation with Github Actions\nDockerization \nBuilding python packages.\nKPIs:\nUnderstanding of Software Engineering concepts.\nUnderstanding of MLOps concepts\nDemonstrating Code, Data, and Model CI/CD skill\nMinimum Essential To Do:\nMerge your day 1 branches (e.g. task-1 branch) to the main branch\nFrom the main branch, create at least one new branch called \u201ctask-2\u201d to develop your analysis of Task 2\nCreate at least one unit test into tests/ folder\nSummarise the different MLOps components and their use\nAnswer the following questions \nPerform Keyword extraction/modelling using TF-IDF, KeyBert, YAKE,  or other similar algorithms\nHow similar are keywords in the headline/title compared to keywords in the news body across sites? To do this you may need to perform TF-IDF or similar keyword identification \nPerform topic modelling. Checkout ref1 ref2 ref3\ncategorise the title/content into a known set of topic categories\nAnalyse topics and trends \nWhich websites reported the most diverse topics?\nAnalyse the topic trends. For example, plot a 2D scatter plot such that x-axis is date, y-axis is the topics, and the color represents the count of the topic in that particular date. What  are the observed trends?\nModel the events that the news articles are written about (this is the most challenging part of this project). You may follow similar methodology as this one or this one   \nyou will require to associate/model the event that the article is covering. For example 500 of the news articles by 60 news media  could be about the event of a global disruption of the Meta company Apps such as Instagram). \nCluster news articles by events \nHow many events are covered in the data?\nAnalyse which news sites report events the earliest?\nWhich events have the highest reporting?\nWhat is the correlation between news sites reporting events?\nVersion your ML models and their artefacts using MLFlow\nTask 3: \nPostgreSQL\nTasks: \nDatabase Technologies (SQL vs NOSQL), \nDatabase schema design.\nUsing PostgreSQL as Feature Store and load into it ML model features \nLoading data into PostgresSQL \nKPIs:\nSchema design for SQL\nDemonstrating SQL skills\nMinimum Essential To Do:\nMerge the necessary branches from task-2 into the main branch using a Pull Request (PR)\nCreate at least 1-branch called task-3 to develop your work for Task 3\nDesign a PostgreSQL table schema to store your ML features - the data used to create the Topic & Event Models\nCreate a schema using dbdiagram.io or DbSchema to host the data features you prepared in Task 1 & 2\nCreate your tables using python \nLoad all the relevant features into the database\nTask 4: (Optional)\nDashboards\nTasks: \nStreamlit-based dashboard design\nProgramming a simple React app or component (streamlit)\nKPIs\nWireframing tools familiarity.\nDashboard Design.\nDesign thinking.\nBuilding dashboards with Streamlit.", "start_char_idx": 11433, "end_char_idx": 16041, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d1a4df3-e851-4dae-9dd2-07a0e1a8e5fa": {"__data__": {"id_": "7d1a4df3-e851-4dae-9dd2-07a0e1a8e5fa", "embedding": null, "metadata": {"source": "10 Academy Intensive Training_ Week-0 Challenge Document.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "05f3a61d-ded7-4eb3-97e0-9b624ba9e052", "node_type": "4", "metadata": {"source": "10 Academy Intensive Training_ Week-0 Challenge Document.docx"}, "hash": "5d2c1b73b648d2e3d32c9253536db534423d4df28f7c0d2a19df6b99ff869b1e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de101251-6a1b-45eb-862f-363b603f355a", "node_type": "1", "metadata": {"source": "10 Academy Intensive Training_ Week-0 Challenge Document.docx"}, "hash": "d6df9815566b5df8a42e678737c9ca374a02d58a03d82c5643833140fbf3d95d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99e38979-48ca-4ada-a916-fc6d0a9b1599", "node_type": "1", "metadata": {}, "hash": "d7834298fe970c106ccd29ef702e2c955e8f81948088b9c62f79d7672ed6d3a2", "class_name": "RelatedNodeInfo"}}, "text": "Using PostgreSQL as Feature Store and load into it ML model features \nLoading data into PostgresSQL \nKPIs:\nSchema design for SQL\nDemonstrating SQL skills\nMinimum Essential To Do:\nMerge the necessary branches from task-2 into the main branch using a Pull Request (PR)\nCreate at least 1-branch called task-3 to develop your work for Task 3\nDesign a PostgreSQL table schema to store your ML features - the data used to create the Topic & Event Models\nCreate a schema using dbdiagram.io or DbSchema to host the data features you prepared in Task 1 & 2\nCreate your tables using python \nLoad all the relevant features into the database\nTask 4: (Optional)\nDashboards\nTasks: \nStreamlit-based dashboard design\nProgramming a simple React app or component (streamlit)\nKPIs\nWireframing tools familiarity.\nDashboard Design.\nDesign thinking.\nBuilding dashboards with Streamlit.\nFullstack concepts understanding using React and Python\nMinimum Essential To Do:\nMerge the necessary branches from task-3 into the main branch using a Pull Request (PR)\nCreate at least 1-branch called task-4 to develop your work for Task 4\nDesign a dashboard to show results from Task 1 & 2 in Figma or similar tools \nImplement the dashboard you designed using Streamlit \nWrite a basic React App or Streamlit component to demonstrate your understanding of Fullstack (frontend using React, database using SQL, and backend using Python) development \nStreamlit components are built using React - see this reference\nTask 5: (Optional)\nDeployment\nTasks: \nStreamlit or React App\nWriting at least one github action for continuous deployment (CD)\nPython Backend\nWriting at least one github action for continuous deployment (CD)\nConfigure environment variables for database credentials and other sensitive information.\nPostgreSQL Database\nInitialize the PostgreSQL database by running the necessary setup scripts. Typically involves creating the initial database cluster and superuser.\nUnderstanding deployment\nUnderstanding Kubernetes (Docker Swarm or other distributed docker container based deployments)\nKPIs\nUnderstanding of deployment technologies\nMinimum Essential To Do:\nMerge the necessary branches from task-4 into the main branch using a Pull Request (PR)\nCreate at least 1-branch called task-5 to develop your work for Task 5\nCreate at least one github action file in the .github/workflow that will package your code as docker image and push it to docker hub. \nIn the future you may add a step in your github actions to trigger serverless cloud deployment e.g. in AWS Lambda or other docker based deployment infrastructure\nWrite a summary of your understanding of the different deployment tech stack.\n\nDeliverables:\nMonday:\nYour public Github repository link \nYou should make sure that all of your code related work for this week is going to be aggregated in this repository. We will analyse your repository link to evaluate your skills and work progress.\nTuesday:\n1-page summary of the project as you understand it.\nGithub link to your main branch\nYou should create at least one new branch called `task-1` for task 1 and merge your latest work with main branch.\nWednesday:\n\nThursday:\n3-pages report on insights and work done in the previous days.\nFriday: Changed to Saturday 8:00 PM (UTC)\nLink to deployed dashboard (or detailed screenshots). (Optional but a plus if you submit)\nGitHub Link to your main branch \nFinal report - covering all week-0 work. \nWrite it in the format that you could post it as a Blog in Medium.\n\nOther Considerations:\nDocumentation: Encourage detailed documentation in code and report writing.\nCollaboration: Emphasise collaboration through Github issues and projects.\nCommunication: Regular check-ins, Q&A sessions, and a supportive community atmosphere.\nFlexibility: Acknowledge potential challenges and encourage proactive communication.\nProfessionalism: Emphasise work ethics and professional behaviour.\nTime Management: Stress the importance of punctuality and managing time effectively.\nTutorials Schedule\nIn the following, the colour purple indicates morning sessions, and non-purple indicates afternoon sessions.\n\nDay 1: \nIntroduction to the Challenge\nPython Environment Setup, Git and Github, CI/CD \nProject Planning & EDA - Data Science Workflow using CRISP-DM and EDA techniques \nDay 2: \nData Science Component Building (Architecture Designs, Wifeframing, logic flow)\nTopic Modeling, Sentiment Analysis, Time Series Analysis\nML Engineering components \nDay 3:\n\nDay 4: \nWorking with SQL \nDatabase schema design \nDay 5:\nBuilding Dashboards using Streamlit\nIntroduction to Fullstack programming using React and Python\n\nLeaderboard Updates\nWednesday end of day\nFriday end of day", "start_char_idx": 15178, "end_char_idx": 19855, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99e38979-48ca-4ada-a916-fc6d0a9b1599": {"__data__": {"id_": "99e38979-48ca-4ada-a916-fc6d0a9b1599", "embedding": null, "metadata": {"source": "10 Academy Intensive Training_ Week-0 Challenge Document.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "05f3a61d-ded7-4eb3-97e0-9b624ba9e052", "node_type": "4", "metadata": {"source": "10 Academy Intensive Training_ Week-0 Challenge Document.docx"}, "hash": "5d2c1b73b648d2e3d32c9253536db534423d4df28f7c0d2a19df6b99ff869b1e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d1a4df3-e851-4dae-9dd2-07a0e1a8e5fa", "node_type": "1", "metadata": {"source": "10 Academy Intensive Training_ Week-0 Challenge Document.docx"}, "hash": "5497d7f14f5183f6ce08f3faa3cef5a98ac69b507583c8ad6b43c509466d778e", "class_name": "RelatedNodeInfo"}}, "text": "Flexibility: Acknowledge potential challenges and encourage proactive communication.\nProfessionalism: Emphasise work ethics and professional behaviour.\nTime Management: Stress the importance of punctuality and managing time effectively.\nTutorials Schedule\nIn the following, the colour purple indicates morning sessions, and non-purple indicates afternoon sessions.\n\nDay 1: \nIntroduction to the Challenge\nPython Environment Setup, Git and Github, CI/CD \nProject Planning & EDA - Data Science Workflow using CRISP-DM and EDA techniques \nDay 2: \nData Science Component Building (Architecture Designs, Wifeframing, logic flow)\nTopic Modeling, Sentiment Analysis, Time Series Analysis\nML Engineering components \nDay 3:\n\nDay 4: \nWorking with SQL \nDatabase schema design \nDay 5:\nBuilding Dashboards using Streamlit\nIntroduction to Fullstack programming using React and Python\n\nLeaderboard Updates\nWednesday end of day\nFriday end of day\n\n\nReferences\nHighly recommended references are highlighted in bold.\n\nRelevant academic papers\nThe drivers of global news spreading patterns | Scientific Reports (nature.com)\n\nCRISP-DM, topic modelling, and sentiment analysis:\nhttps://www.datascience-pm.com/crisp-dm-2/\nGensim Topic Modeling - A Guide to Building Best LDA models (machinelearningplus.com)\nBeginners Guide to Topic Modeling in Python and Feature Selection (analyticsvidhya.com)\n\nDatabase & Dashboard design and implementation\nGet started - Streamlit Docs\nTurn Python Scripts into Beautiful ML Tools | by Adrien Treuille | Towards Data Science\nStreamlit 101: An in-depth introduction | by Shail Deliwala | Towards Data Science\nStreamLit - Data Scientists tool for developing web apps | by INSAID | Medium\n\nPython Programming:\nObject Oriented Programming \nPython Courses and Tutorials: Online and On Site (python-course.eu)\n\nData Engineering\nWhat is Data Engineer: Role Description, Skills, and Background | AltexSoft\n\nVersion control \u2013 Git\nWhat is version control | Atlassian\nLearn Git branching -- interactive way to learn Git\nGit with large files\nWhich files to not track and how to not track them? | Atlassian\n.gitignore docs\nConventional commits -- lightweight convention on top of commit messages.\n\nCI/CD\nWhat is Continuous Integration | Atlassian\nDevOps Pipeline | Atlassian\n7 Popular Open Source CI/CD Tools - DevOps.com\nSetting up a CI/CD pipeline on Github\n\nMLOps\nMLOps: Continuous delivery and automation pipelines in machine learning\nMLwatcher - monitoring model performanc\n\nPython Testing\nhttps://machinelearningmastery.com/a-gentle-introduction-to-unit-testing-in-python/\nhttps://docs.python-guide.org/writing/tests/\nhttps://realpython.com/python-testing/", "start_char_idx": 18927, "end_char_idx": 21588, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef1e2138-0a34-48b8-b2fd-76be3b050586": {"__data__": {"id_": "ef1e2138-0a34-48b8-b2fd-76be3b050586", "embedding": null, "metadata": {"source": "cB- Problem-Solving Skills- Wk3.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2caa6841-b9a9-434b-8739-8f1f25ac9fed", "node_type": "4", "metadata": {"source": "cB- Problem-Solving Skills- Wk3.docx"}, "hash": "a07dea7a4d12d3a256ba8c4c800d4fb56dbd6f3dbf57c6ee04ad030994d07f3a", "class_name": "RelatedNodeInfo"}}, "text": "10 Academy Cohort B: Week 3\n\nNon-Technical Challenge\n\nProblem-Solving Skills \n\nSubmission deadline: Saturday, May 11th, 2024, 8:00 PM UTC\n\nScenario: \nYou are a Product Manager of a tech startup, Canes Gaming Inc., and your job is filled with unexpected challenges that require your immediate attention and decision-making. Canes Gaming Inc. is a gaming company that focuses on the development and publishing of the game Sandy Crash.  The company is struggling with declining user engagement and customer retention in its mobile application. \n\nDespite an initial surge in downloads, the app is experiencing a drop-off in active users, leading to concerns about long-term sustainability and revenue generation. \n\nAs the product manager, you have tasked analysts with helping you analyze the situation to identify the root causes of the declining engagement. You will then  propose actionable solutions to improve user retention based on the analysis and make decisions for your company with a way forward. \n\nThe objective of this exercise is to develop and demonstrate your problem-solving skills by addressing real-world challenges faced when running a product. \n\nSummary of the findings:\nA root cause analysis was conducted by the analysts through a combination of qualitative and quantitative methods, including user surveys, interviews with key stakeholders, and analysis of app usage data. The goal was to identify the primary factors contributing to the decline in user engagement and retention and to develop actionable recommendations for addressing these issues.\n\nThey settled on the following reasons:\nMisleading advertising: The app\u2019s advertisement campaign across social media was very appealing and lured many users into downloading the app. However, the app fell short of user expectations as it lacked the features and updates that were prominently advertised. This resulted in user disappointment and eventual disengagement\n\nMarketing flaw: A lot of money was spent on advertising too soon, and it also wasn\u2019t targeted (mindless advertising).\n\nPoor employee collaboration: There is clearly a problem with team members not collaborating well. Otherwise, what was advertised would have been the same as the app itself. \n\nCustomer dissatisfaction: The rate of active users declined, even though the download rates are high. \n\nCommunication problem: The team doesn\u2019t communicate well with each other and they tend not to comprehend each other\u2019s vision in the team.\n\n\n\nTask: \n\nAs a product manager, your task is to develop 5 strategies to address the issue of misleading advertising to find out how it happened, manage it, and rebuild user trust.\n\nOutline 5 key steps to give guidance to the marketing team to ensure effective and targeted marketing practices.\n\nWhat 5 steps will you take to ensure the team collaboration improves?\n\nWhat are 5 some of the considerations you\u2019d take to ensure customer satisfaction and retention?\n\nHow will you ensure effective communication? \n(i) Within the whole team, i.e. marketing, developer, management, illustrators, etc? Provide 5 ideas.\n\n(ii)Think about how in the gaming industry, describing a game requires you to properly communicate the vision, the idea, and the feel. What 5 measures would you take to ensure everyone has the right vision in mind?\n\nSubmission\n\nA report consisting of slides that you\u2019ll present to your whole team on the way forward, answering the questions in the tasks above. \nSave your PPT as PDF before submission.\n\n\nRubrics\n\nUnderstanding and Addressing Misleading Advertising: Grading will assess the clarity and effectiveness of the proposed strategies to address misleading advertising, considering how it happened, how to manage it, and how to rebuild user trust.\n\nGuidance for Effective and Targeted Marketing Practices: You are tasked with outlining key steps to guide the marketing team towards effective and targeted marketing practices. Grading will focus on the specificity, relevance, and feasibility of the outlined steps.\n\nImproving Team Collaboration: You are asked to propose steps to enhance team collaboration. Grading criteria include the comprehensiveness, feasibility, and potential impact of the suggested strategies.\n\nConsiderations for Customer Satisfaction and Retention: Grading will evaluate the depth and relevance of considerations outlined to ensure customer satisfaction and retention, considering the identified root causes and potential solutions.\n\nEnsuring Effective Communication:\n\n(i) Within the Whole Team:\nYou are required to provide ideas to ensure effective communication within the entire team. Grading will consider the clarity, feasibility, and inclusiveness of the suggested communication methods.\n\n(ii) Communicating Vision in the Gaming Industry:\nGrading will assess the effectiveness and relevance of measures proposed to ensure alignment on the game's vision, idea, and feel among team members, considering the unique challenges of the gaming industry.\n\n\n\nUsefulness in real-life\nThe given exercise provides an opportunity to apply problem-solving skills in real-world scenarios. By addressing the challenges faced by Canes Gaming Inc., specifically the declining user engagement and retention in its mobile application, you will be able to gain insights into the complexities of managing a tech startup and devising strategic solutions to overcome obstacles.", "start_char_idx": 0, "end_char_idx": 5382, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc32dd09-e343-46df-a203-14e691b18d43": {"__data__": {"id_": "cc32dd09-e343-46df-a203-14e691b18d43", "embedding": null, "metadata": {"source": "cB - Careers challenge - Time Management - W2.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e0e4a08-388f-4272-97d6-6915c4233ecd", "node_type": "4", "metadata": {"source": "cB - Careers challenge - Time Management - W2.docx"}, "hash": "bd92fc3ec42f0a47e4570bb9c39abb5e519b6812d2a536b1a6ced45e35acaadd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc4ef1cb-dcea-46b9-a494-f0acc3d66efd", "node_type": "1", "metadata": {}, "hash": "93e40694e8497cedafa3d875a9cb504a25891a91a495e0f1081418b4c3732eb0", "class_name": "RelatedNodeInfo"}}, "text": "10 Academy cB : Week 2\nCareers - Exercise 2\n\nTime Management\n\nDue Date: Thursday, 2th May 2024, 10AM UTC\n\nBackground\nToday is a typical Thursday at work, and you have a list of tasks that need to be completed before the day ends. You start your day at 8:00 am and finish at 5:00 pm. Review the tasks you need to accomplish; remember, you don't have to complete everything today, but you must finish 11 of the tasks. You can address the remaining tasks the following day.\n\nSeries of Tasks\n1. Attend the daily stand-up meeting at 8:00 am, which lasts for 30 minutes.\n2. Fix a bug for a client who has been complaining for over a week.\n3. Train an intern who has encountered a blocker in their code that\u2019s hindering them to continue their work.\n4. Onboard a new team member who started on Monday and is still unclear about their responsibilities.\n5. Meet with the marketing team to share the data they requested last week and need to use them today.\n6. Submit the management update report for yesterday's deployment.\n7. Attend a one-hour webinar relevant to your current projects, although the recording will be available the following day.\n8. Prepare a 30-minute presentation for a stakeholder meeting scheduled for 2:00 pm.\n9. Your colleagues requested to have lunch with you by 12:00 pm.\n10. Respond to emails from other departments requesting technical support.\n11. Participate in an online conference call with a potential software vendor at 1:00 PM (note that you are not presenting).\n12. Check in call with your remote team members about their progress on various tasks.\n13. Coordinate with the QA (Quality Assurance) team to test new features added last week.\n14. Schedule a team meeting for next week to discuss upcoming project milestones.\n15. Analyse user feedback on a newly deployed app received through the customer support portal.\n16. Plan and organise the team's participation in an upcoming tech conference that is happening in two weeks.\n\nExercise\nDistribute your chosen 11 tasks into 3 columns; High Priority, Medium Priority and Low Priority and then explain why you put a certain task in that priority column. \nPlan Your Day: Using Google Calendar, schedule each task of those 11 taks, allocating specific today\u2019s time blocks (8am to 5pm) based on their priority and deadline. Then, take a screenshot of the schedule and add it in your PPT.\nYou have the remaining 5 tasks which you decided to not work on today. Please explain why you chose to not work on each one of them today. \nFrom the time management strategies shared in the Tutorial, which one did you choose to use while doing this exercise, and why?\nWhat reflection do you have on your experiences from this exercise, particularly focusing on how you managed your time during the day. \n\nSubmission\nCreate a 7 slides maximum presentation containing the answers to the exercise above, then convert it to a PDF format for submission on Tenx. \n\nNote: While submitting on Tenx, Please submit a PDF format of your presentation.\nRubrics\nTask Prioritisation: In this question, trainees are required to categorize the given tasks into three priority levels: high, medium, and low. Grading will focus on whether they justified their decisions based on factors such as urgency, importance, and deadlines, demonstrating strategic thinking and decision-making skills.\n\nTask Allocation on Google calendar: Trainees are required to use Google Calendar to schedule each of the 11 chosen tasks into specific today\u2019s time blocks, considering their priority and deadline. Grading will look on if the schedule is realistic, well-organized, and clearly labeled for easy reference.\n\nStrategic Decision Making: Trainees are asked to explain why they chose not to work on each of the remaining 5 tasks on the given day. Grading will focus on their justification of their decisions based on strategic considerations, task dependencies, and overall priorities, demonstrating effective decision-making and communication skills.\n\nTime Management Strategy: Trainees are required to identify and justify the time management strategy they chose to use during the exercise. Grading will look at whether they explained how this strategy was applied and its effectiveness in managing their time effectively throughout the task prioritization and scheduling process.\n\nReflection on Time Management: Trainees are asked to reflect on their experiences managing their time during the exercise, focusing on insights gained, strengths and weaknesses identified, and implications for future time management practices. Grading will look on whether they provided thoughtful reflections that demonstrate self-awareness and a commitment to continuous improvement.", "start_char_idx": 0, "end_char_idx": 4695, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc4ef1cb-dcea-46b9-a494-f0acc3d66efd": {"__data__": {"id_": "cc4ef1cb-dcea-46b9-a494-f0acc3d66efd", "embedding": null, "metadata": {"source": "cB - Careers challenge - Time Management - W2.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e0e4a08-388f-4272-97d6-6915c4233ecd", "node_type": "4", "metadata": {"source": "cB - Careers challenge - Time Management - W2.docx"}, "hash": "bd92fc3ec42f0a47e4570bb9c39abb5e519b6812d2a536b1a6ced45e35acaadd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc32dd09-e343-46df-a203-14e691b18d43", "node_type": "1", "metadata": {"source": "cB - Careers challenge - Time Management - W2.docx"}, "hash": "7b9d211936bb405966c25f08320e6368b1a747001e81b024f8b584757e217fc8", "class_name": "RelatedNodeInfo"}}, "text": "Usefulness in life\nEffective time management is essential for meeting deadlines, prioritising tasks, and maintaining productivity without feeling overwhelmed. By engaging in this exercise, trainees learn to allocate their time wisely, handle multiple tasks, and respond flexibly to unexpected challenges. Additionally, these skills are transferable across various domains beyond work, including personal life management, where balancing multiple responsibilities is key. Therefore, this will serve as a practical tool for developing competencies that improve organisational outcomes and personal effectiveness in managing daily demands.", "start_char_idx": 4699, "end_char_idx": 5335, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f04a4751-239b-46ed-9c00-ab824a58827b": {"__data__": {"id_": "f04a4751-239b-46ed-9c00-ab824a58827b", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 7.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b0116d15-a8a7-4c8c-a4a5-d6249046e4ba", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 7.docx"}, "hash": "1dd76d0fd722c116abe23d96b5fabc494a12d8a329c98913faf99c48a39dc368", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f3e0020-1373-44d5-b4d8-92c0f7c36f36", "node_type": "1", "metadata": {}, "hash": "5be9bc18fe2fa8618ec35ce1ac775cfb385dfd0a026cb003f3256f994ee16cc3", "class_name": "RelatedNodeInfo"}}, "text": "10 Academy Cohort B\nWeekly Challenge: Week 7\nPrecision RAG: Prompt Tuning For Building Enterprise Grade RAG Systems\n\nBusiness objective  \nPromptlyTech is an innovative e-business specializing in providing AI-driven solutions for optimizing the use of Language Models (LLMs) in various industries. The company aims to revolutionize how businesses interact with LLMs, making the technology more accessible, efficient, and effective. By addressing the challenges of prompt engineering, the company plays a pivotal role in enhancing decision-making, operational efficiency, and customer experience across various industries. PromptlyTech's solutions are designed to cater to the evolving needs of a digitally-driven business landscape, where speed and accuracy are key to staying competitive.\nThe company focuses on key services: Automatic Prompt Generation, Automatic Evaluation Data Generation, and Prompt Testing and Ranking.\n1. Automatic Prompt Generation Service:\nThis service streamlines the process of creating effective prompts, enabling businesses to efficiently utilize LLMs for generating high-quality, relevant content. It significantly reduces the time and expertise required in crafting prompts manually.\n2. Automatic Evaluation Data Generation Service:\nPromptlyTech\u2019s service automates the generation of diverse test cases, ensuring comprehensive coverage and identifying potential issues. This enhances the reliability and performance of LLM applications, saving significant time in the QA(Quality Assurance) process.\n3. Prompt Testing and Ranking Service:\nPromptlyTech\u2019s service evaluates and ranks different prompts based on effectiveness, helping Users to get the desired outcome from LLM. It ensures that chatbots and virtual assistants provide accurate, contextually relevant responses, thereby improving user engagement and satisfaction.\nBackground Context\n\nIn the evolving field of artificial intelligence, Language Models (LLMs) like GPT-3.5 and GPT-4 have become crucial for various applications. Their effectiveness, however, heavily depends on the quality of the prompts they receive, leading to the emergence of \"prompt engineering\" as a key skill.\n\nPrompt engineering is the craft of designing queries or statements to guide LLMs to produce desired outcomes. The challenge lies in the sensitivity of these models to prompt nuances, where slight variations can yield vastly different results. This poses a significant hurdle for users, especially in business contexts where accuracy and relevance are paramount.\n\nThe need for simplified, efficient prompt engineering is clear. Automating and optimizing this process can save time, enhance LLM productivity, and make advanced AI capabilities more accessible to a broader range of users. The tasks of Automatic Prompt Generation, Evaluation Data Generation, and Prompt Testing and Ranking are aimed at addressing these challenges, streamlining the prompt engineering process for more effective use of LLMs.\nLearning Outcomes\nSkills Development\nPrompt Engineering Proficiency: Gain expertise in crafting effective prompts that guide LLMs to desired outputs, understanding nuances and variations in language that impact model responses.\nCritical Analysis: Develop the ability to critically analyze and evaluate the effectiveness of different prompts based on their performance in varied scenarios.\nTechnical Aptitude with LLMs: Enhance technical skills in using advanced language models like GPT-4 and GPT-3.5-Turbo, understanding their functionalities and capabilities.\nProblem-Solving and Creativity: Cultivate creative problem-solving skills by generating innovative prompts and test cases, addressing complex and varied objectives.\nData Interpretation: Learn to interpret and analyze data from test cases and prompt evaluations, deriving meaningful insights from performance metrics.\n\nKnowledge Acquisition\nUnderstanding of Language Models: Acquire a deeper understanding of how LLMs function, including their strengths, limitations, and the principles behind their responses.\nInsights into Automated Evaluation Data Generation: Gain knowledge about the methodology and importance of creating test cases for evaluating prompt effectiveness.\nELO Rating System and its Applications: Learn about the ELO rating system used for ranking prompts, understanding its mechanics and relevance in performance evaluation.\nPrompt Optimization Strategies: Understand various strategies for refining and optimizing prompts to achieve better alignment with specific goals and desired outcomes.\nIndustry Best Practices: Familiarize with the best practices in prompt engineering within different industries, learning about real-world applications and challenges.\n\nTeam\nTutors: \nYabebal\nEmtinan\nRehmet\nBadges\nEach week, one user will be awarded one of the badges below for the best performance in the category below.\n\nIn addition to being the badge holder for that badge, each badge winner will get +20 points to the overall score.", "start_char_idx": 1, "end_char_idx": 4987, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f3e0020-1373-44d5-b4d8-92c0f7c36f36": {"__data__": {"id_": "2f3e0020-1373-44d5-b4d8-92c0f7c36f36", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 7.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b0116d15-a8a7-4c8c-a4a5-d6249046e4ba", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 7.docx"}, "hash": "1dd76d0fd722c116abe23d96b5fabc494a12d8a329c98913faf99c48a39dc368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f04a4751-239b-46ed-9c00-ab824a58827b", "node_type": "1", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 7.docx"}, "hash": "adc22827b85a662804c510b57b7a951fbffbef2a752bdc5d90edae821a351253", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "469ccf88-5155-4e6c-b8ad-f7ce7e0cd6d9", "node_type": "1", "metadata": {}, "hash": "60e102129d5967bbb1be029c97c6a2687f1d7fabcc98ae6c394d8e7b3848d232", "class_name": "RelatedNodeInfo"}}, "text": "Insights into Automated Evaluation Data Generation: Gain knowledge about the methodology and importance of creating test cases for evaluating prompt effectiveness.\nELO Rating System and its Applications: Learn about the ELO rating system used for ranking prompts, understanding its mechanics and relevance in performance evaluation.\nPrompt Optimization Strategies: Understand various strategies for refining and optimizing prompts to achieve better alignment with specific goals and desired outcomes.\nIndustry Best Practices: Familiarize with the best practices in prompt engineering within different industries, learning about real-world applications and challenges.\n\nTeam\nTutors: \nYabebal\nEmtinan\nRehmet\nBadges\nEach week, one user will be awarded one of the badges below for the best performance in the category below.\n\nIn addition to being the badge holder for that badge, each badge winner will get +20 points to the overall score.\n\nVisualization - quality of visualizations, understandability, skimmability, choice of visualization\nQuality of code - reliability, maintainability, efficiency, commenting - in future this will be CICD/CML\nInnovative approach to analysis -using latest algorithms, adding in research paper content and other innovative approaches\nWriting and presentation - clarity of written outputs, clarity of slides, overall production value\nMost supportive in the community - helping others, adding links, tutoring those struggling\n\nThe goal of this approach is to support and reward expertise in different parts of the Machine learning engineering toolbox.\n\nGroup Work Policy\nEveryone has to submit all their work individually. \nInstruction: Automatic Prompt Engineering\nFundamental Tasks\nThe core tasks for this week\u2019s challenge in Automatic Prompt Engineering are outlined below:\n\nUnderstand Prompt Engineering Tools and Concepts: Gain a thorough understanding of the tools and theoretical concepts involved in prompt engineering for Language Models (LLMs).\n\nFamiliarize with Language Models: Learn about the capabilities and functionalities of advanced LLMs like GPT-4 and GPT-3.5-Turbo.\n\nDevelop a Plan for Prompt Generation and Testing: Create a comprehensive plan that outlines the approach for automated prompt generation, test case creation, and prompt evaluation.\n\nSet Up a Development Environment: Prepare a suitable development environment that supports the integration and testing of LLMs in the prompt engineering process.\n\nDesign User Interface for Prompt System: Plan and initiate the development of a user-friendly interface for prompt input, refinement, and performance analysis.\n\nPlan Integration of LLMs: Strategize the integration of LLMs into the prompt system for automated generation and testing.\n\nBuild and Refine Prompt Generation System: Develop the automated prompt generation system, ensuring it aligns with user inputs and objectives.\n\nDevelop Automatic Evaluation Data Generation System: Create a system for generating test cases that evaluate the effectiveness of prompts in various scenarios.\n\nImplement Prompt Testing and Evaluation Mechanism: Set up testing procedures using Monte Carlo matchmaking and ELO rating systems to evaluate and rank prompts.\n\nRefine and Optimize System Based on Feedback: Continuously refine the prompt generation and evaluation system based on user feedback and performance data.\n\n\n\n\n\n\n\nTask 1: Review the Evolution of Automatic Prompt Engineering\nFocus on understanding the key developments in the field of automatic prompt engineering for Language Models (LLMs).\n\nStudy Key Concepts and Tools:\nUnderstand the key components of an enterprise-grade RAG systems\nRetrieval-augmented generation (RAG): What it is and why it\u2019s a hot topic for enterprise AI\nAdvanced RAG for LLMs/SLMs\nRAG for Text Generation Processes in Businesses (check part 1, 3, & 4 as well) \nLangchain Reterivers\nUnderstand the need for advanced prompt engineering in building enterprise grade RAG systems\nFull Fine-Tuning, PEFT, Prompt Engineering, and RAG: Which One Is Right for You?\nAdvanced Prompt Engineering - Practical Examples\nPrompt Engineering 201: Advanced methods and toolkits\nDo you agree with this article?", "start_char_idx": 4052, "end_char_idx": 8227, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "469ccf88-5155-4e6c-b8ad-f7ce7e0cd6d9": {"__data__": {"id_": "469ccf88-5155-4e6c-b8ad-f7ce7e0cd6d9", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 7.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b0116d15-a8a7-4c8c-a4a5-d6249046e4ba", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 7.docx"}, "hash": "1dd76d0fd722c116abe23d96b5fabc494a12d8a329c98913faf99c48a39dc368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f3e0020-1373-44d5-b4d8-92c0f7c36f36", "node_type": "1", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 7.docx"}, "hash": "0216f17cb2a73d916e6bd2e127c119679a100c35c64dab90404806d7dcfcd7bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6777f239-17c9-45f8-a06a-e094d1332ded", "node_type": "1", "metadata": {}, "hash": "66af43942f3e81224107aa1e2272c7cb9f05f5d481bb593eedd6112a04380ae1", "class_name": "RelatedNodeInfo"}}, "text": "Task 1: Review the Evolution of Automatic Prompt Engineering\nFocus on understanding the key developments in the field of automatic prompt engineering for Language Models (LLMs).\n\nStudy Key Concepts and Tools:\nUnderstand the key components of an enterprise-grade RAG systems\nRetrieval-augmented generation (RAG): What it is and why it\u2019s a hot topic for enterprise AI\nAdvanced RAG for LLMs/SLMs\nRAG for Text Generation Processes in Businesses (check part 1, 3, & 4 as well) \nLangchain Reterivers\nUnderstand the need for advanced prompt engineering in building enterprise grade RAG systems\nFull Fine-Tuning, PEFT, Prompt Engineering, and RAG: Which One Is Right for You?\nAdvanced Prompt Engineering - Practical Examples\nPrompt Engineering 201: Advanced methods and toolkits\nDo you agree with this article? RAG is Just Fancier Prompt Engineering\nUnderstand the need for evaluating RAG components\nAn Overview on RAG Evaluation\nEvaluating RAG: Using LLMs to Automate Benchmarking of Retrieval Augmented Generation Systems\nEvaluating RAG Applications with RAGAs\nRAG Evaluation Using LangChain and Ragas\nRAG System: Metrics and Evaluation Analysis with LlamaIndex\nEvaluating RAG Part I: How to Evaluate Document Retrieval\nEvaluating RAG/LLMs in highly technical settings using synthetic QA generation\nEvaluating Multi-Modal RAG\nUnderstand the tools and techniques to automatically generate RAG evaluation data \nThe Tech Buffet #16: Quickly Evaluate your RAG Without Manually Labeling Test Data\nGenerating a Synthetic Dataset for RAG\n\nLearn key packages to planning, building, testing, monitoring, and deploying enterprise grade RAG system\nIterate on LLMs faster: Measure LLM quality and catch regressions\nBuilding RAG-based LLM Applications for Production\nARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems\nUnderstand the end-to-end technology stack of RAG systems\nEnd-to-End LLMOps Platform\nAn Enterprise-Grade Reference Architecture for the Production Deployment of LLMs Using the RAG Pattern on Azure OpenAI\nTask 2: Design and Develop the Prompt Generation System\nUsers can input a description of their objective or task and specify a few scenarios along with their expected outputs. \nWrite or adopt sophisticated algorithms, you generate multiple prompt options based on the provided information. \nThis automated prompt generation process saves time and provides a diverse range of alternatives to consider. But add evaluation metrics that check whether the generated prompt candidate aligns with the input description.\nNote that you may use the collection of prompts from danielmiessler/fabric: fabric is an open-source framework for augmenting humans using AI. It provides a modular framework for solving specific problems using a crowdsourced set of AI prompts that can be used anywhere. (github.com) to get started. You can apply RAG on prompt knowledge base. For a simple way of fetching semantically similar prompts, think how to use semantic routes\nTask 3: Implement Evaluation Data Generation and Evaluation\nTo further enhance the prompt generation process, incorporate automatic Evaluation Data Generation. \nBy analysing the description provided by the user,  create a set of test cases that serve as evaluation benchmarks for the prompt candidates.\nThese test cases simulate various scenarios, enabling users to observe how each prompt performs in different contexts. \nThe generated test cases serve as a starting point, sparking creativity and inspiring additional test cases for comprehensive evaluation.\nTask 4: Prompt Testing and Ranking\nGoals\nComprehensive Evaluation: Provide a robust system that uses various methodologies for a thorough assessment of prompts.\nCustomizable and User-Centric: Allow users to choose or customize their preferred evaluation methods.\nDynamic and Adaptive: Ensure the system remains flexible and adaptive, capable of incorporating new ranking methodologies as they emerge.\n\nPrimary Methods\n\nMonte Carlo Matchmaking: This method is used to select and match different prompt candidates against each other. The Monte Carlo method, known for its applications in problem-solving and decision-making processes, helps in optimizing the information gained from each prompt battle. By simulating various matchups, it allows the system to test the effectiveness of each prompt in different scenarios.\nELO Rating System:  This system, which is commonly used in chess and other competitive games, rates the prompts based on their performance in the battles. Each prompt candidate is assigned a rating that reflects its success in previous matchups. The system takes into account not just the number of wins but also the strength of the opponents each prompt has defeated. This rating helps in objectively ranking the prompts based on their effectiveness.\n\nAdditional Ranking and Matching Mechanisms\nTrueSkill Rating System: Ideal for scenarios involving multiple competitors, adjusting ratings based on not just wins and losses but also the uncertainty in performance.", "start_char_idx": 7425, "end_char_idx": 12454, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6777f239-17c9-45f8-a06a-e094d1332ded": {"__data__": {"id_": "6777f239-17c9-45f8-a06a-e094d1332ded", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 7.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b0116d15-a8a7-4c8c-a4a5-d6249046e4ba", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 7.docx"}, "hash": "1dd76d0fd722c116abe23d96b5fabc494a12d8a329c98913faf99c48a39dc368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "469ccf88-5155-4e6c-b8ad-f7ce7e0cd6d9", "node_type": "1", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 7.docx"}, "hash": "55e13a834f797362a8414fea424f9a7301fe1623b1603efedf19e72a27095576", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abba96d6-93d0-4a8c-b1dd-f0b3b5492caf", "node_type": "1", "metadata": {}, "hash": "0f283c3a51b4a59b934c77656860aefbe6aec6ab053a72ff91e3e3d1931b8fde", "class_name": "RelatedNodeInfo"}}, "text": "Primary Methods\n\nMonte Carlo Matchmaking: This method is used to select and match different prompt candidates against each other. The Monte Carlo method, known for its applications in problem-solving and decision-making processes, helps in optimizing the information gained from each prompt battle. By simulating various matchups, it allows the system to test the effectiveness of each prompt in different scenarios.\nELO Rating System:  This system, which is commonly used in chess and other competitive games, rates the prompts based on their performance in the battles. Each prompt candidate is assigned a rating that reflects its success in previous matchups. The system takes into account not just the number of wins but also the strength of the opponents each prompt has defeated. This rating helps in objectively ranking the prompts based on their effectiveness.\n\nAdditional Ranking and Matching Mechanisms\nTrueSkill Rating System: Ideal for scenarios involving multiple competitors, adjusting ratings based on not just wins and losses but also the uncertainty in performance.\nGlicko Rating System: Similar to ELO but with added flexibility, accounting for the volatility in a player's (or prompt\u2019s) performance and the reliability of their rating.\nBayesian Rating Systems: Applies Bayesian inference for a probabilistic approach to rating, considering uncertainties and contextual variations in prompt performance.\nPairwise Comparison Methods: Involves direct comparisons between pairs of prompts, potentially integrating user preferences or expert evaluations into the ranking process.\nCategorical Ranking: Instead of a numerical rating, prompts are categorized based on performance criteria like creativity, relevance, etc., for more qualitative assessments.\nAdaptive Ranking Algorithms: Algorithms that learn and adjust over time, considering historical performance data and evolving user preferences or requirements.\nSemantic Similarity Matching: Using NLP techniques to match prompts based on semantic content, ideal for understanding nuanced differences in prompt effectiveness.\n\nYou should adopt an innovative approach to prompt evaluation by utilizing Monte Carlo matchmaking and  ELO rating systems, or any alternative method to match and rank.\nTask 5: User Interface Development\nDevelop a user-friendly interface for interacting with the prompt engineering system.\nUI Design: Plan and design a user interface that allows users to easily input data, receive prompts, and view evaluation results.\nUI Implementation: Develop and integrate the user interface with the backend prompt engineering system.\nTask 6: System Integration and Testing\nIntegrate all components of the system and conduct comprehensive testing.\nIntegrate the prompt generation, Evaluation Data Generation, evaluation, and user interface components.\nTest the entire system for functionality, usability, and performance. Refine based on feedback and test results.\n\n\nTutorials Schedule\nIn the following, the colour purple indicates morning sessions, and blue indicates afternoon sessions.\nMonday: Understanding Prompt engineering \nHere the trainees will understand the week\u2019s challenge.\nIntroduction to Week Challenge (Yabebal)\nIntroduction and challenges to prompt engineering (Emtinan)\n\nKey Performance Indicators:\n\nUnderstanding week\u2019s challenge\nUnderstanding the prompt engineering\nAbility to reuse previous knowledge\nTuesday\nRAG components (Rehmet)\nTechniques to improve R (Retrievers) in RAG (Emtinan)\n\nKey Performance Indicators:\n\nUnderstanding Prompt ranking \nUnderstanding prompt matching \nAbility to reuse previous knowledge\n\nWednesday\nRAG Evaluation Data Generation (Rehmet)\nUnderstanding of prompt matching and ranking (Emtinan)\nThursday\nRAG evaluation metrics (Emtinan)\nRAGOps - DevOps of RAG development and production deployment (Rehmet) \n\n\nDeliverables\nNOTE: Document should be a PDF stored in google drive or published blog link. DO NOT SUBMIT A LINK as PDF! If you want to submit pdf document, it should be the content of your report not a link. \nInterim Submission - Wednesday 8pm UTC\nLink to your code in GitHub\nRepository where you will be using to complete the tasks in this week's challenge. A minimum requirement is that you have a well structured repository and some coding progress is made.\n\n\nA review report of your reading and understanding of Task 1 and any progress you made in other tasks. \nFeedback\nYou may not receive detailed comments on your interim submission but will receive a grade.\nFinal Submission - Saturday 8pm UTC\nLink to your code in GitHub \nComplete work  for Automatic prompt generation\nComplete work  for Automatic evaluation \nComplete work for Evaluation Data Generation\n\nA blog post entry (which you can submit for example to Medium publishing) or a pdf report.\nFeedback\nYou will receive comments/feedback in addition to a grade.", "start_char_idx": 11372, "end_char_idx": 16232, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abba96d6-93d0-4a8c-b1dd-f0b3b5492caf": {"__data__": {"id_": "abba96d6-93d0-4a8c-b1dd-f0b3b5492caf", "embedding": null, "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 7.docx"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b0116d15-a8a7-4c8c-a4a5-d6249046e4ba", "node_type": "4", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 7.docx"}, "hash": "1dd76d0fd722c116abe23d96b5fabc494a12d8a329c98913faf99c48a39dc368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6777f239-17c9-45f8-a06a-e094d1332ded", "node_type": "1", "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 7.docx"}, "hash": "0b0ba60bb507a62daa29690437a87627d80972c825d65dd40225ea4c80886579", "class_name": "RelatedNodeInfo"}}, "text": "A review report of your reading and understanding of Task 1 and any progress you made in other tasks. \nFeedback\nYou may not receive detailed comments on your interim submission but will receive a grade.\nFinal Submission - Saturday 8pm UTC\nLink to your code in GitHub \nComplete work  for Automatic prompt generation\nComplete work  for Automatic evaluation \nComplete work for Evaluation Data Generation\n\nA blog post entry (which you can submit for example to Medium publishing) or a pdf report.\nFeedback\nYou will receive comments/feedback in addition to a grade.\n\n\n\n\n\n\n\n\n\n\nReferences\nMeistrari didn\u2019t see a good solution for prompt engineering, so it\u2019s building one\nAutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\nLarge Language Models Are Human-Level Prompt Engineers\nPrompt Engineering\nHow to Create a Monte Carlo Simulation using Python\nMonte Carlo Method Explained\nWhat is Monte Carlo Simulation? How does it work?\nElo Rating Algorithm\nElo algorithm implementation in Python\nTrueSkillTM: A Bayesian skill rating system\n\nCompanies doing something similar to this project\nAI Prompt Generator (promptlygenerated.com)", "start_char_idx": 15672, "end_char_idx": 16829, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"e968cea1-a502-4cb3-9fa5-165fad0fbe97": {"node_ids": ["0cc264c1-fb4d-4cbf-be10-8841aa149d7d"], "metadata": {"source": "W4 - Utilizing AI to Enhance Social Media Management and Customer Engagement.docx"}}, "1a9ac60b-68bc-4cb6-932c-4b6409424270": {"node_ids": ["832a7118-3876-4f66-a980-63222ab5349d"], "metadata": {"source": "cB - Careers challenge - Effective communication - W3.docx"}}, "86b88a2a-506c-42c5-9be6-c89d9f176441": {"node_ids": ["b2c785b1-167f-471f-92a3-70d1cd47447e", "e7e6113e-562a-4166-96a6-1040ae75dde5"], "metadata": {"source": "cB - Careers challenge - Procrastination - W4.docx"}}, "72ee50bd-91ae-40bf-9d6d-f0729f591944": {"node_ids": ["27dd70b0-b8b8-43d1-860e-14b506ce2d09", "de4e5b1e-4261-4722-94b4-321670498b49"], "metadata": {"source": "W0 - Careers Exercise - Ideas to change the world-day4.docx"}}, "bb63bed9-e96d-450a-befe-74d04be6c3d7": {"node_ids": ["f8eabfcb-05a3-40fa-bbf4-08deaec84d86", "f37d496b-4500-4fce-a23b-f42180c83aa6", "d145b5c1-7984-4215-b3c2-2f37e3b1f599", "21f68195-32d4-4909-92be-0b6c1b099fec", "14ebf21a-8198-4ee7-915a-268a0baa80cd", "944c9001-5ef6-486f-9882-cef669827d0f", "c96f8d42-e760-4e1a-baf9-75850d4c4e4d", "f1a974ba-729b-4d25-bc00-5b1631ae6eed", "07e103ba-50e5-4ec1-9cc1-584ec91aa634", "bd1ab8ec-ff13-4e1e-ae80-025843e92cde", "5a4d36f3-355f-4f48-8aa8-47c41173d166", "a1f6190a-6a6d-4bb2-b4b7-a7f9f2ac96d3", "77d9da54-7c42-4207-a8e7-caee2ac87d6a"], "metadata": {"source": "Reverse Engineered Brief for Existing Telegram Ads.docx"}}, "c9745c1c-43b7-4c6f-b03e-9ed27d1b2c7c": {"node_ids": ["25eadb1e-1448-4df1-b9d3-69d08cfada22"], "metadata": {"source": "Extra Tuition Deferral Scholarship - Cohort B.docx"}}, "983be719-4f7d-4b46-a2e0-de573c270f83": {"node_ids": ["450022b1-1ffe-416f-9f58-4e00afa23d5d"], "metadata": {"source": "Week-0 Materials.docx"}}, "c368a139-fdd0-4cfd-862f-89dedca67464": {"node_ids": ["34630ba6-49bb-4122-92eb-b42adf1f58c3"], "metadata": {"source": "cB - Careers challenge - Proactivity - W6.docx"}}, "400ed0e3-96e8-4fac-b259-53fce81b308b": {"node_ids": ["b452b052-3e9f-4ef5-a677-48b2239b1884", "f525727f-f691-4885-815d-02d4eb24c55a", "b3415edd-b73b-44c4-be73-c16c66c62240", "4f7a87bf-2e16-44fd-a3f1-d39ea3ba3eb9", "cf883145-f96b-4c75-bd42-99f12ce780d9"], "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 3.docx"}}, "281d72ae-706e-4730-9aa2-7fed4b3dcd2c": {"node_ids": ["2ad3ff9e-7acb-411f-bcd6-b3dc21f4a045"], "metadata": {"source": "cB - Career Challenge - 3 Real World Jobs.docx"}}, "ffe3dbcf-b7d0-4566-b4ae-d519caa9ed8a": {"node_ids": ["66ca1fde-05a7-4c6a-a2e2-5ebbfe650f3f", "be69b7c1-ab89-4f61-807a-a9e778f60ae8", "32a9398f-0f4f-4a08-aff1-43c30a00529a", "a66145b4-44c0-40bf-8ad1-fc8a43384b8f", "fd13e060-dc5e-4c64-9ae6-5afebea58d32"], "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 6.docx"}}, "ca80b0f6-b94d-4be9-9164-f0933cd58098": {"node_ids": ["fc8fd058-446b-46fb-8f4d-09a4f3db721a", "8e105af5-c502-4415-a3c1-eb97d221356e", "5743c4ec-2a50-495b-9bae-c52c6842b09e"], "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 2.docx"}}, "913bd659-b6ac-42c3-9d68-e20ec1639db6": {"node_ids": ["a3ba542d-51c7-4ebc-bf1f-be8ad36cb7d3", "6e4eeb68-d623-4d3d-8e04-ad45c67a2a35", "3216d25b-d3c5-4a7a-ae21-f1c0cbf4f168"], "metadata": {"source": "cB - Careers challenge - Peer Mentorship - W1.docx"}}, "85341c2a-a043-4165-8d5b-db221409e1d0": {"node_ids": ["50ca1b87-339f-4b5b-b9df-a0c7aba9682c"], "metadata": {"source": "cB - Careers challenge - Leadership - W5.docx"}}, "fd04ce89-ee65-4c26-8cee-eb0ed9120c40": {"node_ids": ["805357ac-c84c-4cab-acd4-55c6f0450b78", "68a074e6-6323-4b68-a70b-ad619a311f35", "045f4922-71e0-4de9-acca-71a3b5657f00", "4fd96d3c-91ba-4f7d-9795-3ddc3c7545dc"], "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week 5.docx"}}, "99eb7832-16cf-4b6e-8300-fb674fb0651d": {"node_ids": ["197e6528-11b7-45ed-80d2-ee6800bc02ac", "9fa8dda1-b3c5-4ded-9f93-2ded8dc0afce"], "metadata": {"source": "cB - 10 Academy - Careers - Week 2 - Tools for Remote Work.docx"}}, "8706c933-385c-4f7a-8d1b-0f7c40870e61": {"node_ids": ["535fb58a-bbef-423f-972c-3592d270b0f7"], "metadata": {"source": "W0 - Careers Exercise - CV Submission-day2.docx"}}, "d3f5a848-f1f9-4ba3-b78b-f2246d4186aa": {"node_ids": ["93e4ae38-a046-457f-9d23-96f5155bb1c8", "c5086421-dd73-499c-9157-71e45a9463ea", "5cba45a4-5a27-404b-ba56-9767f5f47759", "cd027211-1b03-4652-a363-f601de8f98e9", "5d21f329-2c8d-453c-90d0-b18559722131", "f2c40a1d-2be6-42e1-80a5-6cb208c16cff"], "metadata": {"source": "10 Academy Cohort B - Week 1 Challenge.docx"}}, "485e9781-7c77-4015-a0e4-1f399abd0e2d": {"node_ids": ["8ed6b151-b867-4ba5-9f5e-4ba929044d23"], "metadata": {"source": "Teamwork.docx"}}, "22bb4129-e78e-48d3-921a-fa73a3d1f7df": {"node_ids": ["9836a060-fb0f-41c3-b0cc-39c736b961dc", "8485f53e-4366-4925-94db-1f1f0f1db64a", "e8f7cca3-b9d9-4703-8274-93bdbd62048b"], "metadata": {"source": "Asking Good Questions Challenge - W5 - cB.docx"}}, "f1cd40ef-11ea-4e7a-a88a-05c774a32226": {"node_ids": ["9e46ee6f-6e1e-4890-afde-dbe518f9fdfc"], "metadata": {"source": "How to use Tenx.docx"}}, "d97e69df-0a2f-4b0a-a3d4-3634e73dbab4": {"node_ids": ["3e2f842f-f73d-42d0-a9c6-26735788256e"], "metadata": {"source": "Payment Processing Manual_.docx"}}, "05f3a61d-ded7-4eb3-97e0-9b624ba9e052": {"node_ids": ["c718f1af-8ad9-4372-9b32-7b285523ef03", "944464a4-f05f-4d22-90bf-e51df3ec7572", "200ae9be-35b3-4dce-b978-78a9ab45051c", "de101251-6a1b-45eb-862f-363b603f355a", "7d1a4df3-e851-4dae-9dd2-07a0e1a8e5fa", "99e38979-48ca-4ada-a916-fc6d0a9b1599"], "metadata": {"source": "10 Academy Intensive Training_ Week-0 Challenge Document.docx"}}, "2caa6841-b9a9-434b-8739-8f1f25ac9fed": {"node_ids": ["ef1e2138-0a34-48b8-b2fd-76be3b050586"], "metadata": {"source": "cB- Problem-Solving Skills- Wk3.docx"}}, "9e0e4a08-388f-4272-97d6-6915c4233ecd": {"node_ids": ["cc32dd09-e343-46df-a203-14e691b18d43", "cc4ef1cb-dcea-46b9-a494-f0acc3d66efd"], "metadata": {"source": "cB - Careers challenge - Time Management - W2.docx"}}, "b0116d15-a8a7-4c8c-a4a5-d6249046e4ba": {"node_ids": ["f04a4751-239b-46ed-9c00-ab824a58827b", "2f3e0020-1373-44d5-b4d8-92c0f7c36f36", "469ccf88-5155-4e6c-b8ad-f7ce7e0cd6d9", "6777f239-17c9-45f8-a06a-e094d1332ded", "abba96d6-93d0-4a8c-b1dd-f0b3b5492caf"], "metadata": {"source": "10 Academy Cohort B - Weekly Challenge_ Week - 7.docx"}}}}